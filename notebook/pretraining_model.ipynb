{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea500607",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install --user lightning-bolts -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ead55d06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version:  1.8.0\n",
      "Device available: True\n",
      "Device name: Tesla P100-PCIE-16GB\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import time\n",
    "import os\n",
    "import gc\n",
    "import math\n",
    "import multiprocessing\n",
    "import io\n",
    "import logging \n",
    "import itertools\n",
    "import shutil\n",
    "import pysnooper\n",
    "import warnings\n",
    "import glob\n",
    "import pendulum\n",
    "import random\n",
    "import json\n",
    "import sys\n",
    "import wandb\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import scikitplot as skplt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from icecream import ic\n",
    "from collections import Counter\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from pathlib import Path\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import transformers\n",
    "import torchmetrics\n",
    "from torch import nn\n",
    "from torch import cuda\n",
    "\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "matplotlib.use('Agg')\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "seed = 9527\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.WARNING)\n",
    "\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "print('Torch version: ', torch.__version__)\n",
    "print('Device available:', torch.cuda.is_available())\n",
    "print('Device name:', torch.cuda.get_device_name(0))\n",
    "torch.set_printoptions(precision=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d3378ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom torch.utils.data.distributed import DistributedSampler\\nfrom torch.nn.parallel import DistributedDataParallel as DDP\\ntorch.distributed.init_process_group(backend=\\'nccl\\')\\n\\nlocal_rank = -1\\nif local_rank not in [-1, 0]:\\n    torch.distributed.barrier()  # Make sure only the first process in distributed training will download model & vocab\\n    \\ndef setup(rank, world_size):\\n    os.environ[\\'MASTER_ADDR\\'] = \\'localhost\\'\\n    os.environ[\\'MASTER_PORT\\'] = \\'12355\\'\\n\\n    # initialize the process group\\n    dist.init_process_group(\"gloo\", rank=rank, world_size=world_size)\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "torch.distributed.init_process_group(backend='nccl')\n",
    "\n",
    "local_rank = -1\n",
    "if local_rank not in [-1, 0]:\n",
    "    torch.distributed.barrier()  # Make sure only the first process in distributed training will download model & vocab\n",
    "    \n",
    "def setup(rank, world_size):\n",
    "    os.environ['MASTER_ADDR'] = 'localhost'\n",
    "    os.environ['MASTER_PORT'] = '12355'\n",
    "\n",
    "    # initialize the process group\n",
    "    dist.init_process_group(\"gloo\", rank=rank, world_size=world_size)\n",
    "'''    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd09b86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_value = 9527\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "set_seed(seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0df80c09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--input_file'], dest='input_file', nargs=None, const=None, default=None, type=None, choices=None, help='Input raw text file (or comma-separated list of files).', metavar=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--output_file'], dest='output_file', nargs=None, const=None, default=None, type=None, choices=None, help='Output TF example file (or comma-separated list of files).', metavar=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--vocab_file'], dest='vocab_file', nargs=None, const=None, default=None, type=None, choices=None, help='The vocabulary file that the ALBERT model was trained on.', metavar=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--spm_model_file'], dest='spm_model_file', nargs=None, const=None, default=None, type=None, choices=None, help='The model file for sentence piece tokenization.', metavar=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--input_file_mode'], dest='input_file_mode', nargs=None, const=None, default='r', type=None, choices=None, help='The data format of the input file.', metavar=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--do_lower_case'], dest='do_lower_case', nargs=None, const=None, default=True, type=None, choices=None, help='Whether to lower case the input text. Should be True for uncased models and False for cased models.', metavar=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--do_whole_word_mask'], dest='do_whole_word_mask', nargs=None, const=None, default=True, type=None, choices=None, help='Whether to use whole word masking rather than per-WordPiece masking.', metavar=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--do_permutation'], dest='do_permutation', nargs=None, const=None, default=False, type=None, choices=None, help='Whether to do the permutation training.', metavar=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--favor_shorter_ngram'], dest='favor_shorter_ngram', nargs=None, const=None, default=True, type=None, choices=None, help='Whether to set higher probabilities for sampling shorter ngrams.', metavar=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--random_next_sentence'], dest='random_next_sentence', nargs=None, const=None, default=False, type=None, choices=None, help=\"Whether to use the sentence that's right before the current sentence as the negative sample for next sentence prection, rather than using sentences from other random documents.\", metavar=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--max_seq_length'], dest='max_seq_length', nargs=None, const=None, default=512, type=None, choices=None, help='Maximum sequence length.', metavar=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--ngram'], dest='ngram', nargs=None, const=None, default=3, type=None, choices=None, help='Maximum number of ngrams to mask.', metavar=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--max_predictions_per_seq'], dest='max_predictions_per_seq', nargs=None, const=None, default=20, type=None, choices=None, help='Maximum number of masked LM predictions per sequence.', metavar=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--random_seed'], dest='random_seed', nargs=None, const=None, default=12345, type=None, choices=None, help='Random seed for data generation.', metavar=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--dupe_factor'], dest='dupe_factor', nargs=None, const=None, default=5, type=None, choices=None, help='Number of times to duplicate the input data (with different masks).', metavar=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--masked_lm_prob'], dest='masked_lm_prob', nargs=None, const=None, default=0.15, type=None, choices=None, help='Masked LM probability.', metavar=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--short_seq_prob'], dest='short_seq_prob', nargs=None, const=None, default=0.1, type=None, choices=None, help='Probability of creating sequences which are shorter than the maximum length.', metavar=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--input_file', default=None, help=\"Input raw text file (or comma-separated list of files).\")\n",
    "parser.add_argument('--output_file', default=None, help=\"Output TF example file (or comma-separated list of files).\")\n",
    "parser.add_argument('--vocab_file', default=None, help=\"The vocabulary file that the ALBERT model was trained on.\")\n",
    "parser.add_argument('--spm_model_file', default=None, help=\"The model file for sentence piece tokenization.\")\n",
    "parser.add_argument('--input_file_mode', default=\"r\",  help=\"The data format of the input file.\")\n",
    "parser.add_argument('--do_lower_case', default=True, help=\"Whether to lower case the input text. Should be True for uncased models and False for cased models.\")\n",
    "parser.add_argument('--do_whole_word_mask', default=True, help=\"Whether to use whole word masking rather than per-WordPiece masking.\")\n",
    "parser.add_argument('--do_permutation', default=False, help=\"Whether to do the permutation training.\")\n",
    "parser.add_argument('--favor_shorter_ngram', default=True, help=\"Whether to set higher probabilities for sampling shorter ngrams.\")\n",
    "parser.add_argument('--random_next_sentence', default=False, help=\"Whether to use the sentence that's right before the current sentence \"\n",
    "                    \"as the negative sample for next sentence prection, rather than using \"\n",
    "                    \"sentences from other random documents.\")\n",
    "parser.add_argument('--max_seq_length', default=512, help=\"Maximum sequence length.\")\n",
    "parser.add_argument('--ngram', default=3, help=\"Maximum number of ngrams to mask.\")\n",
    "parser.add_argument('--max_predictions_per_seq', default=20, help=\"Maximum number of masked LM predictions per sequence.\")\n",
    "parser.add_argument('--random_seed', default=12345, help=\"Random seed for data generation.\")\n",
    "parser.add_argument('--dupe_factor', default=5, help=\"Number of times to duplicate the input data (with different masks).\")\n",
    "parser.add_argument('--masked_lm_prob', default=0.15, help=\"Masked LM probability.\")\n",
    "parser.add_argument('--short_seq_prob', default=0.1, help=\"Probability of creating sequences which are shorter than the maximum length.\")\n",
    "\n",
    "\n",
    "opt = parser.parse_args(args=[\n",
    "    '--input_file', '1995_income',  \n",
    "    '--output_file', 'MLP',\n",
    "    '--spm_model_file', './wiki-ja_albert.model',\n",
    "    '--vocab_file', './wiki-ja_albert.vocab',\n",
    "    '--do_whole_word_mask', False,\n",
    "    '--do_permutation', False,\n",
    "    '--favor_shorter_ngram', False,\n",
    "    '--random_next_sentenc', False\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d8663ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 202105\n",
    "\n",
    "# main'\n",
    "main_path = Path('/home/jupyter/gogolook')\n",
    "main_cached_path = Path('/home/jupyter/gogolook/data')\n",
    "\n",
    "# general setting\n",
    "main_data_path = main_path / 'data' / 'jp_data' \n",
    "main_model_path = main_path / 'models'\n",
    "cache_data_path = main_cached_path / 'cache_data_dir'\n",
    "cache_models_path = main_cached_path / 'cache_models_fir'\n",
    "\n",
    "# models\n",
    "albert_zh_path = main_model_path / 'albert_zh'\n",
    "\n",
    "# data\n",
    "regex_file_format = '*.json'\n",
    "data_tag = 'pretraining_data'\n",
    "valid_data_tag = 'pretraining_data'\n",
    "test_data_tag = 'pretraining_data'\n",
    "\n",
    "experiment_train_data_path = main_data_path / f'train_{data_tag}'    \n",
    "experiment_valid_data_path = main_data_path / f'valid_{valid_data_tag}'\n",
    "experiment_test_data_path = main_data_path / f'test_{test_data_tag}'\n",
    "\n",
    "training_data_path = experiment_train_data_path\n",
    "validation_data_path = experiment_valid_data_path\n",
    "testing_data_path = experiment_test_data_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed54099b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run id is jp-pretrain-model_baseline_20210902155112\n"
     ]
    }
   ],
   "source": [
    "project = 'jp-pretrain-model'\n",
    "project_shortname = 'jp-sms'\n",
    "group_tag = 'experiment' # 1. functional 2. experiment 3. staging 4. production\n",
    "job_type = 'baseline' # 1. baseline 2. optimize 3. hyper-tuning\n",
    "addition_tag = [data_tag, 'pytorch'] # exponential_decay\n",
    "method_tag = 'pretrain' # pretrain / finetune / pretrain_finetune\n",
    "time_tag = pendulum.now(tz='Asia/Taipei').strftime('%Y%m%d%H%M%S')\n",
    "run_id = '{}_{}_{}'.format(project, job_type, time_tag)\n",
    "print('Run id is {}'.format(run_id))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74be098e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "from datasets import load_dataset\n",
    "from datasets import total_allocated_bytes\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, Optional, Union, List\n",
    "from transformers import (\n",
    "    BertTokenizer, AlbertForPreTraining, AlbertModel, AlbertConfig, PreTrainedTokenizer)\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer, BertJapaneseTokenizer\n",
    "from pl_bolts.optimizers.lr_scheduler import LinearWarmupCosineAnnealingLR\n",
    "\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch import optim\n",
    "from torch.optim.swa_utils import AveragedModel, SWALR\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, CyclicLR\n",
    "from transformers import get_polynomial_decay_schedule_with_warmup\n",
    "        \n",
    "#import datasets\n",
    "#datasets.logging.set_verbosity_info()\n",
    "#datasets.logging.get_verbosity()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "98fbab1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] アンパサンド (&、 英語 名 :) と は 並立 助詞 「... と...」 を 意味 する 記号 で ある 。 ラテン語 の の 合 字 で 、 Trebuchet MS フォント で は 、 と 表示 さ れ \" et \" の 合 字 で ある こと が 容易 に わかる 。 [SEP]\n"
     ]
    }
   ],
   "source": [
    "mecab_tokenizer = BertJapaneseTokenizer.from_pretrained(\"cl-tohoku/bert-base-japanese\", word_tokenizer_type=\"mecab\", cache_dir=cache_models_path)\n",
    "# Input Japanese Text\n",
    "line = \"アンパサンド (&、英語名：) とは並立助詞「…と…」を意味する記号である。ラテン語の の合字で、Trebuchet MSフォントでは、と表示され \\\"et\\\" の合字であることが容易にわかる。\"\n",
    "mecab_inputs = mecab_tokenizer(line, return_tensors=\"pt\")\n",
    "print(mecab_tokenizer.decode(mecab_inputs['input_ids'][0]))\n",
    "corpus_size = len(mecab_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90f3b3bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cl-tohoku/bert-base-japanese': 512,\n",
       " 'cl-tohoku/bert-base-japanese-whole-word-masking': 512,\n",
       " 'cl-tohoku/bert-base-japanese-char': 512,\n",
       " 'cl-tohoku/bert-base-japanese-char-whole-word-masking': 512}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mecab_tokenizer.max_model_input_sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90b3626",
   "metadata": {},
   "source": [
    "# Model config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd11b801",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_config = {\n",
    "    \"SGD\": {\n",
    "        \"init_learning_rate\": 1e-1, \n",
    "        \"pre_finetune_learning_rate\": 1e-3\n",
    "    },\n",
    "    \"Adam\": {\n",
    "        \"init_learning_rate\": 1e-3,\n",
    "        \"pre_finetune_learning_rate\": 1e-5\n",
    "    }, \n",
    "    \"RAdam\": {\n",
    "        \"init_learning_rate\": 1e-3,\n",
    "        \"pre_finetune_learning_rate\": 2e-5\n",
    "    }     \n",
    "}\n",
    "\n",
    "model_config = {\n",
    "    \"epochs\": 3,\n",
    "    \"initial_epochs\": 20,\n",
    "    \"batch_size\": 128,\n",
    "    \"max_tokens_length\": 512,\n",
    "    \"threshold\": 0.5,\n",
    "    \"optimizer_method\": \"Adam\",\n",
    "    \"init_learning_rate\": optimizer_config['Adam']['init_learning_rate'],\n",
    "    \"pre_finetune_learning_rate\": optimizer_config['Adam']['pre_finetune_learning_rate'],\n",
    "    \"end_learning_rate\": 1e-5,\n",
    "    \"lsm\": 0.0,\n",
    "    \"hidden_dropout_prob\": 0.1,\n",
    "    \"use_warmup\": True,\n",
    "    \"use_multi_gpus\": True\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaddcfbc",
   "metadata": {},
   "source": [
    "\n",
    "# Data loader pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a6c6438",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Using custom data configuration default-597f3863485f3654\n",
      "WARNING:datasets.builder:Reusing dataset parquet (/home/jupyter/gogolook/data/cache_data_dir/parquet/default-597f3863485f3654/0.0.0/03dda9603b6ba3760d9d286684a3d7d8ec00448c154f765795485acd3229ecba)\n"
     ]
    }
   ],
   "source": [
    "torch.multiprocessing.set_start_method('spawn')\n",
    "#datasets.config.IN_MEMORY_MAX_SIZE\n",
    "@dataclass(eq=False)\n",
    "class GenerateDatasets: \n",
    "    #files_list: str = field(\n",
    "    #    default=None, metadata={\"help\": \"The files list of data path\"}\n",
    "    #)\n",
    "    data_path: str = field(\n",
    "        default=None, metadata={\"help\": \"The prefix path of files location\"}\n",
    "    )\n",
    "    regex_file_format: str = field(\n",
    "        default='*.parquet', metadata={\"help\": \"The files format.\"}\n",
    "    )\n",
    "    batch_size: int = field(\n",
    "        default=128, metadata={\"help\": \"Batch size\"}\n",
    "    )\n",
    "    is_training: bool = field(\n",
    "        default=True, metadata={\"help\": \"Is use training mode to create data pipeline\"}\n",
    "    )\n",
    "    device: str = field(\n",
    "        default='cpu', metadata={\"help\": \"Which device to use [cpu, cuda]\"}\n",
    "    )\n",
    "    cache_data_path: str = field(\n",
    "        default=None, metadata={\"help\": \"The path to cache data.\"}\n",
    "    )\n",
    "        \n",
    "    def __post_init__(self):\n",
    "        self.get_files_list = glob.glob(os.path.join(str(self.data_path), self.regex_file_format))\n",
    "        #self.get_files_list = '/home/jupyter/gogolook/data/jp_data/valid_pretraining_data/valid_all-maxseq512_BG.parquet'\n",
    "        self.encoding_columns = ['input_ids', 'token_type_ids', 'attention_mask']\n",
    "        self.target_columns = ['masked_lm_labels', 'next_sentence_labels']\n",
    "        \n",
    "    def __call__(self, **kwargs):\n",
    "        # data 已經存在 device (cuda) 裡，所以再用 pin_memory 會出現 error\n",
    "        # RuntimeError: cannot pin 'torch.cuda.LongTensor' only dense CPU tensors can be pinned        \n",
    "        dataset = load_dataset('parquet', data_files=self.get_files_list, cache_dir=self.cache_data_path, split='train')\n",
    "        dataset.set_format(type='torch', columns=self.encoding_columns + self.target_columns) # , device=self.device\n",
    "        #dataset = dataset.rename_column(self.target_column, 'labels')\n",
    "        if self.is_training:\n",
    "            drop_last = True\n",
    "        else: \n",
    "            drop_last = False\n",
    "            \n",
    "        dataloader = torch.utils.data.DataLoader(\n",
    "            dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            pin_memory=True,\n",
    "            shuffle=True,\n",
    "            drop_last=drop_last,\n",
    "            num_workers=multiprocessing.cpu_count())\n",
    "        return dataloader\n",
    "        \n",
    "\n",
    "get_train_dataset = GenerateDatasets(\n",
    "    data_path=training_data_path,\n",
    "    batch_size=model_config['batch_size'],\n",
    "    is_training=True,\n",
    "    device=device,\n",
    "    cache_data_path=cache_data_path)\n",
    "\n",
    "get_valid_dataset = GenerateDatasets(\n",
    "    data_path=training_data_path,\n",
    "    batch_size=model_config['batch_size'],\n",
    "    is_training=False,\n",
    "    device=device,\n",
    "    cache_data_path=cache_data_path)\n",
    "\n",
    "train_dataloader = get_train_dataset()\n",
    "#val_dataloader = get_valid_dataset()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e7f914c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/jupyter/gogolook/data/jp_data/train_pretraining_data/train_all-maxseq512_AC.parquet',\n",
       " '/home/jupyter/gogolook/data/jp_data/train_pretraining_data/train_all-maxseq512_AU.parquet',\n",
       " '/home/jupyter/gogolook/data/jp_data/train_pretraining_data/train_all-maxseq512_BA.parquet',\n",
       " '/home/jupyter/gogolook/data/jp_data/train_pretraining_data/train_all-maxseq512_AJ.parquet',\n",
       " '/home/jupyter/gogolook/data/jp_data/train_pretraining_data/train_all-maxseq512_AN.parquet',\n",
       " '/home/jupyter/gogolook/data/jp_data/train_pretraining_data/train_all-maxseq512_AR.parquet',\n",
       " '/home/jupyter/gogolook/data/jp_data/train_pretraining_data/train_all-maxseq512_AQ.parquet',\n",
       " '/home/jupyter/gogolook/data/jp_data/train_pretraining_data/train_all-maxseq512_BD.parquet',\n",
       " '/home/jupyter/gogolook/data/jp_data/train_pretraining_data/train_all-maxseq512_AA.parquet',\n",
       " '/home/jupyter/gogolook/data/jp_data/train_pretraining_data/train_all-maxseq512_BF.parquet',\n",
       " '/home/jupyter/gogolook/data/jp_data/train_pretraining_data/train_all-maxseq512_AV.parquet',\n",
       " '/home/jupyter/gogolook/data/jp_data/train_pretraining_data/train_all-maxseq512_AM.parquet',\n",
       " '/home/jupyter/gogolook/data/jp_data/train_pretraining_data/train_all-maxseq512_AI.parquet',\n",
       " '/home/jupyter/gogolook/data/jp_data/train_pretraining_data/train_all-maxseq512_AW.parquet',\n",
       " '/home/jupyter/gogolook/data/jp_data/train_pretraining_data/train_all-maxseq512_AS.parquet',\n",
       " '/home/jupyter/gogolook/data/jp_data/train_pretraining_data/train_all-maxseq512_BB.parquet',\n",
       " '/home/jupyter/gogolook/data/jp_data/train_pretraining_data/train_all-maxseq512_AK.parquet',\n",
       " '/home/jupyter/gogolook/data/jp_data/train_pretraining_data/train_all-maxseq512_BE.parquet',\n",
       " '/home/jupyter/gogolook/data/jp_data/train_pretraining_data/train_all-maxseq512_BG.parquet',\n",
       " '/home/jupyter/gogolook/data/jp_data/train_pretraining_data/train_all-maxseq512_AB.parquet',\n",
       " '/home/jupyter/gogolook/data/jp_data/train_pretraining_data/train_all-maxseq512_AE.parquet',\n",
       " '/home/jupyter/gogolook/data/jp_data/train_pretraining_data/train_all-maxseq512_AZ.parquet',\n",
       " '/home/jupyter/gogolook/data/jp_data/train_pretraining_data/train_all-maxseq512_AY.parquet',\n",
       " '/home/jupyter/gogolook/data/jp_data/train_pretraining_data/train_all-maxseq512_AX.parquet',\n",
       " '/home/jupyter/gogolook/data/jp_data/train_pretraining_data/train_all-maxseq512_AO.parquet',\n",
       " '/home/jupyter/gogolook/data/jp_data/train_pretraining_data/train_all-maxseq512_AH.parquet',\n",
       " '/home/jupyter/gogolook/data/jp_data/train_pretraining_data/train_all-maxseq512_BC.parquet',\n",
       " '/home/jupyter/gogolook/data/jp_data/train_pretraining_data/train_all-maxseq512_AT.parquet',\n",
       " '/home/jupyter/gogolook/data/jp_data/train_pretraining_data/train_all-maxseq512_AD.parquet',\n",
       " '/home/jupyter/gogolook/data/jp_data/train_pretraining_data/train_all-maxseq512_AP.parquet',\n",
       " '/home/jupyter/gogolook/data/jp_data/train_pretraining_data/train_all-maxseq512_AG.parquet',\n",
       " '/home/jupyter/gogolook/data/jp_data/train_pretraining_data/train_all-maxseq512_AF.parquet',\n",
       " '/home/jupyter/gogolook/data/jp_data/train_pretraining_data/train_all-maxseq512_AL.parquet']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_train_dataset.get_files_list\n",
    "#get_valid_dataset.get_files_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ff5d1868",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    2,  7482, 28511,  ...,    31,     4,     3],\n",
       "         [    2,   265,     5,  ...,     0,     0,     0],\n",
       "         [    2,  5574,  1941,  ...,    10,     8,     3],\n",
       "         ...,\n",
       "         [    2,   106,     6,  ...,  5966,     9,     3],\n",
       "         [    2, 11752,  5609,  ...,     0,     0,     0],\n",
       "         [    2,  2413,   225,  ...,     0,     0,     0]]),\n",
       " 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]]),\n",
       " 'token_type_ids': tensor([[0, 0, 0,  ..., 1, 1, 1],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 1, 1, 1],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 1, 1, 1],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]]),\n",
       " 'masked_lm_labels': tensor([[-100, -100, -100,  ..., -100,    8, -100],\n",
       "         [   0, -100, -100,  ..., -100, -100, -100],\n",
       "         [-100, -100, -100,  ..., -100, -100, -100],\n",
       "         ...,\n",
       "         [-100, -100, -100,  ..., -100, -100, -100],\n",
       "         [-100, -100, -100,  ..., -100, -100, -100],\n",
       "         [   0, -100, -100,  ..., -100, -100, -100]]),\n",
       " 'next_sentence_labels': tensor([1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,\n",
       "         0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1,\n",
       "         0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0])}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c7d36320",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of AlbertForPreTraining were not initialized from the model checkpoint at voidful/albert_chinese_tiny and are newly initialized: ['sop_classifier.classifier.weight', 'sop_classifier.classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(32000, 128)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if model_config['use_warmup']:\n",
    "    model_config['warmup_steps'] = int(len(train_dataloader) * model_config['epochs'] * 0.1)\n",
    "    model_config['decay_steps'] = len(train_dataloader) * model_config['epochs']\n",
    "else:\n",
    "    model_config['warmup_steps'] = None \n",
    "    model_config['decay_steps'] = None\n",
    "model_config['training_steps'] = len(train_dataloader)\n",
    "\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "albert_config = AlbertConfig.from_json_file(albert_zh_path / 'albert_config' / 'albert_config_tiny.json')\n",
    "pretrained_model_name_or_path = 'voidful/albert_chinese_tiny'\n",
    "albert_pretrain_model = AlbertForPreTraining.from_pretrained(\n",
    "    pretrained_model_name_or_path, \n",
    "    config=albert_config,             \n",
    "    cache_dir=cache_models_path)\n",
    "albert_pretrain_model.resize_token_embeddings(corpus_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bc525d5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AlbertConfig {\n",
       "  \"_name_or_path\": \"voidful/albert_chinese_tiny\",\n",
       "  \"attention_probs_dropout_prob\": 0.0,\n",
       "  \"bos_token_id\": 2,\n",
       "  \"classifier_dropout_prob\": 0.1,\n",
       "  \"directionality\": \"bidi\",\n",
       "  \"embedding_size\": 128,\n",
       "  \"eos_token_id\": 3,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.0,\n",
       "  \"hidden_size\": 312,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"inner_group_num\": 1,\n",
       "  \"intermediate_size\": 1248,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"ln_type\": \"postln\",\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"albert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_groups\": 1,\n",
       "  \"num_hidden_layers\": 4,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"pooler_fc_size\": 768,\n",
       "  \"pooler_num_attention_heads\": 12,\n",
       "  \"pooler_num_fc_layers\": 3,\n",
       "  \"pooler_size_per_head\": 128,\n",
       "  \"pooler_type\": \"first_token_transform\",\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.6.0\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"vocab_size\": 32000\n",
       "}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "albert_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ed8b1770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's use 4 GPUs!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): AlbertForPreTraining(\n",
       "    (albert): AlbertModel(\n",
       "      (embeddings): AlbertEmbeddings(\n",
       "        (word_embeddings): Embedding(32000, 128)\n",
       "        (position_embeddings): Embedding(512, 128)\n",
       "        (token_type_embeddings): Embedding(2, 128)\n",
       "        (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (encoder): AlbertTransformer(\n",
       "        (embedding_hidden_mapping_in): Linear(in_features=128, out_features=312, bias=True)\n",
       "        (albert_layer_groups): ModuleList(\n",
       "          (0): AlbertLayerGroup(\n",
       "            (albert_layers): ModuleList(\n",
       "              (0): AlbertLayer(\n",
       "                (full_layer_layer_norm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
       "                (attention): AlbertAttention(\n",
       "                  (query): Linear(in_features=312, out_features=312, bias=True)\n",
       "                  (key): Linear(in_features=312, out_features=312, bias=True)\n",
       "                  (value): Linear(in_features=312, out_features=312, bias=True)\n",
       "                  (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (output_dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (dense): Linear(in_features=312, out_features=312, bias=True)\n",
       "                  (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
       "                )\n",
       "                (ffn): Linear(in_features=312, out_features=1248, bias=True)\n",
       "                (ffn_output): Linear(in_features=1248, out_features=312, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (pooler): Linear(in_features=312, out_features=312, bias=True)\n",
       "      (pooler_activation): Tanh()\n",
       "    )\n",
       "    (predictions): AlbertMLMHead(\n",
       "      (LayerNorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (dense): Linear(in_features=312, out_features=128, bias=True)\n",
       "      (decoder): Linear(in_features=128, out_features=32000, bias=True)\n",
       "    )\n",
       "    (sop_classifier): AlbertSOPHead(\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (classifier): Linear(in_features=312, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if model_config[\"use_multi_gpus\"]:\n",
    "    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    device_ids = [idx for idx in range(torch.cuda.device_count())]\n",
    "    albert_pretrain_model = nn.DataParallel(albert_pretrain_model, device_ids=device_ids)\n",
    "albert_pretrain_model.to(device)   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d4e62d",
   "metadata": {},
   "source": [
    "# Define Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d4d3281d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'params': ['module.albert.embeddings.word_embeddings.weight',\n",
       "   'module.albert.embeddings.position_embeddings.weight',\n",
       "   'module.albert.embeddings.token_type_embeddings.weight',\n",
       "   'module.albert.embeddings.LayerNorm.weight',\n",
       "   'module.albert.encoder.embedding_hidden_mapping_in.weight',\n",
       "   'module.albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm.weight',\n",
       "   'module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query.weight',\n",
       "   'module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key.weight',\n",
       "   'module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value.weight',\n",
       "   'module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.dense.weight',\n",
       "   'module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm.weight',\n",
       "   'module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn.weight',\n",
       "   'module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output.weight',\n",
       "   'module.albert.pooler.weight',\n",
       "   'module.predictions.LayerNorm.weight',\n",
       "   'module.predictions.dense.weight',\n",
       "   'module.sop_classifier.classifier.weight'],\n",
       "  'weight_decay_rate': 0.01},\n",
       " {'params': ['module.albert.embeddings.LayerNorm.bias',\n",
       "   'module.albert.encoder.embedding_hidden_mapping_in.bias',\n",
       "   'module.albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm.bias',\n",
       "   'module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query.bias',\n",
       "   'module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key.bias',\n",
       "   'module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value.bias',\n",
       "   'module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.dense.bias',\n",
       "   'module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm.bias',\n",
       "   'module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn.bias',\n",
       "   'module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output.bias',\n",
       "   'module.albert.pooler.bias',\n",
       "   'module.predictions.bias',\n",
       "   'module.predictions.LayerNorm.bias',\n",
       "   'module.predictions.dense.bias',\n",
       "   'module.sop_classifier.classifier.bias'],\n",
       "  'weight_decay_rate': 0.0}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_params = list(albert_pretrain_model.named_parameters())\n",
    "optimizer_grounded_parameters_by_name = [\n",
    "    {'params': [n for n, p in model_params if not any(nd in n for nd in ['bias', 'gamma', 'beta'])], \n",
    "     'weight_decay_rate': 1e-2  },\n",
    "    {'params': [n for n, p in model_params if any(nd in n for nd in ['bias', 'gamma', 'beta'])], \n",
    "     'weight_decay_rate': 0.0 }\n",
    "]\n",
    "\n",
    "optimizer_grounded_parameters_by_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b78ea12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = list(albert_pretrain_model.named_parameters())\n",
    "\n",
    "optimizer_grounded_parameters = [\n",
    "    {'params': [p for n, p in model_params if not any(nd in n for nd in ['bias', 'gamma', 'beta'])], \n",
    "     'weight_decay_rate': 1e-2  },\n",
    "    {'params': [p for n, p in model_params if any(nd in n for nd in ['bias', 'gamma', 'beta'])], \n",
    "     'weight_decay_rate': 0.0 }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "33cdc857",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "\n",
    "class PolynomialDecay(_LRScheduler):\n",
    "    def __init__(self, optimizer, decay_steps, end_learning_rate=0.0001, power=0.5, cycle=False, last_epoch=-1, verbose=False):\n",
    "        if decay_steps <= 1.:\n",
    "            raise ValueError('max_decay_steps should be greater than 1.')            \n",
    "        self.decay_steps = decay_steps\n",
    "        self.end_learning_rate = end_learning_rate\n",
    "        self.power = power\n",
    "        self.cycle = cycle\n",
    "        super(PolynomialDecay, self).__init__(optimizer, last_epoch, verbose)\n",
    "    \n",
    "    def get_lr(self):\n",
    "        if not self._get_lr_called_within_step:\n",
    "            warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
    "                          \"please use `get_last_lr()`.\")\n",
    "        #dtype = initial_learning_rate.dtype\n",
    "        #end_learning_rate = math_ops.cast(self.end_learning_rate, dtype)\n",
    "        #power = math_ops.cast(self.power, dtype)\n",
    "        #global_step_recomp = math_ops.cast(step, dtype)\n",
    "        #decay_steps_recomp = math_ops.cast(self.decay_steps, dtype)\n",
    "        global_step_recomp = self.last_epoch\n",
    "        decay_steps_recomp = self.decay_steps\n",
    "        \n",
    "        if self.cycle:\n",
    "            if global_step_recomp == 0:\n",
    "                multiplier = 1.0 \n",
    "            else:\n",
    "                multiplier = math.ceil(global_step_recomp / self.decay_steps)\n",
    "            decay_steps_recomp = decay_steps_recomp * multiplier\n",
    "        else:\n",
    "            global_step_recomp = min(global_step_recomp, decay_steps_recomp)\n",
    "            \n",
    "        p = global_step_recomp / decay_steps_recomp\n",
    "        ic(self.last_epoch, optimizer.param_groups[0]['lr'], p)\n",
    "        return [((group['lr'] - self.end_learning_rate) * math.pow(1 - p, self.power) + self.end_learning_rate) for group in self.optimizer.param_groups]\n",
    "    \n",
    "    def _get_closed_form_lr(self):\n",
    "        return [(base_lr - self.end_learning_rate) * math.pow(1 - p, self.power) + self.end_learning_rate for base_lr in self.base_lrs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0c1d3ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(\n",
    "    params=optimizer_grounded_parameters,\n",
    "    lr=model_config[\"init_learning_rate\"],\n",
    "    betas=(0.9, 0.98),\n",
    "    weight_decay=0.0,\n",
    "    eps=1e-6)\n",
    "\n",
    "#optimizer = optim.SGD(sms_model.parameters(), lr=model_config['init_learning_rate'], weight_decay=1e-4)\n",
    "#optimizer = optim.SGD(filter(lambda p: p.requires_grad, sms_model.parameters()), lr=model_config['init_learning_rate'], weight_decay=1e-4)\n",
    "\n",
    "#scheduler = CyclicLR(\n",
    "#    optimizer, \n",
    "#    base_lr=1e-5,\n",
    "#    max_lr=model_config['init_learning_rate'],\n",
    "#    step_size_up=model_config['training_steps'] * 1,\n",
    "#    mode='triangular2',\n",
    "#    scale_mode='cycle',\n",
    "#    cycle_momentum=False\n",
    "#)\n",
    "\n",
    "\n",
    "#if model_config[\"use_multi_gpus\"]:\n",
    "#optimizer = nn.DataParallel(optimizer, device_ids=device_ids)\n",
    "\n",
    " \n",
    "scheduler = LinearWarmupCosineAnnealingLR(\n",
    "    optimizer, \n",
    "    warmup_epochs=model_config['warmup_steps'], \n",
    "    max_epochs=model_config['training_steps'] * model_config['epochs'], \n",
    "    eta_min=model_config[\"end_learning_rate\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f6bbeda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Mertice\n",
    "from torchmetrics import MetricCollection\n",
    "metric_collection = MetricCollection([\n",
    "    torchmetrics.Accuracy(num_classes=2, average='macro', multiclass=True, dist_sync_on_step=True, mdmc_average='global').to(device),\n",
    "    torchmetrics.Precision(num_classes=2, average='macro', multiclass=True, dist_sync_on_step=True, mdmc_average='global').to(device),\n",
    "    torchmetrics.Recall(num_classes=2, average='macro', multiclass=True, dist_sync_on_step=True, mdmc_average='global').to(device),\n",
    "    torchmetrics.F1(num_classes=2, average='macro', multiclass=True, dist_sync_on_step=True, mdmc_average='global').to(device)\n",
    "], prefix='Train_')\n",
    "\n",
    "val_metric_collection = MetricCollection([\n",
    "    torchmetrics.Accuracy(num_classes=2, average='macro', multiclass=True, dist_sync_on_step=True, mdmc_average='global').to(device),\n",
    "    torchmetrics.Precision(num_classes=2, average='macro', multiclass=True, dist_sync_on_step=True, mdmc_average='global').to(device),\n",
    "    torchmetrics.Recall(num_classes=2, average='macro', multiclass=True, dist_sync_on_step=True, mdmc_average='global').to(device),\n",
    "    torchmetrics.F1(num_classes=2, average='macro', multiclass=True, dist_sync_on_step=True, mdmc_average='global').to(device),\n",
    "], prefix='Val_')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c36c521",
   "metadata": {},
   "source": [
    "# Training model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72fa6486",
   "metadata": {},
   "source": [
    "### Init wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f37b2d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33myuyuliao20\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.1 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.11.1<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">jp-pretrain-model_baseline_20210902155112</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/yuyuliao20/jp-pretrain-model\" target=\"_blank\">https://wandb.ai/yuyuliao20/jp-pretrain-model</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/yuyuliao20/jp-pretrain-model/runs/3s8gyuuo\" target=\"_blank\">https://wandb.ai/yuyuliao20/jp-pretrain-model/runs/3s8gyuuo</a><br/>\n",
       "                Run data is saved locally in <code>/home/jupyter/gogolook/albert-japanese/notebook/wandb/run-20210902_075124-3s8gyuuo</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h1>Run(3s8gyuuo)</h1><iframe src=\"https://wandb.ai/yuyuliao20/jp-pretrain-model/runs/3s8gyuuo\" style=\"border:none;width:100%;height:400px\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f7042fd2c10>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if model_config['use_warmup']:\n",
    "    model_config['warmup_steps'] = int(len(train_dataloader) * model_config['epochs'] * 0.1)\n",
    "    model_config['decay_steps'] = len(train_dataloader) * model_config['epochs']\n",
    "else:\n",
    "    model_config['warmup_steps'] = None \n",
    "    model_config['decay_steps'] = None\n",
    "model_config['training_steps'] = len(train_dataloader)\n",
    "\n",
    "wandb.init(\n",
    "    project=project,\n",
    "    group=group_tag,\n",
    "    job_type=job_type,\n",
    "    name=run_id,\n",
    "    notes=method_tag,\n",
    "    tags=addition_tag,\n",
    "    sync_tensorboard=False,\n",
    "    config={**model_config},\n",
    "    reinit=True    \n",
    ")\n",
    "\n",
    "wandb_config = wandb.config\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b82bc48",
   "metadata": {},
   "source": [
    "### Simple test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7ce13b8b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntorch.cuda.empty_cache()\\nprefix = \\'train\\'\\nfor epoch in tqdm(range(10)): # model_config[\\'epochs\\']\\n    start_time = time.time()    \\n    train_batch_loss = 0\\n    val_batch_loss = 0    \\n    \\n    # Training Step\\n    albert_pretrain_model = albert_pretrain_model.train()\\n    for step, train_batch in tqdm(enumerate(train_dataloader), \\n                                  dynamic_ncols=False, \\n                                  bar_format=\"{n_fmt}/{total_fmt}{bar} ETA: {remaining}s - {desc}\", \\n                                  total=len(train_dataloader),\\n                                  leave=True, \\n                                  unit=\\'steps\\'):        \\n        input_ids = train_batch[\\'input_ids\\'].to(device)\\n        attention_mask = train_batch[\\'attention_mask\\'].to(device)\\n        token_type_ids = train_batch[\\'token_type_ids\\'].to(device)\\n\\n        mlm_labels = train_batch[\\'masked_lm_labels\\'].to(device)\\n        sop_labels = train_batch[\\'next_sentence_labels\\'].to(device)\\n        \\n        outputs = albert_pretrain_model(\\n            input_ids=input_ids,\\n            attention_mask=attention_mask,\\n            token_type_ids=token_type_ids,\\n            labels=mlm_labels,\\n            sentence_order_label=sop_labels\\n        )\\n                    \\n        loss = outputs.loss.mean()\\n        perplexity  = torch.exp(loss)\\n        \\n        optimizer.zero_grad()\\n        loss.backward()\\n        if model_config[\"use_multi_gpus\"]:\\n            optimizer.module.step()\\n        else:\\n            optimizer.step()\\n            \\n        wandb.log({\\n            \"loss\": loss,\\n            \"perplexity\": perplexity,            \\n        }, step=step)\\n        \\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "torch.cuda.empty_cache()\n",
    "prefix = 'train'\n",
    "for epoch in tqdm(range(10)): # model_config['epochs']\n",
    "    start_time = time.time()    \n",
    "    train_batch_loss = 0\n",
    "    val_batch_loss = 0    \n",
    "    \n",
    "    # Training Step\n",
    "    albert_pretrain_model = albert_pretrain_model.train()\n",
    "    for step, train_batch in tqdm(enumerate(train_dataloader), \n",
    "                                  dynamic_ncols=False, \n",
    "                                  bar_format=\"{n_fmt}/{total_fmt}{bar} ETA: {remaining}s - {desc}\", \n",
    "                                  total=len(train_dataloader),\n",
    "                                  leave=True, \n",
    "                                  unit='steps'):        \n",
    "        input_ids = train_batch['input_ids'].to(device)\n",
    "        attention_mask = train_batch['attention_mask'].to(device)\n",
    "        token_type_ids = train_batch['token_type_ids'].to(device)\n",
    "\n",
    "        mlm_labels = train_batch['masked_lm_labels'].to(device)\n",
    "        sop_labels = train_batch['next_sentence_labels'].to(device)\n",
    "        \n",
    "        outputs = albert_pretrain_model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            labels=mlm_labels,\n",
    "            sentence_order_label=sop_labels\n",
    "        )\n",
    "                    \n",
    "        loss = outputs.loss.mean()\n",
    "        perplexity  = torch.exp(loss)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        if model_config[\"use_multi_gpus\"]:\n",
    "            optimizer.module.step()\n",
    "        else:\n",
    "            optimizer.step()\n",
    "            \n",
    "        wandb.log({\n",
    "            \"loss\": loss,\n",
    "            \"perplexity\": perplexity,            \n",
    "        }, step=step)\n",
    "        \n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfff2dd5",
   "metadata": {},
   "source": [
    "### Define training stepsm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "03a73e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_step(model, input_ids, attention_mask, token_type_ids, mlm_labels, sop_labels, scaler, use_multi_gpus=False):\n",
    "    with autocast():\n",
    "        # Forward pass\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            labels=mlm_labels,\n",
    "            sentence_order_label=sop_labels\n",
    "        )\n",
    "        assert outputs.prediction_logits.dtype is torch.float16\n",
    "        \n",
    "        loss = outputs.loss.mean()\n",
    "        assert loss.dtype is torch.float32\n",
    "    # Backward pass\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    scaler.scale(loss).backward()\n",
    "    #if use_multi_gpus:\n",
    "    #    scaler.step(optimizer.module)\n",
    "    #else:\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "    #torch.nn.utils.clip_grad_norm_(optimizer_grounded_parameters, max_norm=0.5)\n",
    "    #if epoch > swa_start:\n",
    "    #    swa_model.update_parameters(model)\n",
    "    #    swa_scheduler.step()\n",
    "    #else:\n",
    "    #    scheduler.step()\n",
    "    return loss\n",
    "\n",
    "@torch.no_grad()\n",
    "def validataion_step(model, input_ids, attention_mask, token_type_ids, mlm_labels, sop_labels):    \n",
    "    with autocast():\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            labels=mlm_labels,\n",
    "            sentence_order_label=sop_labels\n",
    "        )\n",
    "        loss = outputs.loss.mean()\n",
    "    return loss\n",
    "\n",
    "@torch.no_grad()\n",
    "def testing_step(model, dataset_inputs):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b30da57",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32462da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RUN ID]: jp-pretrain-model_baseline_20210902155112\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6658f03761b94806a6730261ff424cc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12bacdc56b924f07879a4f9dc0786410",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=131487.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('[RUN ID]: {}'.format(run_id))\n",
    "torch.cuda.empty_cache()\n",
    "use_epoch_tracking = False\n",
    "use_step_tracking = True\n",
    "#wandb.watch(sms_model, log=\"all\", log_freq=1000)        \n",
    "def show_logs(loss, step, is_epoch=False, prefix='Train', **kwargs):\n",
    "    loss = float(loss)\n",
    "    if is_epoch:\n",
    "        wandb.log({\"epoch\": step, f\"{prefix}_loss\": loss}, step=step)\n",
    "    else:\n",
    "        wandb.log({f\"{prefix}_step_loss\": loss}, step=step)\n",
    "        #print(f\"{prefix} loss after \" + str(example_ct).zfill(5) + f\" examples: {loss:.3f}\")\n",
    "    if \"perplexity\" in kwargs.keys():\n",
    "        wandb.log({f\"{prefix}_perplexity\": kwargs[\"perplexity\"]}, step=step)\n",
    "\n",
    "# Creates a GradScaler once at the beginning of training.\n",
    "scaler = GradScaler()\n",
    "\n",
    "for epoch in tqdm(range(model_config['epochs'])): # model_config['epochs']\n",
    "    start_time = time.time()    \n",
    "    train_batch_loss = 0\n",
    "    valid_batch_loss = 0  \n",
    "    \n",
    "    train_perplexity = 0\n",
    "    valid_perplexity = 0\n",
    "    \n",
    "    # Training Step\n",
    "    albert_pretrain_model = albert_pretrain_model.train()\n",
    "    for step, train_batch in tqdm(enumerate(train_dataloader), \n",
    "                                  dynamic_ncols=False, \n",
    "                                  bar_format=\"{n_fmt}/{total_fmt}{bar} ETA: {remaining}s - {desc}\", \n",
    "                                  total=len(train_dataloader),\n",
    "                                  leave=True, \n",
    "                                  unit='steps'):        \n",
    "        input_ids = train_batch['input_ids'].to(device)\n",
    "        attention_mask = train_batch['attention_mask'].to(device)\n",
    "        token_type_ids = train_batch['token_type_ids'].to(device)        \n",
    "        mlm_labels = train_batch['masked_lm_labels'].to(device)\n",
    "        sop_labels = train_batch['next_sentence_labels'].to(device)\n",
    "        \n",
    "        train_loss = training_step(\n",
    "            model=albert_pretrain_model, \n",
    "            input_ids=input_ids, \n",
    "            attention_mask=attention_mask, \n",
    "            token_type_ids=token_type_ids, \n",
    "            mlm_labels=mlm_labels, \n",
    "            sop_labels=sop_labels,\n",
    "            scaler=scaler,\n",
    "            use_multi_gpus=model_config[\"use_multi_gpus\"]\n",
    "        )\n",
    "        scheduler.step()\n",
    "        train_batch_loss += train_loss.item()\n",
    "        train_perplexity += torch.exp(train_loss)\n",
    "        \n",
    "        #if model_config[\"use_multi_gpus\"]:\n",
    "        #    last_lr = optimizer.module.param_groups[0]['lr']\n",
    "        #else:\n",
    "        #last_lr = optimizer.param_groups[0]['lr']\n",
    "        last_lr = scheduler.optimizer.param_groups[0][\"lr\"]\n",
    "          \n",
    "        if use_step_tracking:\n",
    "            record_step = (step + 1) * (epoch + 1)\n",
    "            wandb.log({'learning_rate': last_lr}, step=record_step)\n",
    "            show_logs(\n",
    "                train_batch_loss / record_step, \n",
    "                record_step, \n",
    "                perplexity=train_perplexity.item() / record_step)\n",
    "    \n",
    "    if use_epoch_tracking:\n",
    "        train_epoch_loss = train_batch_loss / step\n",
    "        wandb.log({'learning_rate': last_lr}, step=epoch)\n",
    "        show_log(train_epoch_loss, epoch, is_epoch=True)  \n",
    "        #train_metric_records = metric_collection.compute()\n",
    "        #wandb.log(train_metric_records, step=epoch)\n",
    "    \n",
    "    # Validation Step\n",
    "    albert_pretrain_model = albert_pretrain_model.eval()\n",
    "    for step, valid_batch in tqdm(enumerate(val_dataloader), \n",
    "                                dynamic_ncols=False, \n",
    "                                bar_format=\"{n_fmt}/{total_fmt}{bar} ETA: {remaining}s - {desc}\", \n",
    "                                total=len(val_dataloader),\n",
    "                                leave=True, \n",
    "                                unit='steps'):            \n",
    "        input_ids = valid_batch['input_ids'].to(device)\n",
    "        attention_mask = valid_batch['attention_mask'].to(device)\n",
    "        token_type_ids = valid_batch['token_type_ids'].to(device)\n",
    "        mlm_labels = valid_batch['masked_lm_labels'].to(device)\n",
    "        sop_labels = valid_batch['next_sentence_labels'].to(device)\n",
    "            \n",
    "        valid_loss = validataion_step(\n",
    "            model=albert_pretrain_model, \n",
    "            input_ids=input_ids, \n",
    "            attention_mask=attention_mask, \n",
    "            token_type_ids=token_type_ids, \n",
    "            mlm_labels=mlm_labels,\n",
    "            sop_labels=sop_labels\n",
    "        )\n",
    "        valid_batch_loss += valid_loss.item()\n",
    "        valid_perplexity += torch.exp(valid_loss)\n",
    "        \n",
    "        if use_step_tracking:\n",
    "            record_step = (step + 1) * (epoch + 1)\n",
    "            wandb.log({'learning_rate': last_lr}, step=record_step)\n",
    "            show_logs(\n",
    "                valid_batch_loss / record_step, \n",
    "                record_step, \n",
    "                prefix='valid', \n",
    "                perplexity=valid_perplexity.item() / record_step)\n",
    "            \n",
    "        #sk_metrics = sklearn_metrics(val_outputs, labels, 'train')\n",
    "        #ic(sk_metrics)\n",
    "        #ic(val_metric_collection(outputs, labels).compute())\n",
    "        #ic(val_metric_collection(outputs, labels))\n",
    "        \n",
    "    if use_epoch_tracking:        \n",
    "        valid_epoch_loss = valid_batch_loss / step \n",
    "        show_logs(valid_epoch_loss, epoch, is_epoch=True, prefix='Val')\n",
    "        #val_metric_records = val_metric_collection.compute()\n",
    "        #wandb.log(val_metric_records, step=epoch)\n",
    "    \n",
    "    loss_template = (\"Epoch {}/{} - {:.0f}s {:.0f}ms/step - lr:{:} - loss: {:.6f} - val_loss: {:.6f}\")  \n",
    "    #metrics_template = (\n",
    "    #    \"\"\"\n",
    "    #    categorical_accuracy: {:.4f} - f1_score: {:.4f} - multi_precision: {:.4f} - multi_recall: {:.4f}\n",
    "    #    val_categorical_accuracy: {:.4f} -  val_f1_score: {:.4f} - val_multi_precision: {:.4f} - val_multi_recall: {:.4f}\n",
    "    #    \"\"\"\n",
    "    #)\n",
    "    end_time = time.time()\n",
    "    each_steps_compute_time = (end_time - start_time)\n",
    "    print(loss_template.format(\n",
    "        epoch,\n",
    "        model_config['epochs'], \n",
    "        each_steps_compute_time,\n",
    "        each_steps_compute_time * 1000 / model_config['training_steps'],\n",
    "        last_lr,\n",
    "        train_epoch_loss,\n",
    "        val_epoch_loss)\n",
    "    )\n",
    "\n",
    "    #print(metrics_template.format(\n",
    "    #    train_metric_records['Train_Accuracy'],\n",
    "    #    train_metric_records['Train_F1'],\n",
    "    #    train_metric_records['Train_Precision'],\n",
    "    #    train_metric_records['Train_Recall'],\n",
    "    #    val_metric_records['Val_Accuracy'],\n",
    "    #    val_metric_records['Val_F1'],\n",
    "    #    val_metric_records['Val_Precision'],\n",
    "    #    val_metric_records['Val_Recall']\n",
    "    #))\n",
    "    \n",
    "    if use_epoch_tracking:\n",
    "        metric_collection.reset()\n",
    "        val_metric_collection.reset()\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6563dbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00bd3d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert 1 == 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91367f63",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a910ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import math\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "from torch import nn\n",
    "from torch import cuda\n",
    "from torch import optim\n",
    "from torch.optim.swa_utils import AveragedModel, SWALR\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, CyclicLR\n",
    "from pl_bolts.optimizers.lr_scheduler import LinearWarmupCosineAnnealingLR\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "net = NeuralNetwork()\n",
    "\n",
    "optimizer = optim.SGD(net.parameters(), lr = 1e-2)\n",
    "lambda1 = lambda epoch: 0.2 if epoch % 5 == 0 else 1\n",
    "lambda2 = lambda epoch: 0.2\n",
    "\n",
    "#scheduler = optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda = lambda2)\n",
    "#scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[5,10,15], gamma=0.1)\n",
    "#scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma = 0.9)\n",
    "#scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda step: PolynomialDecay(step))\n",
    "\n",
    "class PolynomialDecay(_LRScheduler):\n",
    "    def __init__(self, optimizer, decay_steps, end_learning_rate=0.0001, power=0.5, cycle=False, last_epoch=-1, verbose=False):\n",
    "        if decay_steps <= 1.:\n",
    "            raise ValueError('max_decay_steps should be greater than 1.')            \n",
    "        self.decay_steps = decay_steps\n",
    "        self.end_learning_rate = end_learning_rate\n",
    "        self.power = power\n",
    "        self.cycle = cycle\n",
    "        super(PolynomialDecay, self).__init__(optimizer, last_epoch, verbose)\n",
    "    \n",
    "    def get_lr(self):\n",
    "        if not self._get_lr_called_within_step:\n",
    "            warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
    "                          \"please use `get_last_lr()`.\")\n",
    "        #dtype = initial_learning_rate.dtype\n",
    "        #end_learning_rate = math_ops.cast(self.end_learning_rate, dtype)\n",
    "        #power = math_ops.cast(self.power, dtype)\n",
    "        #global_step_recomp = math_ops.cast(step, dtype)\n",
    "        #decay_steps_recomp = math_ops.cast(self.decay_steps, dtype)\n",
    "        global_step_recomp = self.last_epoch\n",
    "        decay_steps_recomp = self.decay_steps\n",
    "        \n",
    "        if self.cycle:\n",
    "            if global_step_recomp == 0:\n",
    "                multiplier = 1.0 \n",
    "            else:\n",
    "                multiplier = math.ceil(global_step_recomp / self.decay_steps)\n",
    "            decay_steps_recomp = decay_steps_recomp * multiplier\n",
    "        else:\n",
    "            global_step_recomp = min(global_step_recomp, decay_steps_recomp)\n",
    "            \n",
    "        p = global_step_recomp / decay_steps_recomp\n",
    "        #c(self.last_epoch, optimizer.param_groups[0]['lr'], p)\n",
    "        return [((group['lr'] - self.end_learning_rate) * math.pow(1 - p, self.power) + self.end_learning_rate) for group in self.optimizer.param_groups]\n",
    "    \n",
    "    def _get_closed_form_lr(self):\n",
    "        return [(base_lr - self.end_learning_rate) * math.pow(1 - p, self.power) + self.end_learning_rate for base_lr in self.base_lrs]\n",
    "\n",
    "\n",
    "    \n",
    "def polynomial_decay_scale_fun(global_steps, initial_learning_rate=1e-2, decay_steps=100, power=0.5, end_learning_rate=1e-5, cycle=False):\n",
    "    if cycle:\n",
    "        if global_steps == 0:\n",
    "            multiplier = 1.0 \n",
    "        else:\n",
    "            multiplier = math.ceil(global_steps / decay_steps)\n",
    "            decay_steps = decay_steps * multiplier\n",
    "    else:\n",
    "        global_steps = min(global_steps, decay_steps)\n",
    "    p = global_steps / decay_steps\n",
    "    #ic(global_steps, p)\n",
    "    return (initial_learning_rate - end_learning_rate) * math.pow(1 - p, power) + end_learning_rate\n",
    "    \n",
    "    \n",
    "#optimizer = optim.SGD(net.parameters(), lr=1e-2)\n",
    "optimizer = optim.Adam(net.parameters(), lr=1e-3)\n",
    "#scheduler = PolynomialDecay(optimizer, decay_steps=1000, end_learning_rate=1e-5)\n",
    " \n",
    "scheduler = LinearWarmupCosineAnnealingLR(\n",
    "    optimizer, \n",
    "    warmup_epochs=model_config['warmup_steps'], \n",
    "    max_epochs=model_config['training_steps'] * model_config['epochs'], \n",
    "    eta_min=model_config[\"end_learning_rate\"])\n",
    "\n",
    "#scheduler = optim.lr_scheduler.CyclicLR(\n",
    "#    optimizer, \n",
    "#    base_lr=1e-5,\n",
    "#    max_lr=1e-2,\n",
    "#    step_size_up=20,\n",
    "#    scale_fn=polynomial_decay_scale_fun,\n",
    "#    mode='triangular2',\n",
    "#    scale_mode='cycle',\n",
    "#    cycle_momentum=False)\n",
    "\n",
    "iteration = model_config['epochs']\n",
    "scheduler_lr_list = []\n",
    "for epoch in range(1, iteration):\n",
    "    scheduler.step()\n",
    "    #print(epoch, scheduler.get_last_lr()[0])\n",
    "    scheduler_lr_list.append(scheduler.get_last_lr()[0])\n",
    "\n",
    "plt.xlabel('Training Iterations')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.title(\"CLR - 'triangular' Policy\")\n",
    "plt.plot(range(1, iteration), scheduler_lr_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa670268",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-8.m73",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-8:m73"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
