{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "sporting-quantum",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install --user lightning-bolts -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "exterior-plastic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version:  1.8.0\n",
      "Device available: True\n",
      "Device name: Tesla P100-PCIE-16GB\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import time\n",
    "import os\n",
    "import gc\n",
    "import math\n",
    "import multiprocessing\n",
    "import io\n",
    "import logging \n",
    "import itertools\n",
    "import shutil\n",
    "import pysnooper\n",
    "import warnings\n",
    "import glob\n",
    "import pendulum\n",
    "import random\n",
    "import json\n",
    "import sys\n",
    "import wandb\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import scikitplot as skplt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from icecream import ic\n",
    "from collections import Counter\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from pathlib import Path\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import transformers\n",
    "import torchmetrics\n",
    "from torch import nn\n",
    "from torch import cuda\n",
    "from typing import Dict, List, Tuple\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n",
    "try:\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "except ImportError:\n",
    "    from tensorboardX import SummaryWriter\n",
    "    \n",
    "import pyarrow as pa\n",
    "from datasets import load_dataset\n",
    "from datasets import total_allocated_bytes\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, Optional, Union, List\n",
    "from transformers import (\n",
    "    BertTokenizer, AlbertForPreTraining, AlbertModel, AlbertConfig, BertJapaneseTokenizer, PreTrainedTokenizer, PreTrainedModel\n",
    ")\n",
    "from pl_bolts.optimizers.lr_scheduler import LinearWarmupCosineAnnealingLR\n",
    "\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch import optim\n",
    "from torch.optim.swa_utils import AveragedModel, SWALR\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, CyclicLR\n",
    "from transformers import get_polynomial_decay_schedule_with_warmup    \n",
    "    \n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "matplotlib.use('Agg')\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "seed = 9527\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.WARNING)\n",
    "\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "print('Torch version: ', torch.__version__)\n",
    "print('Device available:', torch.cuda.is_available())\n",
    "print('Device name:', torch.cuda.get_device_name(0))\n",
    "torch.set_printoptions(precision=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "brazilian-chaos",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Logger __main__ (WARNING)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "according-gauge",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom torch.utils.data.distributed import DistributedSampler\\nfrom torch.nn.parallel import DistributedDataParallel as DDP\\ntorch.distributed.init_process_group(backend=\\'nccl\\')\\n\\nlocal_rank = -1\\nif local_rank not in [-1, 0]:\\n    torch.distributed.barrier()  # Make sure only the first process in distributed training will download model & vocab\\n    \\ndef setup(rank, world_size):\\n    os.environ[\\'MASTER_ADDR\\'] = \\'localhost\\'\\n    os.environ[\\'MASTER_PORT\\'] = \\'12355\\'\\n\\n    # initialize the process group\\n    dist.init_process_group(\"gloo\", rank=rank, world_size=world_size)\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "torch.distributed.init_process_group(backend='nccl')\n",
    "\n",
    "local_rank = -1\n",
    "if local_rank not in [-1, 0]:\n",
    "    torch.distributed.barrier()  # Make sure only the first process in distributed training will download model & vocab\n",
    "    \n",
    "def setup(rank, world_size):\n",
    "    os.environ['MASTER_ADDR'] = 'localhost'\n",
    "    os.environ['MASTER_PORT'] = '12355'\n",
    "\n",
    "    # initialize the process group\n",
    "    dist.init_process_group(\"gloo\", rank=rank, world_size=world_size)\n",
    "'''    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "liquid-nickname",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--local_rank'], dest='local_rank', nargs=None, const=None, default=-1, type=<class 'int'>, choices=None, help='For distributed training: local_rank', metavar=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--server_ip'], dest='server_ip', nargs=None, const=None, default='', type=<class 'str'>, choices=None, help='For distant debugging.', metavar=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--server_port'], dest='server_port', nargs=None, const=None, default='', type=<class 'str'>, choices=None, help='For distant debugging.', metavar=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--local_rank\", type=int, default=-1, help=\"For distributed training: local_rank\")\n",
    "parser.add_argument(\"--server_ip\", type=str, default=\"\", help=\"For distant debugging.\")\n",
    "parser.add_argument(\"--server_port\", type=str, default=\"\", help=\"For distant debugging.\")\n",
    "\n",
    "args = parser.parse_args(args=[\n",
    "    '--server_ip', 'localhost',\n",
    "    '--server_port', '12355'\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "unauthorized-pollution",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 202105\n",
    "\n",
    "# main'\n",
    "main_path = Path('/home/jupyter/gogolook')\n",
    "main_cached_path = Path('/home/jupyter/gogolook/data')\n",
    "\n",
    "# general setting\n",
    "main_data_path = main_path / 'data' / 'jp_data' \n",
    "main_model_path = main_path / 'models'\n",
    "tensorboard_path = main_path / 'tensorboard_log_dir'\n",
    "cache_data_path = main_cached_path / 'cache_data_dir'\n",
    "cache_models_path = main_cached_path / 'cache_models_dir'\n",
    "\n",
    "# models\n",
    "albert_zh_path = main_model_path / 'albert_zh'\n",
    "pretrain_model_path = main_model_path / 'jp_pretrain_model'\n",
    "\n",
    "# data\n",
    "regex_file_format = '*.json'\n",
    "data_tag = 'pretraining_data'\n",
    "valid_data_tag = 'pretraining_data'\n",
    "test_data_tag = 'pretraining_data'\n",
    "\n",
    "experiment_total_data_path = main_data_path / f'total_{data_tag}'\n",
    "experiment_train_data_path = main_data_path / f'train_{data_tag}'\n",
    "experiment_valid_data_path = main_data_path / f'valid_{valid_data_tag}'\n",
    "experiment_test_data_path = main_data_path / f'test_{test_data_tag}'\n",
    "\n",
    "training_data_path = experiment_train_data_path\n",
    "validation_data_path = experiment_valid_data_path\n",
    "testing_data_path = experiment_test_data_path\n",
    "\n",
    "\n",
    "for folder in [pretrain_model_path, tensorboard_path]:\n",
    "    if not folder.exists():\n",
    "        folder.mkdir()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "abstract-contamination",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run id is jp-pretrain-model_baseline_20210914151521\n"
     ]
    }
   ],
   "source": [
    "project = 'jp-pretrain-model'\n",
    "project_shortname = 'jp-sms'\n",
    "group_tag = 'experiment' # 1. functional 2. experiment 3. staging 4. production\n",
    "job_type = 'baseline' # 1. baseline 2. optimize 3. hyper-tuning\n",
    "addition_tag = [data_tag, 'pytorch'] # exponential_decay\n",
    "method_tag = 'pretrain' # pretrain / finetune / pretrain_finetune\n",
    "time_tag = pendulum.now(tz='Asia/Taipei').strftime('%Y%m%d%H%M%S')\n",
    "run_id = '{}_{}_{}'.format(project, job_type, time_tag)\n",
    "print('Run id is {}'.format(run_id))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "painted-positive",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] アンパサンド (&、 英語 名 :) と は 並立 助詞 「... と...」 を 意味 する 記号 で ある 。 ラテン語 の の 合 字 で 、 Trebuchet MS フォント で は 、 と 表示 さ れ \" et \" の 合 字 で ある こと が 容易 に わかる 。 [SEP]\n"
     ]
    }
   ],
   "source": [
    "mecab_tokenizer = BertJapaneseTokenizer.from_pretrained(\"cl-tohoku/bert-base-japanese\", word_tokenizer_type=\"mecab\", cache_dir=cache_models_path)\n",
    "# Input Japanese Text\n",
    "line = \"アンパサンド (&、英語名：) とは並立助詞「…と…」を意味する記号である。ラテン語の の合字で、Trebuchet MSフォントでは、と表示され \\\"et\\\" の合字であることが容易にわかる。\"\n",
    "mecab_inputs = mecab_tokenizer(line, return_tensors=\"pt\")\n",
    "print(mecab_tokenizer.decode(mecab_inputs['input_ids'][0]))\n",
    "corpus_size = len(mecab_tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "enhanced-moore",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cl-tohoku/bert-base-japanese': 512,\n",
       " 'cl-tohoku/bert-base-japanese-whole-word-masking': 512,\n",
       " 'cl-tohoku/bert-base-japanese-char': 512,\n",
       " 'cl-tohoku/bert-base-japanese-char-whole-word-masking': 512}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mecab_tokenizer.max_model_input_sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ready-firewall",
   "metadata": {},
   "source": [
    "# Model config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "southwest-favor",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_config = {\n",
    "    \"SGD\": {\n",
    "        \"learning_rate\": 1e-1, \n",
    "        \"end_learning_rate\": 1e-3\n",
    "    },\n",
    "    \"Adam\": {\n",
    "        \"learning_rate\": 1e-3,\n",
    "        \"end_learning_rate\": 1e-5,\n",
    "        \"weight_decay\": 0.01,\n",
    "        \"epsilon\": 1e-8\n",
    "    }, \n",
    "    \"RAdam\": {\n",
    "        \"learning_rate\": 1e-3,\n",
    "        \"end_learning_rate\": 2e-5,\n",
    "        \"weight_decay\": 0.01,\n",
    "        \"epsilon\": 1e-8\n",
    "    }     \n",
    "}\n",
    "\n",
    "model_config = {\n",
    "    \"epochs\": 3,\n",
    "    \"num_train_epochs\": 3,\n",
    "    \"per_gpu_train_batch_size\": 16,\n",
    "    \"per_gpu_eval_batch_size\": 32,\n",
    "    \"batch_size\": 128,\n",
    "    \"max_tokens_length\": 512,\n",
    "    \"threshold\": 0.5,\n",
    "    \"optimizer_method\": \"Adam\",\n",
    "    \"learning_rate\": optimizer_config['Adam']['learning_rate'],\n",
    "    \"end_learning_rate\": optimizer_config['Adam']['end_learning_rate'],\n",
    "    \"weight_decay\": optimizer_config['Adam']['weight_decay'],\n",
    "    \"epsilon\": optimizer_config['Adam']['epsilon'],\n",
    "    \"lsm\": 0.0,\n",
    "    \"hidden_dropout_prob\": 0.1,\n",
    "    \"max_grad_norm\": 1,\n",
    "    \"use_warmup\": True,\n",
    "    \"n_gpu\": torch.cuda.device_count(),\n",
    "    \"gradient_accumulation_steps\": 8,\n",
    "    \"output_dir\": str(pretrain_model_path / run_id),\n",
    "    \"max_steps\": 125000,\n",
    "    \"logging_steps\": 100,\n",
    "    \"save_steps\": 25000,\n",
    "    \"evaluate_during_training\": False,\n",
    "    \"save_total_limit\": 3,\n",
    "    \"seed\": 9527,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shaped-postage",
   "metadata": {},
   "source": [
    "\n",
    "# Data loader pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "documented-developer",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Using custom data configuration default-597f3863485f3654\n",
      "WARNING:datasets.builder:Reusing dataset parquet (/home/jupyter/gogolook/data/cache_data_dir/parquet/default-597f3863485f3654/0.0.0/03dda9603b6ba3760d9d286684a3d7d8ec00448c154f765795485acd3229ecba)\n",
      "WARNING:datasets.builder:Using custom data configuration default-597f3863485f3654\n",
      "WARNING:datasets.builder:Reusing dataset parquet (/home/jupyter/gogolook/data/cache_data_dir/parquet/default-597f3863485f3654/0.0.0/03dda9603b6ba3760d9d286684a3d7d8ec00448c154f765795485acd3229ecba)\n"
     ]
    }
   ],
   "source": [
    "#torch.multiprocessing.set_start_method('spawn')\n",
    "#datasets.config.IN_MEMORY_MAX_SIZE\n",
    "@dataclass(eq=False)\n",
    "class GenerateDatasets: \n",
    "    #files_list: str = field(\n",
    "    #    default=None, metadata={\"help\": \"The files list of data path\"}\n",
    "    #)\n",
    "    data_path: str = field(\n",
    "        default=None, metadata={\"help\": \"The prefix path of files location\"}\n",
    "    )\n",
    "    regex_file_format: str = field(\n",
    "        default='*.parquet', metadata={\"help\": \"The files format.\"}\n",
    "    )\n",
    "    batch_size: int = field(\n",
    "        default=128, metadata={\"help\": \"Batch size\"}\n",
    "    )\n",
    "    is_training: bool = field(\n",
    "        default=True, metadata={\"help\": \"Is use training mode to create data pipeline\"}\n",
    "    )\n",
    "    device: str = field(\n",
    "        default='cpu', metadata={\"help\": \"Which device to use [cpu, cuda]\"}\n",
    "    )\n",
    "    cache_data_path: str = field(\n",
    "        default=None, metadata={\"help\": \"The path to cache data.\"}\n",
    "    )\n",
    "    use_streaming_mode: bool = field(\n",
    "        default=False, metadata={\"help\": \"Use streaming mode to download data.\"}\n",
    "    )\n",
    "        \n",
    "    def __post_init__(self):\n",
    "        self.get_files_list = glob.glob(os.path.join(str(self.data_path), self.regex_file_format))\n",
    "        #self.get_files_list = '/home/jupyter/gogolook/data/jp_data/valid_pretraining_data/valid_all-maxseq512_BG.parquet'\n",
    "        self.encoding_columns = ['input_ids', 'token_type_ids', 'attention_mask']\n",
    "        self.target_columns = ['masked_lm_labels', 'next_sentence_labels']\n",
    "        \n",
    "    def __call__(self, **kwargs):\n",
    "        # data 已經存在 device (cuda) 裡，所以再用 pin_memory 會出現 error\n",
    "        # RuntimeError: cannot pin 'torch.cuda.LongTensor' only dense CPU tensors can be pinned        \n",
    "        dataset = load_dataset('parquet', data_files=self.get_files_list, cache_dir=self.cache_data_path, split='train')\n",
    "        dataset.set_format(type='torch', columns=self.encoding_columns + self.target_columns) # , device=self.device\n",
    "        #dataset = dataset.rename_column(self.target_column, 'labels')\n",
    "        if self.is_training:\n",
    "            drop_last = True\n",
    "        else: \n",
    "            drop_last = False\n",
    "            \n",
    "        #dataloader = torch.utils.data.DataLoader(\n",
    "        #    dataset,\n",
    "        #    batch_size=self.batch_size,\n",
    "        #    pin_memory=True,\n",
    "        #    shuffle=True,\n",
    "        #    drop_last=drop_last,\n",
    "        #    num_workers=multiprocessing.cpu_count())\n",
    "        return dataset # dataloader\n",
    "        \n",
    "\n",
    "get_train_dataset = GenerateDatasets(\n",
    "    data_path=training_data_path,\n",
    "    batch_size=model_config['batch_size'],\n",
    "    is_training=True,\n",
    "    device=device,\n",
    "    cache_data_path=cache_data_path)\n",
    "\n",
    "get_valid_dataset = GenerateDatasets(\n",
    "    data_path=training_data_path,\n",
    "    batch_size=model_config['batch_size'],\n",
    "    is_training=False,\n",
    "    device=device,\n",
    "    cache_data_path=cache_data_path)\n",
    "\n",
    "train_dataset = get_train_dataset()\n",
    "valid_dataset = get_valid_dataset()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finite-opening",
   "metadata": {},
   "source": [
    "### Testing load from gcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "constant-devices",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import gcsfs\n",
    "#from datasets import load_from_disk\n",
    "#gcs = gcsfs.GCSFileSystem(project='data-research-216307') \n",
    "#gcs_files_list = gcs.glob('gs://gogolook-ml-data-production/serve-dev/sms/data/experimental_jp_data/train_pretraining_data/*.parquet')\n",
    "#gcs_files_list = [ \"gs://\" + path for path in gcs_files_list]\n",
    "#dataset = load_from_disk(dataset_path=\"gs://gogolook-ml-data-production/serve-dev/sms/data/experimental_jp_data/train_pretraining_data/\", fs=gcs)\n",
    "\n",
    "# saves encoded_dataset to your s3 bucket\n",
    "#train_dataset.save_to_disk('gcs://gogolook-ml-data-production/serve-dev/sms/data/experimental_jp_data/preprocessing_dataset', fs=gcs)\n",
    "#train_dataset.save_to_disk('/home/jupyter/gogolook/data/jp_data/preprocessing_dataset/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medical-generic",
   "metadata": {},
   "source": [
    "### Streaming test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "stretch-spokesman",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nget_files_list = glob.glob(os.path.join(str(experiment_train_data_path), \"*.parquet\"))\\n\\ndataset = load_dataset(\\'parquet\\', data_files=get_files_list[0], cache_dir=cache_data_path, split=\\'train\\', streaming=True)\\n\\n\\nmap_dataset = dataset.map(lambda example: (example[\"input_ids\"], example[\"token_type_ids\"], example[\"attention_mask\"]), batched=True, batch_size=64)\\n\\nshuffled_dataset = map_dataset.shuffle(buffer_size=100, seed=seed)\\n\\n\\ntorch_dataset  = shuffled_dataset.with_format(\"torch\")\\nassert isinstance(torch_dataset, torch.utils.data.IterableDataset)\\n#sampler = torch.utils.data.Sampler(torch_dataset)\\n#batch_sampler = torch.utils.data.BatchSampler(sampler, 64, False)\\n\\ndef worker_init_fn(_):\\n    worker_info = torch.utils.data.get_worker_info()\\n    dataset = worker_info.dataset\\n    worker_id = worker_info.id\\n    split_size = len(dataset.data) // worker_info.num_workers\\n    dataset.data = dataset.data[worker_id * split_size:(worker_id + 1) * split_size] \\n    \\ndef worker_init_fn(worker_id):\\n...     worker_info = torch.utils.data.get_worker_info()\\n...     dataset = worker_info.dataset  # the dataset copy in this worker process\\n...     overall_start = dataset.start\\n...     overall_end = dataset.end\\n...     # configure the dataset to only process the split workload\\n...     per_worker = int(math.ceil((overall_end - overall_start) / float(worker_info.num_workers)))\\n...     worker_id = worker_info.id\\n...     dataset.start = overall_start + worker_id * per_worker\\n...     dataset.end = min(dataset.start + per_worker, overall_end)\\n\\ndataloader = torch.utils.data.DataLoader(\\n        torch_dataset,\\n        batch_size=128,\\n        pin_memory=True,\\n        drop_last=False,\\n        num_workers=multiprocessing.cpu_count())\\ndef worker_init_fn(_):\\n    worker_info = torch.utils.data.get_worker_info()\\n    dataset = worker_info.dataset\\n    worker_id = worker_info.id\\n    split_size = 64 // worker_info.num_workers\\n    dataset.data = dataset.data[worker_id * split_size:(worker_id + 1) * split_size]     \\ndataloader = torch.utils.data.DataLoader(torch_dataset, batch_size=64, worker_init_fn=worker_init_fn, num_workers=multiprocessing.cpu_count())\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "get_files_list = glob.glob(os.path.join(str(experiment_train_data_path), \"*.parquet\"))\n",
    "\n",
    "dataset = load_dataset('parquet', data_files=get_files_list[0], cache_dir=cache_data_path, split='train', streaming=True)\n",
    "\n",
    "\n",
    "map_dataset = dataset.map(lambda example: (example[\"input_ids\"], example[\"token_type_ids\"], example[\"attention_mask\"]), batched=True, batch_size=64)\n",
    "\n",
    "shuffled_dataset = map_dataset.shuffle(buffer_size=100, seed=seed)\n",
    "\n",
    "\n",
    "torch_dataset  = shuffled_dataset.with_format(\"torch\")\n",
    "assert isinstance(torch_dataset, torch.utils.data.IterableDataset)\n",
    "#sampler = torch.utils.data.Sampler(torch_dataset)\n",
    "#batch_sampler = torch.utils.data.BatchSampler(sampler, 64, False)\n",
    "\n",
    "def worker_init_fn(_):\n",
    "    worker_info = torch.utils.data.get_worker_info()\n",
    "    dataset = worker_info.dataset\n",
    "    worker_id = worker_info.id\n",
    "    split_size = len(dataset.data) // worker_info.num_workers\n",
    "    dataset.data = dataset.data[worker_id * split_size:(worker_id + 1) * split_size] \n",
    "    \n",
    "def worker_init_fn(worker_id):\n",
    "...     worker_info = torch.utils.data.get_worker_info()\n",
    "...     dataset = worker_info.dataset  # the dataset copy in this worker process\n",
    "...     overall_start = dataset.start\n",
    "...     overall_end = dataset.end\n",
    "...     # configure the dataset to only process the split workload\n",
    "...     per_worker = int(math.ceil((overall_end - overall_start) / float(worker_info.num_workers)))\n",
    "...     worker_id = worker_info.id\n",
    "...     dataset.start = overall_start + worker_id * per_worker\n",
    "...     dataset.end = min(dataset.start + per_worker, overall_end)\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "        torch_dataset,\n",
    "        batch_size=128,\n",
    "        pin_memory=True,\n",
    "        drop_last=False,\n",
    "        num_workers=multiprocessing.cpu_count())\n",
    "def worker_init_fn(_):\n",
    "    worker_info = torch.utils.data.get_worker_info()\n",
    "    dataset = worker_info.dataset\n",
    "    worker_id = worker_info.id\n",
    "    split_size = 64 // worker_info.num_workers\n",
    "    dataset.data = dataset.data[worker_id * split_size:(worker_id + 1) * split_size]     \n",
    "dataloader = torch.utils.data.DataLoader(torch_dataset, batch_size=64, worker_init_fn=worker_init_fn, num_workers=multiprocessing.cpu_count())\n",
    "''' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "subject-limit",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_train_dataset.get_files_list\n",
    "#get_valid_dataset.get_files_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "stainless-arctic",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of AlbertForPreTraining were not initialized from the model checkpoint at voidful/albert_chinese_tiny and are newly initialized: ['sop_classifier.classifier.bias', 'sop_classifier.classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(32000, 128)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model_config['training_steps'] = len(train_dataloader) * model_config['epochs']\n",
    "#if model_config['use_warmup']:\n",
    "#    model_config['warmup_steps'] = int(len(train_dataloader) * model_config['epochs'] * 0.1)\n",
    "#    model_config['decay_steps'] = len(train_dataloader) * model_config['epochs']\n",
    "#else:\n",
    "#    model_config['warmup_steps'] = None \n",
    "#    model_config['decay_steps'] = None\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "albert_config = AlbertConfig.from_json_file(albert_zh_path / 'albert_config' / 'albert_config_tiny.json')\n",
    "pretrained_model_name_or_path = 'voidful/albert_chinese_tiny'\n",
    "albert_pretrain_model = AlbertForPreTraining.from_pretrained(\n",
    "    pretrained_model_name_or_path, \n",
    "    config=albert_config,             \n",
    "    cache_dir=cache_models_path)\n",
    "albert_pretrain_model.resize_token_embeddings(corpus_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "trained-prescription",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AlbertConfig {\n",
       "  \"_name_or_path\": \"voidful/albert_chinese_tiny\",\n",
       "  \"attention_probs_dropout_prob\": 0.0,\n",
       "  \"bos_token_id\": 2,\n",
       "  \"classifier_dropout_prob\": 0.1,\n",
       "  \"directionality\": \"bidi\",\n",
       "  \"embedding_size\": 128,\n",
       "  \"eos_token_id\": 3,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.0,\n",
       "  \"hidden_size\": 312,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"inner_group_num\": 1,\n",
       "  \"intermediate_size\": 1248,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"ln_type\": \"postln\",\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"albert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_groups\": 1,\n",
       "  \"num_hidden_layers\": 4,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"pooler_fc_size\": 768,\n",
       "  \"pooler_num_attention_heads\": 12,\n",
       "  \"pooler_num_fc_layers\": 3,\n",
       "  \"pooler_size_per_head\": 128,\n",
       "  \"pooler_type\": \"first_token_transform\",\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.6.0\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"vocab_size\": 32000\n",
       "}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "albert_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dirty-reform",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's use 4 GPUs!\n"
     ]
    }
   ],
   "source": [
    "if model_config[\"n_gpu\"] > 1:\n",
    "    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    #device_ids = [idx for idx in range(torch.cuda.device_count())]\n",
    "    #albert_pretrain_model = nn.DataParallel(albert_pretrain_model, device_ids=device_ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "romantic-greene",
   "metadata": {},
   "source": [
    "# Define Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "flying-cooling",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'params': ['albert.embeddings.word_embeddings.weight',\n",
       "   'albert.embeddings.position_embeddings.weight',\n",
       "   'albert.embeddings.token_type_embeddings.weight',\n",
       "   'albert.encoder.embedding_hidden_mapping_in.weight',\n",
       "   'albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm.weight',\n",
       "   'albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query.weight',\n",
       "   'albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key.weight',\n",
       "   'albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value.weight',\n",
       "   'albert.encoder.albert_layer_groups.0.albert_layers.0.attention.dense.weight',\n",
       "   'albert.encoder.albert_layer_groups.0.albert_layers.0.ffn.weight',\n",
       "   'albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output.weight',\n",
       "   'albert.pooler.weight',\n",
       "   'predictions.dense.weight',\n",
       "   'sop_classifier.classifier.weight'],\n",
       "  'weight_decay_rate': 0.0},\n",
       " {'params': ['albert.embeddings.LayerNorm.weight',\n",
       "   'albert.embeddings.LayerNorm.bias',\n",
       "   'albert.encoder.embedding_hidden_mapping_in.bias',\n",
       "   'albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm.bias',\n",
       "   'albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query.bias',\n",
       "   'albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key.bias',\n",
       "   'albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value.bias',\n",
       "   'albert.encoder.albert_layer_groups.0.albert_layers.0.attention.dense.bias',\n",
       "   'albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm.weight',\n",
       "   'albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm.bias',\n",
       "   'albert.encoder.albert_layer_groups.0.albert_layers.0.ffn.bias',\n",
       "   'albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output.bias',\n",
       "   'albert.pooler.bias',\n",
       "   'predictions.bias',\n",
       "   'predictions.LayerNorm.weight',\n",
       "   'predictions.LayerNorm.bias',\n",
       "   'predictions.dense.bias',\n",
       "   'sop_classifier.classifier.bias'],\n",
       "  'weight_decay_rate': 0.0}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_params = list(albert_pretrain_model.named_parameters())\n",
    "no_decay = [\"bias\", \"gamma\", \"beta\", \"LayerNorm.weight\"]\n",
    "\n",
    "optimizer_grounded_parameters_by_name = [\n",
    "    {'params': [n for n, p in model_params if not any(nd in n for nd in no_decay)], \n",
    "     'weight_decay_rate': 0.0 },\n",
    "    {'params': [n for n, p in model_params if any(nd in n for nd in no_decay)], \n",
    "     'weight_decay_rate': 0.0 }\n",
    "]\n",
    "\n",
    "optimizer_grounded_parameters_by_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "palestinian-vegetarian",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "\n",
    "class PolynomialDecay(_LRScheduler):\n",
    "    def __init__(self, optimizer, decay_steps, end_learning_rate=0.0001, power=0.5, cycle=False, last_epoch=-1, verbose=False):\n",
    "        if decay_steps <= 1.:\n",
    "            raise ValueError('max_decay_steps should be greater than 1.')            \n",
    "        self.decay_steps = decay_steps\n",
    "        self.end_learning_rate = end_learning_rate\n",
    "        self.power = power\n",
    "        self.cycle = cycle\n",
    "        super(PolynomialDecay, self).__init__(optimizer, last_epoch, verbose)\n",
    "    \n",
    "    def get_lr(self):\n",
    "        if not self._get_lr_called_within_step:\n",
    "            warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
    "                          \"please use `get_last_lr()`.\")\n",
    "        #dtype = initial_learning_rate.dtype\n",
    "        #end_learning_rate = math_ops.cast(self.end_learning_rate, dtype)\n",
    "        #power = math_ops.cast(self.power, dtype)\n",
    "        #global_step_recomp = math_ops.cast(step, dtype)\n",
    "        #decay_steps_recomp = math_ops.cast(self.decay_steps, dtype)\n",
    "        global_step_recomp = self.last_epoch\n",
    "        decay_steps_recomp = self.decay_steps\n",
    "        \n",
    "        if self.cycle:\n",
    "            if global_step_recomp == 0:\n",
    "                multiplier = 1.0 \n",
    "            else:\n",
    "                multiplier = math.ceil(global_step_recomp / self.decay_steps)\n",
    "            decay_steps_recomp = decay_steps_recomp * multiplier\n",
    "        else:\n",
    "            global_step_recomp = min(global_step_recomp, decay_steps_recomp)\n",
    "            \n",
    "        p = global_step_recomp / decay_steps_recomp\n",
    "        ic(self.last_epoch, optimizer.param_groups[0]['lr'], p)\n",
    "        return [((group['lr'] - self.end_learning_rate) * math.pow(1 - p, self.power) + self.end_learning_rate) for group in self.optimizer.param_groups]\n",
    "    \n",
    "    def _get_closed_form_lr(self):\n",
    "        return [(base_lr - self.end_learning_rate) * math.pow(1 - p, self.power) + self.end_learning_rate for base_lr in self.base_lrs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "pediatric-attachment",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AdamW,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "\n",
    "\n",
    "def get_optimizer(config: dict, model: PreTrainedModel, num_training_steps: int):\n",
    "    model_params = list(model.named_parameters())\n",
    "    no_decay = [\"bias\", \"gamma\", \"beta\", \"LayerNorm.weight\"]\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in model_params if not any(nd in n for nd in no_decay)], \n",
    "         'weight_decay_rate': 1e-2  },\n",
    "        {'params': [p for n, p in model_params if any(nd in n for nd in no_decay)], \n",
    "         'weight_decay_rate': 0.0 }\n",
    "    ]\n",
    "\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=config[\"learning_rate\"], eps=config[\"epsilon\"])\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=int(num_training_steps * 0.1), num_training_steps=num_training_steps\n",
    "    )\n",
    "    return optimizer, scheduler\n",
    "\n",
    "    \n",
    "#optimizer = torch.optim.Adam(\n",
    "#    params=optimizer_grounded_parameters,\n",
    "#    lr=model_config[\"learning_rate\"],\n",
    "#    betas=(0.9, 0.98),\n",
    "#    weight_decay=config[\"weight_decay\"],\n",
    "#    eps=config[\"adam_epsilon\"])\n",
    "\n",
    "#scheduler = LinearWarmupCosineAnnealingLR(\n",
    "#    optimizer, \n",
    "#    warmup_epochs=model_config['warmup_steps'], \n",
    "#    max_epochs=model_config['training_steps'],\n",
    "#    eta_min=model_config[\"end_learning_rate\"])\n",
    "\n",
    "#optimizer = optim.SGD(sms_model.parameters(), lr=model_config['init_learning_rate'], weight_decay=1e-4)\n",
    "#optimizer = optim.SGD(filter(lambda p: p.requires_grad, sms_model.parameters()), lr=model_config['init_learning_rate'], weight_decay=1e-4)\n",
    "\n",
    "#scheduler = CyclicLR(\n",
    "#    optimizer, \n",
    "#    base_lr=1e-5,\n",
    "#    max_lr=model_config['init_learning_rate'],\n",
    "#    step_size_up=model_config['training_steps'] * 1,\n",
    "#    mode='triangular2',\n",
    "#    scale_mode='cycle',\n",
    "#    cycle_momentum=False\n",
    "#)\n",
    "\n",
    "\n",
    "#if model_config[\"use_multi_gpus\"]:\n",
    "#optimizer = nn.DataParallel(optimizer, device_ids=device_ids)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "blond-december",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Mertice\n",
    "from torchmetrics import MetricCollection\n",
    "metric_collection = MetricCollection([\n",
    "    torchmetrics.Accuracy(num_classes=2, average='macro', multiclass=True, dist_sync_on_step=True, mdmc_average='global').to(device),\n",
    "    torchmetrics.Precision(num_classes=2, average='macro', multiclass=True, dist_sync_on_step=True, mdmc_average='global').to(device),\n",
    "    torchmetrics.Recall(num_classes=2, average='macro', multiclass=True, dist_sync_on_step=True, mdmc_average='global').to(device),\n",
    "    torchmetrics.F1(num_classes=2, average='macro', multiclass=True, dist_sync_on_step=True, mdmc_average='global').to(device)\n",
    "], prefix='Train_')\n",
    "\n",
    "val_metric_collection = MetricCollection([\n",
    "    torchmetrics.Accuracy(num_classes=2, average='macro', multiclass=True, dist_sync_on_step=True, mdmc_average='global').to(device),\n",
    "    torchmetrics.Precision(num_classes=2, average='macro', multiclass=True, dist_sync_on_step=True, mdmc_average='global').to(device),\n",
    "    torchmetrics.Recall(num_classes=2, average='macro', multiclass=True, dist_sync_on_step=True, mdmc_average='global').to(device),\n",
    "    torchmetrics.F1(num_classes=2, average='macro', multiclass=True, dist_sync_on_step=True, mdmc_average='global').to(device),\n",
    "], prefix='Val_')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "future-absorption",
   "metadata": {},
   "source": [
    "# Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "educated-arkansas",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "trying-publisher",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _sorted_checkpoints(config, checkpoint_prefix=\"checkpoint\", use_mtime=False) -> List[str]:\n",
    "    if not os.path.isdir(config[\"output_dir\"]):\n",
    "        os.makedirs(config[\"output_dir\"], exist_ok=True)\n",
    "        \n",
    "    ordering_and_checkpoint_path = []\n",
    "    glob_checkpoints = glob.glob(os.path.join(config[\"output_dir\"], \"{}-*\".format(checkpoint_prefix)))\n",
    "\n",
    "    for path in glob_checkpoints:\n",
    "        if use_mtime:\n",
    "            ordering_and_checkpoint_path.append((os.path.getmtime(path), path))\n",
    "        else:\n",
    "            regex_match = re.match(\".*{}-([0-9]+)\".format(checkpoint_prefix), path)\n",
    "            if regex_match and regex_match.groups():\n",
    "                ordering_and_checkpoint_path.append((int(regex_match.groups()[0]), path))\n",
    "\n",
    "    checkpoints_sorted = sorted(ordering_and_checkpoint_path)\n",
    "    checkpoints_sorted = [checkpoint[1] for checkpoint in checkpoints_sorted]\n",
    "    return checkpoints_sorted\n",
    "\n",
    "\n",
    "def _rotate_checkpoints(config, checkpoint_prefix=\"checkpoint\", use_mtime=False) -> None:\n",
    "    if not config[\"save_total_limit\"]:\n",
    "        return\n",
    "    if config[\"save_total_limit\"] <= 0:\n",
    "        return\n",
    "\n",
    "    # Check if we should delete older checkpoint(s)\n",
    "    checkpoints_sorted = _sorted_checkpoints(config, checkpoint_prefix, use_mtime)\n",
    "    if len(checkpoints_sorted) <= config[\"save_total_limit\"]:\n",
    "        return\n",
    "\n",
    "    number_of_checkpoints_to_delete = max(0, len(checkpoints_sorted) - config[\"save_total_limit\"])\n",
    "    checkpoints_to_be_deleted = checkpoints_sorted[:number_of_checkpoints_to_delete]\n",
    "    for checkpoint in checkpoints_to_be_deleted:\n",
    "        logger.info(\"Deleting older checkpoint [{}] due to args.save_total_limit\".format(checkpoint))\n",
    "        shutil.rmtree(checkpoint)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "touched-amsterdam",
   "metadata": {},
   "source": [
    "### Init wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "indie-water",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33myuyuliao20\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.1 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.11.1<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">jp-pretrain-model_baseline_20210914151521</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/yuyuliao20/jp-pretrain-model\" target=\"_blank\">https://wandb.ai/yuyuliao20/jp-pretrain-model</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/yuyuliao20/jp-pretrain-model/runs/1ze4ftz4\" target=\"_blank\">https://wandb.ai/yuyuliao20/jp-pretrain-model/runs/1ze4ftz4</a><br/>\n",
       "                Run data is saved locally in <code>/home/jupyter/gogolook/albert-japanese/notebook/wandb/run-20210914_071530-1ze4ftz4</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h1>Run(1ze4ftz4)</h1><iframe src=\"https://wandb.ai/yuyuliao20/jp-pretrain-model/runs/1ze4ftz4\" style=\"border:none;width:100%;height:400px\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fa74c722a10>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#wandb.tensorboard.patch(root_logdir=str(tensorboard_path / run_id))\n",
    "wandb.init(\n",
    "    project=project,\n",
    "    group=group_tag,\n",
    "    job_type=job_type,\n",
    "    name=run_id,\n",
    "    notes=method_tag,\n",
    "    tags=addition_tag,\n",
    "    sync_tensorboard=True,\n",
    "    config={**model_config},\n",
    "    reinit=True\n",
    ")\n",
    "\n",
    "wandb_config = wandb.config\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "living-partnership",
   "metadata": {},
   "source": [
    "### Define training stepsm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "rubber-convertible",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm, trange, tqdm_notebook\n",
    "\n",
    "def training_step(\n",
    "    config: dict,\n",
    "    train_dataset: torch.utils.data.Dataset, \n",
    "    eval_dataset: torch.utils.data.Dataset, \n",
    "    model: PreTrainedModel,  \n",
    "    device: str,\n",
    "    init_wandb: object\n",
    "):\n",
    "    if args.local_rank in [-1, 0]:\n",
    "        tb_writer = SummaryWriter()\n",
    "    train_batch_size = config[\"per_gpu_train_batch_size\"] * max(1, config[\"n_gpu\"])\n",
    "    ic(train_batch_size)\n",
    "    train_sampler = RandomSampler(train_dataset) if args.local_rank == -1 else DistributedSampler(train_dataset)\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset, \n",
    "        sampler=train_sampler, \n",
    "        batch_size=train_batch_size, \n",
    "        pin_memory=True, \n",
    "        drop_last=True, \n",
    "        num_workers=multiprocessing.cpu_count())\n",
    "    \n",
    "    if config[\"max_steps\"] > 0:\n",
    "        t_total = config[\"max_steps\"]\n",
    "        config[\"num_train_epochs\"] = config[\"max_steps\"] // (len(train_dataloader) // config[\"gradient_accumulation_steps\"]) + 1\n",
    "    else:\n",
    "        t_total = len(train_dataloader) // config[\"gradient_accumulation_steps\"] * config[\"num_train_epochs\"]\n",
    "        \n",
    "    optimizer, scheduler = get_optimizer(config, model, t_total)\n",
    "    \n",
    "    if config[\"n_gpu\"]:\n",
    "        model = torch.nn.DataParallel(model)\n",
    "    \n",
    "    if args.local_rank != -1:\n",
    "        model = torch.nn.parallel.DistributedDataParallel(\n",
    "            model, device_ids=[args.local_rank], output_device=args.local_rank, find_unused_parameters=True\n",
    "        )\n",
    "    # Train !\n",
    "    logger.info(\"***** Running training *****\")\n",
    "    logger.info(\"  Num examples = %d\", len(train_dataset))\n",
    "    logger.info(\"  Num Epochs = %d\", config[\"num_train_epochs\"])\n",
    "    logger.info(\"  Instantaneous batch size per GPU = %d\", config[\"per_gpu_train_batch_size\"])\n",
    "    logger.info(\n",
    "        \"  Total train batch size (w. parallel, distributed & accumulation) = %d\",\n",
    "        train_batch_size * config[\"gradient_accumulation_steps\"]* (torch.distributed.get_world_size() if args.local_rank != -1 else 1),\n",
    "    )\n",
    "    logger.info(\"  Gradient Accumulation steps = %d\", config[\"gradient_accumulation_steps\"])\n",
    "    logger.info(\"  Total optimization steps = %d\", t_total)\n",
    "    \n",
    "    set_seed(config[\"seed\"])\n",
    "    global_step = 0\n",
    "    epochs_trained = 0 \n",
    "    train_loss, logging_loss = 0.0, 0.0\n",
    "    #train_iterator = trange(\n",
    "    #    epochs_trained, int(config[\"num_train_epochs\"]), desc=\"Epoch\", disable=args.local_rank not in [-1, 0]\n",
    "    #)\n",
    "    \n",
    "    train_iterator = tqdm_notebook(range(\n",
    "        epochs_trained, int(config[\"num_train_epochs\"])), desc=\"Epoch\", disable=args.local_rank not in [-1, 0]\n",
    "    )\n",
    "    scaler = GradScaler()\n",
    "    model.train()\n",
    "    for epoch in train_iterator:\n",
    "        #epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\", disable=args.local_rank not in [-1, 0])\n",
    "        epoch_iterator = tqdm_notebook(train_dataloader, desc=\"Iteration\", disable=args.local_rank not in [-1, 0])\n",
    "        \n",
    "        if args.local_rank != -1:\n",
    "            train_sampler.set_epoch(epoch)\n",
    "            \n",
    "        for step, batch in enumerate(epoch_iterator):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            token_type_ids = batch['token_type_ids'].to(device)        \n",
    "            mlm_labels = batch['masked_lm_labels'].to(device)\n",
    "            sop_labels = batch['next_sentence_labels'].to(device)\n",
    "    \n",
    "            with autocast():\n",
    "                # Forward pass\n",
    "                outputs = model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    token_type_ids=token_type_ids,\n",
    "                    labels=mlm_labels,\n",
    "                    sentence_order_label=sop_labels\n",
    "                )\n",
    "                assert outputs.prediction_logits.dtype is torch.float16\n",
    "                if config[\"n_gpu\"] > 1:\n",
    "                    loss = outputs.loss.mean()  # mean() to average on multi-gpu parallel training\n",
    "                if config[\"gradient_accumulation_steps\"] > 1:\n",
    "                    loss = loss / config[\"gradient_accumulation_steps\"]\n",
    "                assert loss.dtype is torch.float32\n",
    "            scaler.scale(loss).backward()\n",
    "            train_loss += loss.item()\n",
    "            \n",
    "            if (step + 1) % config[\"gradient_accumulation_steps\"] == 0:\n",
    "                #torch.nn.utils.clip_grad_norm_(model.parameters(), config[\"max_grad_norm\"])  \n",
    "                # Backward pass\n",
    "                # Zero gradients, perform a backward pass, and update the weights.            \n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                scheduler.step()  # Update learning rate schedule\n",
    "                model.zero_grad()\n",
    "                global_step += 1\n",
    "                \n",
    "                if (args.local_rank in [-1, 0]) and (config[\"logging_steps\"] > 0) and (global_step % config[\"logging_steps\"] == 0):\n",
    "                    ic(global_step % config[\"logging_steps\"])\n",
    "                    # Log metrics\n",
    "                    if (args.local_rank == -1 and config[\"evaluate_during_training\"]):  \n",
    "                        # Only evaluate when single GPU otherwise metrics may not average well\n",
    "                        results = evaluate_step(\n",
    "                            config=config, \n",
    "                            model=model, \n",
    "                            dataset=eval_dataset, \n",
    "                            device=device,\n",
    "                            init_wandb=init_wandb\n",
    "                        )\n",
    "                        for key, value in results.items():\n",
    "                            tb_writer.add_scalar(\"eval_{}\".format(key), value, global_step)\n",
    "\n",
    "                    #last_lr = optimizer.param_groups[0]['lr']\n",
    "                    #last_lr = scheduler.optimizer.param_groups[0][\"lr\"]\n",
    "                    print(\"=== Sent event to wandb===\")\n",
    "                    init_wandb.log({'lr': scheduler.get_lr()[0]}, step=global_step)\n",
    "                    show_logs(\n",
    "                        _wandb=init_wandb,\n",
    "                        loss=(train_loss - logging_loss) / config[\"logging_steps\"],\n",
    "                        step=global_step\n",
    "                    )\n",
    "                    tb_writer.add_scalar(\"lr\", scheduler.get_lr()[0], global_step)\n",
    "                    tb_writer.add_scalar(\"loss\", (train_loss - logging_loss) / config[\"logging_steps\"], global_step)\n",
    "                    logging_loss = train_loss\n",
    "\n",
    "                if (args.local_rank in [-1, 0]) and (config[\"save_steps\"] > 0) and (global_step % config[\"save_steps\"] == 0):\n",
    "                        checkpoint_prefix = \"checkpoint\"\n",
    "                        # Save model checkpoint\n",
    "                        output_dir = os.path.join(config[\"output_dir\"], \"{}-{}\".format(checkpoint_prefix, global_step))\n",
    "                        os.makedirs(output_dir, exist_ok=True)\n",
    "                        model_to_save = (\n",
    "                            model.module if hasattr(model, \"module\") else model\n",
    "                        )  # Take care of distributed/parallel training\n",
    "                        model_to_save.save_pretrained(output_dir)\n",
    "                        logger.info(\"Saving model checkpoint to %s\", output_dir)\n",
    "                        _rotate_checkpoints(args, checkpoint_prefix)\n",
    "                    \n",
    "            if config[\"max_steps\"]> 0 and global_step > config[\"max_steps\"]:\n",
    "                epoch_iterator.close()\n",
    "                break\n",
    "                \n",
    "        if config[\"max_steps\"] > 0 and global_step > config[\"max_steps\"]:\n",
    "            train_iterator.close()\n",
    "            break\n",
    "\n",
    "    if args.local_rank in [-1, 0]:\n",
    "        tb_writer.close()\n",
    "        \n",
    "    return global_step, tr_loss / global_step\n",
    "\n",
    "def evaluate_step(\n",
    "    config: dict, \n",
    "    model: PreTrainedModel, \n",
    "    dataset: torch.utils.data.Dataset, \n",
    "    device: str, \n",
    "    init_wandb: object,\n",
    "    prefix: Optional[str]=\"\") -> dict:\n",
    "    \n",
    "    eval_output_dir = config[\"output_dir\"]\n",
    "    if args.local_rank in [-1, 0]:\n",
    "        os.makedirs(eval_output_dir, exist_ok=True)\n",
    "    \n",
    "    eval_batch_size = config[\"per_gpu_eval_batch_size\"] * max(1, config[\"n_gpu\"])\n",
    "    eval_sampler = SequentialSampler(dataset)\n",
    "        \n",
    "    eval_dataloader = DataLoader(\n",
    "        dataset,\n",
    "        sampler=eval_sampler,\n",
    "        batch_size=eval_batch_size, \n",
    "        pin_memory=True, \n",
    "        drop_last=False,\n",
    "        num_workers=multiprocessing.cpu_count())\n",
    "    \n",
    "    if config[\"n_gpu\"] > 1:\n",
    "        model = torch.nn.DataParallel(model)\n",
    "        \n",
    "    # Eval!\n",
    "    logger.info(\"***** Running evaluation {} *****\".format(prefix))\n",
    "    logger.info(\"  Num examples = %d\", len(dataset))\n",
    "    logger.info(\"  Batch size = %d\", eval_batch_size)\n",
    "    eval_loss = 0.0\n",
    "    nb_eval_steps = 0\n",
    "    model.eval()\n",
    "    \n",
    "    for batch in tqdm_notebook(eval_dataloader, desc=\"Evaluating\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        token_type_ids = batch['token_type_ids'].to(device)\n",
    "        mlm_labels = batch['masked_lm_labels'].to(device)\n",
    "        sop_labels = batch['next_sentence_labels'].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                token_type_ids=token_type_ids,\n",
    "                labels=mlm_labels,\n",
    "                sentence_order_label=sop_labels\n",
    "            )\n",
    "            loss = outputs.loss.mean()\n",
    "        eval_loss += loss.item()\n",
    "        nb_eval_steps += 1\n",
    "        \n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    perplexity = torch.exp(torch.tensor(eval_loss))\n",
    "    result = {\n",
    "        \"loss\": eval_loss,\n",
    "        \"perplexity\": perplexity\n",
    "    }\n",
    "    show_logs(init_wandb, eval_loss, nb_eval_steps, prefix=\"Eval\", perplexity=perplexity.item())\n",
    "    return result\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "consolidated-introduction",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef training_step(model, input_ids, attention_mask, token_type_ids, mlm_labels, sop_labels, scaler, use_multi_gpus=False):\\n    with autocast():\\n        # Forward pass\\n        outputs = model(\\n            input_ids=input_ids,\\n            attention_mask=attention_mask,\\n            token_type_ids=token_type_ids,\\n            labels=mlm_labels,\\n            sentence_order_label=sop_labels\\n        )\\n        assert outputs.prediction_logits.dtype is torch.float16\\n        \\n        loss = outputs.loss.mean()\\n        assert loss.dtype is torch.float32\\n    # Backward pass\\n    # Zero gradients, perform a backward pass, and update the weights.\\n    optimizer.zero_grad()\\n    scaler.scale(loss).backward()\\n    #if use_multi_gpus:\\n    #    scaler.step(optimizer.module)\\n    #else:\\n    scaler.step(optimizer)\\n    scaler.update()\\n    #torch.nn.utils.clip_grad_norm_(optimizer_grounded_parameters, max_norm=0.5)\\n    #if epoch > swa_start:\\n    #    swa_model.update_parameters(model)\\n    #    swa_scheduler.step()\\n    #else:\\n    #    scheduler.step()\\n    return loss\\n\\n@torch.no_grad()\\ndef validataion_step(model, input_ids, attention_mask, token_type_ids, mlm_labels, sop_labels):    \\n    with autocast():\\n        outputs = model(\\n            input_ids=input_ids,\\n            attention_mask=attention_mask,\\n            token_type_ids=token_type_ids,\\n            labels=mlm_labels,\\n            sentence_order_label=sop_labels\\n        )\\n        loss = outputs.loss.mean()\\n    return loss\\n\\n'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def training_step(model, input_ids, attention_mask, token_type_ids, mlm_labels, sop_labels, scaler, use_multi_gpus=False):\n",
    "    with autocast():\n",
    "        # Forward pass\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            labels=mlm_labels,\n",
    "            sentence_order_label=sop_labels\n",
    "        )\n",
    "        assert outputs.prediction_logits.dtype is torch.float16\n",
    "        \n",
    "        loss = outputs.loss.mean()\n",
    "        assert loss.dtype is torch.float32\n",
    "    # Backward pass\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    scaler.scale(loss).backward()\n",
    "    #if use_multi_gpus:\n",
    "    #    scaler.step(optimizer.module)\n",
    "    #else:\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "    #torch.nn.utils.clip_grad_norm_(optimizer_grounded_parameters, max_norm=0.5)\n",
    "    #if epoch > swa_start:\n",
    "    #    swa_model.update_parameters(model)\n",
    "    #    swa_scheduler.step()\n",
    "    #else:\n",
    "    #    scheduler.step()\n",
    "    return loss\n",
    "\n",
    "@torch.no_grad()\n",
    "def validataion_step(model, input_ids, attention_mask, token_type_ids, mlm_labels, sop_labels):    \n",
    "    with autocast():\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            labels=mlm_labels,\n",
    "            sentence_order_label=sop_labels\n",
    "        )\n",
    "        loss = outputs.loss.mean()\n",
    "    return loss\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eligible-purple",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fleet-corpus",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RUN ID]: jp-pretrain-model_baseline_20210914151521\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AlbertForPreTraining(\n",
       "  (albert): AlbertModel(\n",
       "    (embeddings): AlbertEmbeddings(\n",
       "      (word_embeddings): Embedding(32000, 128)\n",
       "      (position_embeddings): Embedding(512, 128)\n",
       "      (token_type_embeddings): Embedding(2, 128)\n",
       "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): AlbertTransformer(\n",
       "      (embedding_hidden_mapping_in): Linear(in_features=128, out_features=312, bias=True)\n",
       "      (albert_layer_groups): ModuleList(\n",
       "        (0): AlbertLayerGroup(\n",
       "          (albert_layers): ModuleList(\n",
       "            (0): AlbertLayer(\n",
       "              (full_layer_layer_norm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
       "              (attention): AlbertAttention(\n",
       "                (query): Linear(in_features=312, out_features=312, bias=True)\n",
       "                (key): Linear(in_features=312, out_features=312, bias=True)\n",
       "                (value): Linear(in_features=312, out_features=312, bias=True)\n",
       "                (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "                (output_dropout): Dropout(p=0.0, inplace=False)\n",
       "                (dense): Linear(in_features=312, out_features=312, bias=True)\n",
       "                (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
       "              )\n",
       "              (ffn): Linear(in_features=312, out_features=1248, bias=True)\n",
       "              (ffn_output): Linear(in_features=1248, out_features=312, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): Linear(in_features=312, out_features=312, bias=True)\n",
       "    (pooler_activation): Tanh()\n",
       "  )\n",
       "  (predictions): AlbertMLMHead(\n",
       "    (LayerNorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    (dense): Linear(in_features=312, out_features=128, bias=True)\n",
       "    (decoder): Linear(in_features=128, out_features=32000, bias=True)\n",
       "  )\n",
       "  (sop_classifier): AlbertSOPHead(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (classifier): Linear(in_features=312, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| train_batch_size: 64\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c74dc7035be944ec9fc09f7f6200aeff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=8.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e76abe8581ad49b882be2481233a7bce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=131487.0, style=ProgressStyle(description…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| global_step % config[\"logging_steps\"]: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step cannot be set when using syncing with tensorboard. Please log your step values as a metric such as 'global_step'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sent event to wandb===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| global_step % config[\"logging_steps\"]: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sent event to wandb===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| global_step % config[\"logging_steps\"]: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sent event to wandb===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| global_step % config[\"logging_steps\"]: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sent event to wandb===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| global_step % config[\"logging_steps\"]: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sent event to wandb===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| global_step % config[\"logging_steps\"]: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sent event to wandb===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| global_step % config[\"logging_steps\"]: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sent event to wandb===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| global_step % config[\"logging_steps\"]: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sent event to wandb===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| global_step % config[\"logging_steps\"]: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sent event to wandb===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| global_step % config[\"logging_steps\"]: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sent event to wandb===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| global_step % config[\"logging_steps\"]: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sent event to wandb===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| global_step % config[\"logging_steps\"]: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sent event to wandb===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| global_step % config[\"logging_steps\"]: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sent event to wandb===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| global_step % config[\"logging_steps\"]: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sent event to wandb===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| global_step % config[\"logging_steps\"]: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sent event to wandb===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| global_step % config[\"logging_steps\"]: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sent event to wandb===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| global_step % config[\"logging_steps\"]: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sent event to wandb===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| global_step % config[\"logging_steps\"]: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sent event to wandb===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| global_step % config[\"logging_steps\"]: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sent event to wandb===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| global_step % config[\"logging_steps\"]: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sent event to wandb===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| global_step % config[\"logging_steps\"]: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sent event to wandb===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| global_step % config[\"logging_steps\"]: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sent event to wandb===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| global_step % config[\"logging_steps\"]: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sent event to wandb===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| global_step % config[\"logging_steps\"]: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sent event to wandb===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| global_step % config[\"logging_steps\"]: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sent event to wandb===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| global_step % config[\"logging_steps\"]: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sent event to wandb===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| global_step % config[\"logging_steps\"]: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sent event to wandb===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| global_step % config[\"logging_steps\"]: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sent event to wandb===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| global_step % config[\"logging_steps\"]: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sent event to wandb===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| global_step % config[\"logging_steps\"]: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sent event to wandb===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| global_step % config[\"logging_steps\"]: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sent event to wandb===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| global_step % config[\"logging_steps\"]: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sent event to wandb===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| global_step % config[\"logging_steps\"]: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sent event to wandb===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| global_step % config[\"logging_steps\"]: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sent event to wandb===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| global_step % config[\"logging_steps\"]: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sent event to wandb===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| global_step % config[\"logging_steps\"]: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sent event to wandb===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| global_step % config[\"logging_steps\"]: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sent event to wandb===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| global_step % config[\"logging_steps\"]: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sent event to wandb===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| global_step % config[\"logging_steps\"]: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sent event to wandb===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| global_step % config[\"logging_steps\"]: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sent event to wandb===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| global_step % config[\"logging_steps\"]: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sent event to wandb===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| global_step % config[\"logging_steps\"]: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sent event to wandb===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| global_step % config[\"logging_steps\"]: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sent event to wandb===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| global_step % config[\"logging_steps\"]: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sent event to wandb===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| global_step % config[\"logging_steps\"]: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sent event to wandb===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| global_step % config[\"logging_steps\"]: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sent event to wandb===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| global_step % config[\"logging_steps\"]: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sent event to wandb===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| global_step % config[\"logging_steps\"]: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sent event to wandb===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| global_step % config[\"logging_steps\"]: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sent event to wandb===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| global_step % config[\"logging_steps\"]: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sent event to wandb===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| global_step % config[\"logging_steps\"]: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sent event to wandb===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| global_step % config[\"logging_steps\"]: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sent event to wandb===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| global_step % config[\"logging_steps\"]: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sent event to wandb===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| global_step % config[\"logging_steps\"]: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sent event to wandb===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| global_step % config[\"logging_steps\"]: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sent event to wandb===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| global_step % config[\"logging_steps\"]: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sent event to wandb===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| global_step % config[\"logging_steps\"]: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sent event to wandb===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| global_step % config[\"logging_steps\"]: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sent event to wandb===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| global_step % config[\"logging_steps\"]: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sent event to wandb===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| global_step % config[\"logging_steps\"]: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sent event to wandb===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| global_step % config[\"logging_steps\"]: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sent event to wandb===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| global_step % config[\"logging_steps\"]: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sent event to wandb===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| global_step % config[\"logging_steps\"]: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sent event to wandb===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| global_step % config[\"logging_steps\"]: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sent event to wandb===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| global_step % config[\"logging_steps\"]: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sent event to wandb===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| global_step % config[\"logging_steps\"]: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sent event to wandb===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| global_step % config[\"logging_steps\"]: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sent event to wandb===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| global_step % config[\"logging_steps\"]: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sent event to wandb===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| global_step % config[\"logging_steps\"]: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sent event to wandb===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| global_step % config[\"logging_steps\"]: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sent event to wandb===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| global_step % config[\"logging_steps\"]: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sent event to wandb===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| global_step % config[\"logging_steps\"]: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sent event to wandb===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| global_step % config[\"logging_steps\"]: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sent event to wandb===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| global_step % config[\"logging_steps\"]: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sent event to wandb===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| global_step % config[\"logging_steps\"]: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sent event to wandb===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| global_step % config[\"logging_steps\"]: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sent event to wandb===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| global_step % config[\"logging_steps\"]: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sent event to wandb===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| global_step % config[\"logging_steps\"]: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sent event to wandb===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| global_step % config[\"logging_steps\"]: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sent event to wandb===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| global_step % config[\"logging_steps\"]: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sent event to wandb===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| global_step % config[\"logging_steps\"]: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sent event to wandb===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| global_step % config[\"logging_steps\"]: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sent event to wandb===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| global_step % config[\"logging_steps\"]: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sent event to wandb===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| global_step % config[\"logging_steps\"]: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sent event to wandb===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| global_step % config[\"logging_steps\"]: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sent event to wandb===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| global_step % config[\"logging_steps\"]: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sent event to wandb===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| global_step % config[\"logging_steps\"]: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sent event to wandb===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| global_step % config[\"logging_steps\"]: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sent event to wandb===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| global_step % config[\"logging_steps\"]: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sent event to wandb===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| global_step % config[\"logging_steps\"]: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sent event to wandb===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| global_step % config[\"logging_steps\"]: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sent event to wandb===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| global_step % config[\"logging_steps\"]: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sent event to wandb===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| global_step % config[\"logging_steps\"]: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sent event to wandb===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| global_step % config[\"logging_steps\"]: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sent event to wandb===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| global_step % config[\"logging_steps\"]: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sent event to wandb===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| global_step % config[\"logging_steps\"]: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sent event to wandb===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| global_step % config[\"logging_steps\"]: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sent event to wandb===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| global_step % config[\"logging_steps\"]: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sent event to wandb===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| global_step % config[\"logging_steps\"]: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sent event to wandb===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| global_step % config[\"logging_steps\"]: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sent event to wandb===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| global_step % config[\"logging_steps\"]: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sent event to wandb===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| global_step % config[\"logging_steps\"]: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sent event to wandb===\n"
     ]
    }
   ],
   "source": [
    "print('[RUN ID]: {}'.format(run_id))\n",
    "torch.cuda.empty_cache()\n",
    "use_epoch_tracking = False\n",
    "use_step_tracking = True\n",
    "#wandb.watch(sms_model, log=\"all\", log_freq=1000)        \n",
    "def show_logs(_wandb, loss, step, is_epoch=False, prefix='Train', **kwargs):\n",
    "    loss = float(loss)\n",
    "    if is_epoch:\n",
    "        _wandb.log({\"epoch\": step, f\"{prefix}_loss\": loss}, step=step)\n",
    "    else:\n",
    "        _wandb.log({f\"{prefix}_step_loss\": loss}, step=step)\n",
    "        #print(f\"{prefix} loss after \" + str(example_ct).zfill(5) + f\" examples: {loss:.3f}\")\n",
    "    if \"perplexity\" in kwargs.keys():\n",
    "        _wandb.log({f\"{prefix}_perplexity\": kwargs[\"perplexity\"]}, step=step)\n",
    "\n",
    "def save_model(model, save_model_path):\n",
    "    logging.info(\"[INFO] Start to save model ...\")\n",
    "    #if not save_model_path.parent.exists():\n",
    "    #    save_model_path.parent.mkdir()       \n",
    "    torch.save(model.state_dict(), save_model_path)\n",
    "    \n",
    "start_time = time.time()\n",
    "albert_pretrain_model.to(device)\n",
    "\n",
    "if args.local_rank not in [-1, 0]:\n",
    "    torch.distributed.barrier()  # End of barrier to make sure only the first process in distributed training download model & vocab\n",
    "logger.info(\"Training/evaluation parameters %s\", args)\n",
    "\n",
    "global_step, tr_loss = training_step(\n",
    "    config=model_config, \n",
    "    train_dataset=train_dataset, \n",
    "    eval_dataset=valid_dataset, \n",
    "    model=albert_pretrain_model, \n",
    "    device=device,\n",
    "    init_wandb=wandb)\n",
    "\n",
    "ic(\" global_step = %s, average loss = %s\", global_step, tr_loss)\n",
    "\n",
    "end_time = time.time()\n",
    "each_steps_compute_time = (end_time - start_time)\n",
    "print(each_steps_compute_time)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "announced-salem",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "print('[RUN ID]: {}'.format(run_id))\n",
    "torch.cuda.empty_cache()\n",
    "use_epoch_tracking = False\n",
    "use_step_tracking = True\n",
    "#wandb.watch(sms_model, log=\"all\", log_freq=1000)        \n",
    "def show_logs(loss, step, is_epoch=False, prefix='Train', **kwargs):\n",
    "    loss = float(loss)\n",
    "    if is_epoch:\n",
    "        wandb.log({\"epoch\": step, f\"{prefix}_loss\": loss}, step=step)\n",
    "    else:\n",
    "        wandb.log({f\"{prefix}_step_loss\": loss}, step=step)\n",
    "        #print(f\"{prefix} loss after \" + str(example_ct).zfill(5) + f\" examples: {loss:.3f}\")\n",
    "    if \"perplexity\" in kwargs.keys():\n",
    "        wandb.log({f\"{prefix}_perplexity\": kwargs[\"perplexity\"]}, step=step)\n",
    "\n",
    "def save_model(model, save_model_path):\n",
    "    logging.info(\"[INFO] Start to save model ...\")\n",
    "    #if not save_model_path.parent.exists():\n",
    "    #    save_model_path.parent.mkdir()       \n",
    "    torch.save(model.state_dict(), save_model_path)\n",
    "    \n",
    "# Creates a GradScaler once at the beginning of training.\n",
    "scaler = GradScaler()\n",
    "\n",
    "for epoch in tqdm(range(model_config['epochs'])): # model_config['epochs']\n",
    "    start_time = time.time()    \n",
    "    train_batch_loss = 0\n",
    "    valid_batch_loss = 0  \n",
    "    \n",
    "    train_perplexity = 0\n",
    "    valid_perplexity = 0\n",
    "    \n",
    "    # Training Step\n",
    "    albert_pretrain_model = albert_pretrain_model.train()\n",
    "    for step, train_batch in tqdm(enumerate(train_dataloader), \n",
    "                                  dynamic_ncols=False, \n",
    "                                  bar_format=\"{n_fmt}/{total_fmt}{bar} ETA: {remaining}s - {desc}\", \n",
    "                                  total=len(train_dataloader),\n",
    "                                  leave=True, \n",
    "                                  unit='steps'):        \n",
    "        input_ids = train_batch['input_ids'].to(device)\n",
    "        attention_mask = train_batch['attention_mask'].to(device)\n",
    "        token_type_ids = train_batch['token_type_ids'].to(device)        \n",
    "        mlm_labels = train_batch['masked_lm_labels'].to(device)\n",
    "        sop_labels = train_batch['next_sentence_labels'].to(device)\n",
    "        \n",
    "        train_loss = training_step(\n",
    "            model=albert_pretrain_model, \n",
    "            input_ids=input_ids, \n",
    "            attention_mask=attention_mask, \n",
    "            token_type_ids=token_type_ids, \n",
    "            mlm_labels=mlm_labels, \n",
    "            sop_labels=sop_labels,\n",
    "            scaler=scaler,\n",
    "            use_multi_gpus=model_config[\"use_multi_gpus\"]\n",
    "        )\n",
    "        scheduler.step()\n",
    "        train_batch_loss += train_loss.item()\n",
    "        train_perplexity += torch.exp(train_loss)\n",
    "        \n",
    "        #if model_config[\"use_multi_gpus\"]:\n",
    "        #    last_lr = optimizer.module.param_groups[0]['lr']\n",
    "        #else:\n",
    "        #last_lr = optimizer.param_groups[0]['lr']\n",
    "        last_lr = scheduler.optimizer.param_groups[0][\"lr\"]\n",
    "          \n",
    "        if use_step_tracking:\n",
    "            record_step = (step + 1) + (len(train_dataloader)) * epoch\n",
    "            wandb.log({'learning_rate': last_lr}, step=record_step)\n",
    "            show_logs(\n",
    "                train_batch_loss / record_step, \n",
    "                record_step, \n",
    "                perplexity=train_perplexity.item() / record_step)\n",
    "    \n",
    "    save_model_checkpoint_path = str(save_model_path / f'{wandb.run.name}_{epoch}_model_weight.pt')\n",
    "    save_model(albert_pretrain_model, save_model_checkpoint_path)\n",
    "    \n",
    "    if use_epoch_tracking:\n",
    "        train_epoch_loss = train_batch_loss / step\n",
    "        wandb.log({'learning_rate': last_lr}, step=epoch)\n",
    "        show_log(train_epoch_loss, epoch, is_epoch=True)\n",
    "\n",
    "    end_time = time.time()\n",
    "    each_steps_compute_time = (end_time - start_time)\n",
    "    print(each_steps_compute_time)\n",
    "'''    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pursuant-cinema",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addressed-lithuania",
   "metadata": {},
   "source": [
    "# Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "positive-prediction",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broke-flexibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_models_path = main_model_path / wandb.run.name\n",
    "if not save_models_path.exists():\n",
    "    save_models_path.mkdir()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "realistic-vertex",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': albert_pretrain_model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "}, str(save_models_path / 'jp_pretrain_model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inside-swift",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(albert_pretrain_model.state_dict(), str(save_models_path / 'jp_pretrain_model_weight.pt'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "severe-recipient",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(str(save_models_path / 'jp_pretrain_model_weight.pt'))\n",
    "albert_pretrain_model.load_state_dict(checkpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entire-serial",
   "metadata": {},
   "outputs": [],
   "source": [
    "albert_pretrain_model.module.state_dict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excess-reaction",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seasonal-candle",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert 1 == 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recorded-tackle",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "muslim-section",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('[RUN ID]: {}'.format(run_id))\n",
    "torch.cuda.empty_cache()\n",
    "use_epoch_tracking = False\n",
    "use_step_tracking = True\n",
    "#wandb.watch(sms_model, log=\"all\", log_freq=1000)        \n",
    "def show_logs(loss, step, is_epoch=False, prefix='Train', **kwargs):\n",
    "    loss = float(loss)\n",
    "    if is_epoch:\n",
    "        wandb.log({\"epoch\": step, f\"{prefix}_loss\": loss}, step=step)\n",
    "    else:\n",
    "        wandb.log({f\"{prefix}_step_loss\": loss}, step=step)\n",
    "        #print(f\"{prefix} loss after \" + str(example_ct).zfill(5) + f\" examples: {loss:.3f}\")\n",
    "    if \"perplexity\" in kwargs.keys():\n",
    "        wandb.log({f\"{prefix}_perplexity\": kwargs[\"perplexity\"]}, step=step)\n",
    "\n",
    "# Creates a GradScaler once at the beginning of training.\n",
    "scaler = GradScaler()\n",
    "\n",
    "for epoch in tqdm(range(model_config['epochs'])): # model_config['epochs']\n",
    "    start_time = time.time()    \n",
    "    train_batch_loss = 0\n",
    "    valid_batch_loss = 0  \n",
    "    \n",
    "    train_perplexity = 0\n",
    "    valid_perplexity = 0\n",
    "    \n",
    "    # Training Step\n",
    "    albert_pretrain_model = albert_pretrain_model.train()\n",
    "    for step, train_batch in tqdm(enumerate(train_dataloader), \n",
    "                                  dynamic_ncols=False, \n",
    "                                  bar_format=\"{n_fmt}/{total_fmt}{bar} ETA: {remaining}s - {desc}\", \n",
    "                                  total=len(train_dataloader),\n",
    "                                  leave=True, \n",
    "                                  unit='steps'):        \n",
    "        input_ids = train_batch['input_ids'].to(device)\n",
    "        attention_mask = train_batch['attention_mask'].to(device)\n",
    "        token_type_ids = train_batch['token_type_ids'].to(device)        \n",
    "        mlm_labels = train_batch['masked_lm_labels'].to(device)\n",
    "        sop_labels = train_batch['next_sentence_labels'].to(device)\n",
    "        \n",
    "        train_loss = training_step(\n",
    "            model=albert_pretrain_model, \n",
    "            input_ids=input_ids, \n",
    "            attention_mask=attention_mask, \n",
    "            token_type_ids=token_type_ids, \n",
    "            mlm_labels=mlm_labels, \n",
    "            sop_labels=sop_labels,\n",
    "            scaler=scaler,\n",
    "            use_multi_gpus=model_config[\"use_multi_gpus\"]\n",
    "        )\n",
    "        scheduler.step()\n",
    "        train_batch_loss += train_loss.item()\n",
    "        train_perplexity += torch.exp(train_loss)\n",
    "        \n",
    "        #if model_config[\"use_multi_gpus\"]:\n",
    "        #    last_lr = optimizer.module.param_groups[0]['lr']\n",
    "        #else:\n",
    "        #last_lr = optimizer.param_groups[0]['lr']\n",
    "        last_lr = scheduler.optimizer.param_groups[0][\"lr\"]\n",
    "          \n",
    "        if use_step_tracking:\n",
    "            record_step = (step + 1) * (epoch + 1)\n",
    "            wandb.log({'learning_rate': last_lr}, step=record_step)\n",
    "            show_logs(\n",
    "                train_batch_loss / record_step, \n",
    "                record_step, \n",
    "                perplexity=train_perplexity.item() / record_step)\n",
    "    \n",
    "    if use_epoch_tracking:\n",
    "        train_epoch_loss = train_batch_loss / step\n",
    "        wandb.log({'learning_rate': last_lr}, step=epoch)\n",
    "        show_log(train_epoch_loss, epoch, is_epoch=True)  \n",
    "        #train_metric_records = metric_collection.compute()\n",
    "        #wandb.log(train_metric_records, step=epoch)\n",
    "    \n",
    "    # Validation Step\n",
    "    albert_pretrain_model = albert_pretrain_model.eval()\n",
    "    for step, valid_batch in tqdm(enumerate(val_dataloader), \n",
    "                                dynamic_ncols=False, \n",
    "                                bar_format=\"{n_fmt}/{total_fmt}{bar} ETA: {remaining}s - {desc}\", \n",
    "                                total=len(val_dataloader),\n",
    "                                leave=True, \n",
    "                                unit='steps'):            \n",
    "        input_ids = valid_batch['input_ids'].to(device)\n",
    "        attention_mask = valid_batch['attention_mask'].to(device)\n",
    "        token_type_ids = valid_batch['token_type_ids'].to(device)\n",
    "        mlm_labels = valid_batch['masked_lm_labels'].to(device)\n",
    "        sop_labels = valid_batch['next_sentence_labels'].to(device)\n",
    "            \n",
    "        valid_loss = validataion_step(\n",
    "            model=albert_pretrain_model, \n",
    "            input_ids=input_ids, \n",
    "            attention_mask=attention_mask, \n",
    "            token_type_ids=token_type_ids, \n",
    "            mlm_labels=mlm_labels,\n",
    "            sop_labels=sop_labels\n",
    "        )\n",
    "        valid_batch_loss += valid_loss.item()\n",
    "        valid_perplexity += torch.exp(valid_loss)\n",
    "        \n",
    "        if use_step_tracking:\n",
    "            record_step = (step + 1) * (epoch + 1)\n",
    "            show_logs(\n",
    "                valid_batch_loss / record_step, \n",
    "                record_step, \n",
    "                prefix='valid', \n",
    "                perplexity=valid_perplexity.item() / record_step)\n",
    "            \n",
    "        #sk_metrics = sklearn_metrics(val_outputs, labels, 'train')\n",
    "        #ic(sk_metrics)\n",
    "        #ic(val_metric_collection(outputs, labels).compute())\n",
    "        #ic(val_metric_collection(outputs, labels))\n",
    "        \n",
    "    if use_epoch_tracking:        \n",
    "        valid_epoch_loss = valid_batch_loss / step \n",
    "        show_logs(valid_epoch_loss, epoch, is_epoch=True, prefix='Val')\n",
    "        #val_metric_records = val_metric_collection.compute()\n",
    "        #wandb.log(val_metric_records, step=epoch)\n",
    "    \n",
    "    loss_template = (\"Epoch {}/{} - {:.0f}s {:.0f}ms/step - lr:{:} - loss: {:.6f} - val_loss: {:.6f}\")  \n",
    "    #metrics_template = (\n",
    "    #    \"\"\"\n",
    "    #    categorical_accuracy: {:.4f} - f1_score: {:.4f} - multi_precision: {:.4f} - multi_recall: {:.4f}\n",
    "    #    val_categorical_accuracy: {:.4f} -  val_f1_score: {:.4f} - val_multi_precision: {:.4f} - val_multi_recall: {:.4f}\n",
    "    #    \"\"\"\n",
    "    #)\n",
    "    end_time = time.time()\n",
    "    each_steps_compute_time = (end_time - start_time)\n",
    "    print(loss_template.format(\n",
    "        epoch,\n",
    "        model_config['epochs'], \n",
    "        each_steps_compute_time,\n",
    "        each_steps_compute_time * 1000 / model_config['training_steps'],\n",
    "        last_lr,\n",
    "        train_epoch_loss,\n",
    "        val_epoch_loss)\n",
    "    )\n",
    "\n",
    "    #print(metrics_template.format(\n",
    "    #    train_metric_records['Train_Accuracy'],\n",
    "    #    train_metric_records['Train_F1'],\n",
    "    #    train_metric_records['Train_Precision'],\n",
    "    #    train_metric_records['Train_Recall'],\n",
    "    #    val_metric_records['Val_Accuracy'],\n",
    "    #    val_metric_records['Val_F1'],\n",
    "    #    val_metric_records['Val_Precision'],\n",
    "    #    val_metric_records['Val_Recall']\n",
    "    #))\n",
    "    \n",
    "    if use_epoch_tracking:\n",
    "        metric_collection.reset()\n",
    "        val_metric_collection.reset()\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dimensional-shooting",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sound-whale",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import math\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "from torch import nn\n",
    "from torch import cuda\n",
    "from torch import optim\n",
    "from torch.optim.swa_utils import AveragedModel, SWALR\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, CyclicLR\n",
    "from pl_bolts.optimizers.lr_scheduler import LinearWarmupCosineAnnealingLR\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "net = NeuralNetwork()\n",
    "\n",
    "optimizer = optim.SGD(net.parameters(), lr = 1e-2)\n",
    "lambda1 = lambda epoch: 0.2 if epoch % 5 == 0 else 1\n",
    "lambda2 = lambda epoch: 0.2\n",
    "\n",
    "#scheduler = optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda = lambda2)\n",
    "#scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[5,10,15], gamma=0.1)\n",
    "#scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma = 0.9)\n",
    "#scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda step: PolynomialDecay(step))\n",
    "\n",
    "class PolynomialDecay(_LRScheduler):\n",
    "    def __init__(self, optimizer, decay_steps, end_learning_rate=0.0001, power=0.5, cycle=False, last_epoch=-1, verbose=False):\n",
    "        if decay_steps <= 1.:\n",
    "            raise ValueError('max_decay_steps should be greater than 1.')            \n",
    "        self.decay_steps = decay_steps\n",
    "        self.end_learning_rate = end_learning_rate\n",
    "        self.power = power\n",
    "        self.cycle = cycle\n",
    "        super(PolynomialDecay, self).__init__(optimizer, last_epoch, verbose)\n",
    "    \n",
    "    def get_lr(self):\n",
    "        if not self._get_lr_called_within_step:\n",
    "            warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
    "                          \"please use `get_last_lr()`.\")\n",
    "        #dtype = initial_learning_rate.dtype\n",
    "        #end_learning_rate = math_ops.cast(self.end_learning_rate, dtype)\n",
    "        #power = math_ops.cast(self.power, dtype)\n",
    "        #global_step_recomp = math_ops.cast(step, dtype)\n",
    "        #decay_steps_recomp = math_ops.cast(self.decay_steps, dtype)\n",
    "        global_step_recomp = self.last_epoch\n",
    "        decay_steps_recomp = self.decay_steps\n",
    "        \n",
    "        if self.cycle:\n",
    "            if global_step_recomp == 0:\n",
    "                multiplier = 1.0 \n",
    "            else:\n",
    "                multiplier = math.ceil(global_step_recomp / self.decay_steps)\n",
    "            decay_steps_recomp = decay_steps_recomp * multiplier\n",
    "        else:\n",
    "            global_step_recomp = min(global_step_recomp, decay_steps_recomp)\n",
    "            \n",
    "        p = global_step_recomp / decay_steps_recomp\n",
    "        #c(self.last_epoch, optimizer.param_groups[0]['lr'], p)\n",
    "        return [((group['lr'] - self.end_learning_rate) * math.pow(1 - p, self.power) + self.end_learning_rate) for group in self.optimizer.param_groups]\n",
    "    \n",
    "    def _get_closed_form_lr(self):\n",
    "        return [(base_lr - self.end_learning_rate) * math.pow(1 - p, self.power) + self.end_learning_rate for base_lr in self.base_lrs]\n",
    "\n",
    "\n",
    "    \n",
    "def polynomial_decay_scale_fun(global_steps, initial_learning_rate=1e-2, decay_steps=100, power=0.5, end_learning_rate=1e-5, cycle=False):\n",
    "    if cycle:\n",
    "        if global_steps == 0:\n",
    "            multiplier = 1.0 \n",
    "        else:\n",
    "            multiplier = math.ceil(global_steps / decay_steps)\n",
    "            decay_steps = decay_steps * multiplier\n",
    "    else:\n",
    "        global_steps = min(global_steps, decay_steps)\n",
    "    p = global_steps / decay_steps\n",
    "    #ic(global_steps, p)\n",
    "    return (initial_learning_rate - end_learning_rate) * math.pow(1 - p, power) + end_learning_rate\n",
    "    \n",
    "    \n",
    "#optimizer = optim.SGD(net.parameters(), lr=1e-2)\n",
    "optimizer = optim.Adam(net.parameters(), lr=1e-3)\n",
    "#scheduler = PolynomialDecay(optimizer, decay_steps=1000, end_learning_rate=1e-5)\n",
    " \n",
    "scheduler = LinearWarmupCosineAnnealingLR(\n",
    "    optimizer, \n",
    "    warmup_epochs=model_config['warmup_steps'], \n",
    "    max_epochs=model_config['training_steps'],\n",
    "    eta_min=model_config[\"end_learning_rate\"])\n",
    "\n",
    "#scheduler = optim.lr_scheduler.CyclicLR(\n",
    "#    optimizer, \n",
    "#    base_lr=1e-5,\n",
    "#    max_lr=1e-2,\n",
    "#    step_size_up=20,\n",
    "#    scale_fn=polynomial_decay_scale_fun,\n",
    "#    mode='triangular2',\n",
    "#    scale_mode='cycle',\n",
    "#    cycle_momentum=False)\n",
    "\n",
    "iteration = model_config['epochs']\n",
    "scheduler_lr_list = []\n",
    "for epoch in range(1, iteration):\n",
    "    scheduler.step()\n",
    "    #print(epoch, scheduler.get_last_lr()[0])\n",
    "    scheduler_lr_list.append(scheduler.get_last_lr()[0])\n",
    "\n",
    "plt.xlabel('Training Iterations')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.title(\"CLR - 'triangular' Policy\")\n",
    "plt.plot(range(1, iteration), scheduler_lr_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "small-balloon",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pregnant-terrorism",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-4.m65",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-4:m65"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
