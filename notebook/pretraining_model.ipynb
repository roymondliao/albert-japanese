{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7048f4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import time\n",
    "import os\n",
    "import gc\n",
    "import math\n",
    "import multiprocessing\n",
    "import io\n",
    "import logging \n",
    "import itertools\n",
    "import shutil\n",
    "import pysnooper\n",
    "import warnings\n",
    "import glob\n",
    "import pendulum\n",
    "import json\n",
    "import sys\n",
    "import wandb\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import scikitplot as skplt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from icecream import ic\n",
    "from collections import Counter\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from pathlib import Path\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import transformers\n",
    "import torchmetrics\n",
    "from torch import nn\n",
    "from torch import cuda\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "matplotlib.use('Agg')\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "seed = 9527\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.WARNING)\n",
    "\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "print('Torch version: ', torch.__version__)\n",
    "print('Device available:', torch.cuda.is_available())\n",
    "print('Device name:', torch.cuda.get_device_name(0))\n",
    "torch.set_printoptions(precision=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd562ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--input_file', default=None, help=\"Input raw text file (or comma-separated list of files).\")\n",
    "parser.add_argument('--output_file', default=None, help=\"Output TF example file (or comma-separated list of files).\")\n",
    "parser.add_argument('--vocab_file', default=None, help=\"The vocabulary file that the ALBERT model was trained on.\")\n",
    "parser.add_argument('--spm_model_file', default=None, help=\"The model file for sentence piece tokenization.\")\n",
    "parser.add_argument('--input_file_mode', default=\"r\",  help=\"The data format of the input file.\")\n",
    "parser.add_argument('--do_lower_case', default=True, help=\"Whether to lower case the input text. Should be True for uncased models and False for cased models.\")\n",
    "parser.add_argument('--do_whole_word_mask', default=True, help=\"Whether to use whole word masking rather than per-WordPiece masking.\")\n",
    "parser.add_argument('--do_permutation', default=False, help=\"Whether to do the permutation training.\")\n",
    "parser.add_argument('--favor_shorter_ngram', default=True, help=\"Whether to set higher probabilities for sampling shorter ngrams.\")\n",
    "parser.add_argument('--random_next_sentence', default=False, help=\"Whether to use the sentence that's right before the current sentence \"\n",
    "                    \"as the negative sample for next sentence prection, rather than using \"\n",
    "                    \"sentences from other random documents.\")\n",
    "parser.add_argument('--max_seq_length', default=512, help=\"Maximum sequence length.\")\n",
    "parser.add_argument('--ngram', default=3, help=\"Maximum number of ngrams to mask.\")\n",
    "parser.add_argument('--max_predictions_per_seq', default=20, help=\"Maximum number of masked LM predictions per sequence.\")\n",
    "parser.add_argument('--random_seed', default=12345, help=\"Random seed for data generation.\")\n",
    "parser.add_argument('--dupe_factor', default=5, help=\"Number of times to duplicate the input data (with different masks).\")\n",
    "parser.add_argument('--masked_lm_prob', default=0.15, help=\"Masked LM probability.\")\n",
    "parser.add_argument('--short_seq_prob', default=0.1, help=\"Probability of creating sequences which are shorter than the maximum length.\")\n",
    "\n",
    "\n",
    "opt = parser.parse_args(args=[\n",
    "    '--input_file', '1995_income',  \n",
    "    '--output_file', 'MLP',\n",
    "    '--spm_model_file', './wiki-ja_albert.model',\n",
    "    '--vocab_file', './wiki-ja_albert.vocab',\n",
    "    '--do_whole_word_mask', False,\n",
    "    '--do_permutation', False,\n",
    "    '--favor_shorter_ngram', False,\n",
    "    '--random_next_sentenc', False\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6be4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 202105\n",
    "\n",
    "# main'\n",
    "main_path = Path('/home/jupyter/gogolook')\n",
    "main_cached_path = Path('/home/jupyter/gogolook/data')\n",
    "\n",
    "# general setting\n",
    "main_data_path = main_path / 'data' / 'jp_data' \n",
    "main_model_path = main_path / 'models'\n",
    "cache_data_path = main_cached_path / 'cache_data_dir'\n",
    "cache_models_path = main_cached_path / 'cache_models_fir'\n",
    "\n",
    "# models\n",
    "albert_zh_path = main_model_path / 'albert_zh'\n",
    "\n",
    "# data\n",
    "regex_file_format = '*.json'\n",
    "data_tag = 'pretraining_data'\n",
    "valid_data_tag = 'pretraining_data'\n",
    "test_data_tag = 'pretraining_data'\n",
    "\n",
    "experiment_train_data_path = main_data_path / f'train_{data_tag}'    \n",
    "experiment_valid_data_path = main_data_path / f'valid_{valid_data_tag}'\n",
    "experiment_test_data_path = main_data_path / f'test_{test_data_tag}'\n",
    "\n",
    "training_data_path = experiment_train_data_path\n",
    "validation_data_path = experiment_valid_data_path\n",
    "testing_data_path = experiment_test_data_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46b9597",
   "metadata": {},
   "outputs": [],
   "source": [
    "project = 'jp-pretrain-model'\n",
    "project_shortname = 'jp-sms'\n",
    "group_tag = 'experiment' # 1. functional 2. experiment 3. staging 4. production\n",
    "job_type = 'baseline' # 1. baseline 2. optimize 3. hyper-tuning\n",
    "addition_tag = [data_tag, 'pytorch'] # exponential_decay\n",
    "method_tag = 'pretrain' # pretrain / finetune / pretrain_finetune\n",
    "time_tag = pendulum.now(tz='Asia/Taipei').strftime('%Y%m%d%H%M%S')\n",
    "run_id = '{}_{}_{}'.format(project, job_type, time_tag)\n",
    "print('Run id is {}'.format(run_id))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21a483a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "from datasets import load_dataset\n",
    "from datasets import total_allocated_bytes\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, Optional, Union, List\n",
    "from transformers import (\n",
    "    BertTokenizer, AlbertForPreTraining, AlbertModel, AlbertConfig, PreTrainedTokenizer)\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer, BertJapaneseTokenizer\n",
    "#import datasets\n",
    "#datasets.logging.set_verbosity_info()\n",
    "#datasets.logging.get_verbosity()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d75d52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mecab_tokenizer = BertJapaneseTokenizer.from_pretrained(\"cl-tohoku/bert-base-japanese\", word_tokenizer_type=\"mecab\", cache_dir=cache_models_path)\n",
    "# Input Japanese Text\n",
    "line = \"アンパサンド (&、英語名：) とは並立助詞「…と…」を意味する記号である。ラテン語の の合字で、Trebuchet MSフォントでは、と表示され \\\"et\\\" の合字であることが容易にわかる。\"\n",
    "mecab_inputs = mecab_tokenizer(line, return_tensors=\"pt\")\n",
    "print(mecab_tokenizer.decode(mecab_inputs['input_ids'][0]))\n",
    "corpus_size = len(mecab_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8fc92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mecab_tokenizer.max_model_input_sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce87083",
   "metadata": {},
   "source": [
    "# Model config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b8ff48",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_config = {\n",
    "    \"SGD\": {\n",
    "        \"init_learning_rate\": 1e-1, \n",
    "        \"pre_finetune_learning_rate\": 1e-3\n",
    "    },\n",
    "    \"Adam\": {\n",
    "        \"init_learning_rate\": 1e-3,\n",
    "        \"pre_finetune_learning_rate\": 1e-5\n",
    "    }, \n",
    "    \"RAdam\": {\n",
    "        \"init_learning_rate\": 1e-3,\n",
    "        \"pre_finetune_learning_rate\": 2e-5\n",
    "    }     \n",
    "}\n",
    "\n",
    "model_config = {\n",
    "    \"epochs\": 5,\n",
    "    \"initial_epochs\": 20,\n",
    "    \"batch_size\": 32,\n",
    "    \"max_tokens_length\": 512,\n",
    "    \"threshold\": 0.5,\n",
    "    \"optimizer_method\": \"Adam\",\n",
    "    \"init_learning_rate\": optimizer_config['Adam']['init_learning_rate'],\n",
    "    \"pre_finetune_learning_rate\": optimizer_config['Adam']['pre_finetune_learning_rate'],\n",
    "    \"end_learning_rate\": 1e-5,\n",
    "    \"lsm\": 0.0,\n",
    "    \"hidden_dropout_prob\": 0.1,\n",
    "    \"use_warmup\": False,\n",
    "    \"use_multi_gpus\": True\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb8fff6",
   "metadata": {},
   "source": [
    "\n",
    "# Data loader pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8f73e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.multiprocessing.set_start_method('spawn')\n",
    "#datasets.config.IN_MEMORY_MAX_SIZE\n",
    "@dataclass(eq=False)\n",
    "class GenerateDatasets: \n",
    "    #files_list: str = field(\n",
    "    #    default=None, metadata={\"help\": \"The files list of data path\"}\n",
    "    #)\n",
    "    data_path: str = field(\n",
    "        default=None, metadata={\"help\": \"The prefix path of files location\"}\n",
    "    )\n",
    "    regex_file_format: str = field(\n",
    "        default='*.parquet', metadata={\"help\": \"The files format.\"}\n",
    "    )\n",
    "    batch_size: int = field(\n",
    "        default=128, metadata={\"help\": \"Batch size\"}\n",
    "    )\n",
    "    is_training: bool = field(\n",
    "        default=True, metadata={\"help\": \"Is use training mode to create data pipeline\"}\n",
    "    )\n",
    "    device: str = field(\n",
    "        default='cpu', metadata={\"help\": \"Which device to use [cpu, cuda]\"}\n",
    "    )\n",
    "    cache_data_path: str = field(\n",
    "        default=None, metadata={\"help\": \"The path to cache data.\"}\n",
    "    )\n",
    "        \n",
    "    def __post_init__(self):\n",
    "        self.get_files_list = glob.glob(os.path.join(str(self.data_path), self.regex_file_format))\n",
    "        #self.get_files_list = '/home/jupyter/gogolook/data/jp_data/valid_pretraining_data/valid_all-maxseq512_BG.parquet'\n",
    "        self.encoding_columns = ['input_ids', 'token_type_ids', 'attention_mask']\n",
    "        self.target_columns = ['masked_lm_labels', 'next_sentence_labels']\n",
    "        \n",
    "    def __call__(self, **kwargs):\n",
    "        # data 已經存在 device (cuda) 裡，所以再用 pin_memory 會出現 error\n",
    "        # RuntimeError: cannot pin 'torch.cuda.LongTensor' only dense CPU tensors can be pinned        \n",
    "        dataset = load_dataset('parquet', data_files=self.get_files_list, cache_dir=self.cache_data_path, split='train')\n",
    "        dataset.set_format(type='torch', columns=self.encoding_columns + self.target_columns) # , device=self.device\n",
    "        #dataset = dataset.rename_column(self.target_column, 'labels')\n",
    "        if self.is_training:\n",
    "            drop_last = True\n",
    "        else: \n",
    "            drop_last = False\n",
    "            \n",
    "        dataloader = torch.utils.data.DataLoader(\n",
    "            dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            pin_memory=True,\n",
    "            shuffle=True,\n",
    "            drop_last=drop_last,\n",
    "            num_workers=multiprocessing.cpu_count())\n",
    "        return dataloader\n",
    "        \n",
    "\n",
    "get_train_dataset = GenerateDatasets(\n",
    "    data_path=training_data_path,\n",
    "    batch_size=model_config['batch_size'],\n",
    "    is_training=True,\n",
    "    device=device,\n",
    "    cache_data_path=cache_data_path)\n",
    "\n",
    "get_valid_dataset = GenerateDatasets(\n",
    "    data_path=training_data_path,\n",
    "    batch_size=model_config['batch_size'],\n",
    "    is_training=False,\n",
    "    device=device,\n",
    "    cache_data_path=cache_data_path)\n",
    "\n",
    "train_dataloader = get_train_dataset()\n",
    "#val_dataloader = get_valid_dataset()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9cdbe29",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_train_dataset.get_files_list\n",
    "#get_valid_dataset.get_files_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed86cba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68cfe961",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "albert_config = AlbertConfig.from_json_file(albert_zh_path / 'albert_config' / 'albert_config_tiny.json')\n",
    "pretrained_model_name_or_path = 'voidful/albert_chinese_tiny'\n",
    "albert_pretrain_model = AlbertForPreTraining.from_pretrained(\n",
    "    pretrained_model_name_or_path, \n",
    "    config=albert_config,             \n",
    "    cache_dir=cache_models_path)\n",
    "albert_pretrain_model.resize_token_embeddings(corpus_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c9111e",
   "metadata": {},
   "outputs": [],
   "source": [
    "albert_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7207a34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_config[\"use_multi_gpus\"]:\n",
    "    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    device_ids = [idx for idx in range(torch.cuda.device_count())]\n",
    "    albert_pretrain_model = nn.DataParallel(albert_pretrain_model, device_ids=device_ids)\n",
    "albert_pretrain_model.to(device)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac7b6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = list(albert_pretrain_model.named_parameters())\n",
    "optimizer_grounded_parameters_by_name = [\n",
    "    {'params': [n for n, p in model_params if not any(nd in n for nd in ['bias', 'gamma', 'beta'])], \n",
    "     'weight_decay_rate': 1e-2  },\n",
    "    {'params': [n for n, p in model_params if any(nd in n for nd in ['bias', 'gamma', 'beta'])], \n",
    "     'weight_decay_rate': 0.0 }\n",
    "]\n",
    "\n",
    "optimizer_grounded_parameters_by_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8846bc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = list(albert_pretrain_model.named_parameters())\n",
    "\n",
    "optimizer_grounded_parameters = [\n",
    "    {'params': [p for n, p in model_params if not any(nd in n for nd in ['bias', 'gamma', 'beta'])], \n",
    "     'weight_decay_rate': 1e-2  },\n",
    "    {'params': [p for n, p in model_params if any(nd in n for nd in ['bias', 'gamma', 'beta'])], \n",
    "     'weight_decay_rate': 0.0 }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9695c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "from torch.optim.swa_utils import AveragedModel, SWALR\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, CyclicLR\n",
    "        \n",
    "optimizer = torch.optim.Adam(\n",
    "    #params=optimizer_grounded_parameters, \n",
    "    params=albert_pretrain_model.parameters(),\n",
    "    lr=1e-5,\n",
    "    betas=(0.9, 0.98),\n",
    "    weight_decay=0.0,\n",
    "    eps=1e-6)\n",
    "\n",
    "#optimizer = optim.SGD(sms_model.parameters(), lr=model_config['init_learning_rate'], weight_decay=1e-4)\n",
    "#optimizer = optim.SGD(filter(lambda p: p.requires_grad, sms_model.parameters()), lr=model_config['init_learning_rate'], weight_decay=1e-4)\n",
    "\n",
    "#scheduler = CyclicLR(\n",
    "#    optimizer, \n",
    "#    base_lr=1e-5,\n",
    "#    max_lr=model_config['init_learning_rate'],\n",
    "#    step_size_up=model_config['training_steps'] * 1,\n",
    "#    mode='triangular2',\n",
    "#    scale_mode='cycle',\n",
    "#    cycle_momentum=False\n",
    "#)\n",
    "\n",
    "if model_config[\"use_multi_gpus\"]:\n",
    "    optimizer = nn.DataParallel(optimizer, device_ids=device_ids)\n",
    "    \n",
    "#swa_model = AveragedModel(model)\n",
    "#scheduler = CosineAnnealingLR(optimiezer, eta_min=1e-5, T_max=10)\n",
    "#swa_start = 5\n",
    "#swa_scheduler = SWALR(optimizer, swa_lr=0.05)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103427b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Mertice\n",
    "from torchmetrics import MetricCollection\n",
    "metric_collection = MetricCollection([\n",
    "    torchmetrics.Accuracy(num_classes=2, average='macro', multiclass=True, dist_sync_on_step=True, mdmc_average='global').to(device),\n",
    "    torchmetrics.Precision(num_classes=2, average='macro', multiclass=True, dist_sync_on_step=True, mdmc_average='global').to(device),\n",
    "    torchmetrics.Recall(num_classes=2, average='macro', multiclass=True, dist_sync_on_step=True, mdmc_average='global').to(device),\n",
    "    torchmetrics.F1(num_classes=2, average='macro', multiclass=True, dist_sync_on_step=True, mdmc_average='global').to(device)\n",
    "], prefix='Train_')\n",
    "\n",
    "val_metric_collection = MetricCollection([\n",
    "    torchmetrics.Accuracy(num_classes=2, average='macro', multiclass=True, dist_sync_on_step=True, mdmc_average='global').to(device),\n",
    "    torchmetrics.Precision(num_classes=2, average='macro', multiclass=True, dist_sync_on_step=True, mdmc_average='global').to(device),\n",
    "    torchmetrics.Recall(num_classes=2, average='macro', multiclass=True, dist_sync_on_step=True, mdmc_average='global').to(device),\n",
    "    torchmetrics.F1(num_classes=2, average='macro', multiclass=True, dist_sync_on_step=True, mdmc_average='global').to(device),\n",
    "], prefix='Val_')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58d2cea",
   "metadata": {},
   "source": [
    "# Training model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46cf3d8e",
   "metadata": {},
   "source": [
    "### Init wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5dc8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_config['use_warmup']:\n",
    "    model_config['warmup_steps'] = int(len(train_dataloader) * model_config['epochs'] * 0.1)\n",
    "    model_config['decay_steps'] = len(train_dataloader) * model_config['epochs']\n",
    "else:\n",
    "    model_config['warmup_steps'] = None \n",
    "    model_config['decay_steps'] = None\n",
    "model_config['training_steps'] = len(train_dataloader)\n",
    "\n",
    "wandb.init(\n",
    "    project=project,\n",
    "    group=group_tag,\n",
    "    job_type=job_type,\n",
    "    name=run_id,\n",
    "    notes=method_tag,\n",
    "    tags=addition_tag,\n",
    "    sync_tensorboard=False,\n",
    "    config={**model_config},\n",
    "    reinit=True    \n",
    ")\n",
    "\n",
    "wandb_config = wandb.config\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c2d501",
   "metadata": {},
   "source": [
    "### Simple test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8cc76b5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "prefix = 'train'\n",
    "for epoch in tqdm(range(10)): # model_config['epochs']\n",
    "    start_time = time.time()    \n",
    "    train_batch_loss = 0\n",
    "    val_batch_loss = 0    \n",
    "    \n",
    "    # Training Step\n",
    "    albert_pretrain_model = albert_pretrain_model.train()\n",
    "    for step, train_batch in tqdm(enumerate(train_dataloader), \n",
    "                                  dynamic_ncols=False, \n",
    "                                  bar_format=\"{n_fmt}/{total_fmt}{bar} ETA: {remaining}s - {desc}\", \n",
    "                                  total=len(train_dataloader),\n",
    "                                  leave=True, \n",
    "                                  unit='steps'):        \n",
    "        input_ids = train_batch['input_ids'].to(device)\n",
    "        attention_mask = train_batch['attention_mask'].to(device)\n",
    "        token_type_ids = train_batch['token_type_ids'].to(device)\n",
    "\n",
    "        mlm_labels = train_batch['masked_lm_labels'].to(device)\n",
    "        sop_labels = train_batch['next_sentence_labels'].to(device)\n",
    "        \n",
    "        outputs = albert_pretrain_model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            labels=mlm_labels,\n",
    "            sentence_order_label=sop_labels\n",
    "        )\n",
    "                    \n",
    "        loss = outputs.loss.mean()\n",
    "        perplexity  = torch.exp(loss)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        if model_config[\"use_multi_gpus\"]:\n",
    "            optimizer.module.step()\n",
    "        else:\n",
    "            optimizer.step()\n",
    "            \n",
    "        wandb.log({\n",
    "            \"loss\": loss,\n",
    "            \"perplexity\": perplexity,            \n",
    "        }, step=step)\n",
    "        \n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62faa4be",
   "metadata": {},
   "source": [
    "### Define training stepsm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab94870",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_step(model, input_ids, attention_mask, token_type_ids, mlm_labels, sop_labels, use_multi_gpus=False):\n",
    "    # Forward pass\n",
    "    outputs = model(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        token_type_ids=token_type_ids,\n",
    "        labels=mlm_labels,\n",
    "        sentence_order_label=sop_labels\n",
    "    )\n",
    "    loss = outputs.loss.mean()\n",
    "    #if ((step + 1) % 100) == 0:\n",
    "    #    show_log(train_batch_loss / step, example_count, step)\n",
    "\n",
    "    # Backward pass\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    if use_multi_gpus:\n",
    "        optimizer.module.step()\n",
    "    else:\n",
    "        optimizer.step()\n",
    "    #scheduler.step()\n",
    "    #torch.nn.utils.clip_grad_norm_(optimizer_grounded_parameters, max_norm=0.5)    \n",
    "    #if epoch > swa_start:\n",
    "    #    swa_model.update_parameters(model)\n",
    "    #    swa_scheduler.step()\n",
    "    #else:\n",
    "    #    scheduler.step()\n",
    "    return loss\n",
    "\n",
    "@torch.no_grad()\n",
    "def validataion_step(model, input_ids, attention_mask, token_type_ids, mlm_labels, sop_labels):    \n",
    "    model.eval()\n",
    "    outputs = model(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        token_type_ids=token_type_ids,\n",
    "        labels=mlm_labels,\n",
    "        sentence_order_label=sop_labels)\n",
    "    loss = outputs.loss.mean()\n",
    "    return loss\n",
    "\n",
    "@torch.no_grad()\n",
    "def testing_step(model, dataset_inputs):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2838a058",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada02b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('[RUN ID]: {}'.format(run_id))\n",
    "\n",
    "use_epoch_tracking = False\n",
    "use_step_tracking = True\n",
    "#wandb.watch(sms_model, log=\"all\", log_freq=1000)        \n",
    "def show_logs(loss, step, is_epoch=False, prefix='Train', **kwargs):\n",
    "    loss = float(loss)\n",
    "    if is_epoch:\n",
    "        wandb.log({\"epoch\": step, f\"{prefix}_loss\": loss}, step=step)\n",
    "    else:\n",
    "        wandb.log({f\"{prefix}_step_loss\": loss}, step=step)\n",
    "        #print(f\"{prefix} loss after \" + str(example_ct).zfill(5) + f\" examples: {loss:.3f}\")\n",
    "    if \"perplexity\" in kwargs.keys():\n",
    "        wandb.log({f\"{prefix}_perplexity\": kwargs[\"perplexity\"]}, step=step)\n",
    "    \n",
    "total_batches = len(train_dataloader) * model_config['epochs']\n",
    "\n",
    "for epoch in tqdm(range(model_config['epochs'])): # model_config['epochs']\n",
    "    start_time = time.time()    \n",
    "    train_batch_loss = 0\n",
    "    valid_batch_loss = 0  \n",
    "    \n",
    "    train_perplexity = 0\n",
    "    valid_perplexity = 0\n",
    "    \n",
    "    # Training Step\n",
    "    albert_pretrain_model = albert_pretrain_model.train()\n",
    "    for step, train_batch in tqdm(enumerate(train_dataloader), \n",
    "                                  dynamic_ncols=False, \n",
    "                                  bar_format=\"{n_fmt}/{total_fmt}{bar} ETA: {remaining}s - {desc}\", \n",
    "                                  total=len(train_dataloader),\n",
    "                                  leave=True, \n",
    "                                  unit='steps'):        \n",
    "        input_ids = train_batch['input_ids'].to(device)\n",
    "        attention_mask = train_batch['attention_mask'].to(device)\n",
    "        token_type_ids = train_batch['token_type_ids'].to(device)        \n",
    "        mlm_labels = train_batch['masked_lm_labels'].to(device)\n",
    "        sop_labels = train_batch['next_sentence_labels'].to(device)\n",
    "        \n",
    "        train_loss = training_step(\n",
    "            model=albert_pretrain_model, \n",
    "            input_ids=input_ids, \n",
    "            attention_mask=attention_mask, \n",
    "            token_type_ids=token_type_ids, \n",
    "            mlm_labels=mlm_labels, \n",
    "            sop_labels=sop_labels,\n",
    "            use_multi_gpus=model_config[\"use_multi_gpus\"])\n",
    "        train_batch_loss += train_loss.item()\n",
    "        train_perplexity += torch.exp(train_loss)\n",
    "        \n",
    "        if model_config[\"use_multi_gpus\"]:\n",
    "            last_lr = optimizer.module.param_groups[0]['lr']\n",
    "        else:\n",
    "            last_lr = optimizer.param_groups[0]['lr']\n",
    "          \n",
    "        if use_step_tracking:\n",
    "            record_step = (step + 1) * (epoch + 1)\n",
    "            wandb.log({'learning_rate': last_lr}, step=record_step)\n",
    "            show_logs(\n",
    "                train_batch_loss / record_step, \n",
    "                record_step, \n",
    "                perplexity=train_perplexity.item() / record_step)\n",
    "    \n",
    "    if use_epoch_tracking:\n",
    "        train_epoch_loss = train_batch_loss / step\n",
    "        wandb.log({'learning_rate': last_lr}, step=epoch)\n",
    "        show_log(train_epoch_loss, epoch, is_epoch=True)  \n",
    "        #train_metric_records = metric_collection.compute()\n",
    "        #wandb.log(train_metric_records, step=epoch)\n",
    "    \n",
    "    # Validation Step\n",
    "    albert_pretrain_model = albert_pretrain_model.eval()\n",
    "    for step, valid_batch in tqdm(enumerate(val_dataloader), \n",
    "                                dynamic_ncols=False, \n",
    "                                bar_format=\"{n_fmt}/{total_fmt}{bar} ETA: {remaining}s - {desc}\", \n",
    "                                total=len(val_dataloader),\n",
    "                                leave=True, \n",
    "                                unit='steps'):            \n",
    "        input_ids = valid_batch['input_ids'].to(device)\n",
    "        attention_mask = valid_batch['attention_mask'].to(device)\n",
    "        token_type_ids = valid_batch['token_type_ids'].to(device)\n",
    "        mlm_labels = valid_batch['masked_lm_labels'].to(device)\n",
    "        sop_labels = valid_batch['next_sentence_labels'].to(device)\n",
    "            \n",
    "        valid_loss = validataion_step(\n",
    "            model=albert_pretrain_model, \n",
    "            input_ids=input_ids, \n",
    "            attention_mask=attention_mask, \n",
    "            token_type_ids=token_type_ids, \n",
    "            mlm_labels=mlm_labels,\n",
    "            sop_labels=sop_labels\n",
    "        )\n",
    "        valid_batch_loss += valid_loss.item()\n",
    "        valid_perplexity += torch.exp(valid_loss)\n",
    "        \n",
    "        if use_step_tracking:\n",
    "            record_step = (step + 1) * (epoch + 1)\n",
    "            wandb.log({'learning_rate': last_lr}, step=record_step)\n",
    "            show_logs(\n",
    "                valid_batch_loss / record_step, \n",
    "                record_step, \n",
    "                prefix='valid', \n",
    "                perplexity=valid_perplexity.item() / record_step)\n",
    "            \n",
    "        #sk_metrics = sklearn_metrics(val_outputs, labels, 'train')\n",
    "        #ic(sk_metrics)\n",
    "        #ic(val_metric_collection(outputs, labels).compute())\n",
    "        #ic(val_metric_collection(outputs, labels))\n",
    "            \n",
    "    if use_epoch_tracking:        \n",
    "        valid_epoch_loss = valid_batch_loss / step \n",
    "        show_logs(valid_epoch_loss, epoch, is_epoch=True, prefix='Val')\n",
    "        #val_metric_records = val_metric_collection.compute()\n",
    "        #wandb.log(val_metric_records, step=epoch)\n",
    "    \n",
    "    loss_template = (\"Epoch {}/{} - {:.0f}s {:.0f}ms/step - lr:{:} - loss: {:.6f} - val_loss: {:.6f}\")    \n",
    "    #metrics_template = (\n",
    "    #    \"\"\"\n",
    "    #    categorical_accuracy: {:.4f} - f1_score: {:.4f} - multi_precision: {:.4f} - multi_recall: {:.4f}\n",
    "    #    val_categorical_accuracy: {:.4f} -  val_f1_score: {:.4f} - val_multi_precision: {:.4f} - val_multi_recall: {:.4f}\n",
    "    #    \"\"\"\n",
    "    #)\n",
    "    end_time = time.time()\n",
    "    each_steps_compute_time = (end_time - start_time)\n",
    "    print(loss_template.format(\n",
    "        epoch, model_config['epochs'], each_steps_compute_time, each_steps_compute_time * 1000 / model_config['training_steps'], \n",
    "        last_lr, train_epoch_loss, val_epoch_loss))\n",
    "\n",
    "    #print(metrics_template.format(\n",
    "    #    train_metric_records['Train_Accuracy'],\n",
    "    #    train_metric_records['Train_F1'],\n",
    "    #    train_metric_records['Train_Precision'],\n",
    "    #    train_metric_records['Train_Recall'],\n",
    "    #    val_metric_records['Val_Accuracy'],\n",
    "    #    val_metric_records['Val_F1'],\n",
    "    #    val_metric_records['Val_Precision'],\n",
    "    #    val_metric_records['Val_Recall']\n",
    "    #))\n",
    "    \n",
    "    if use_epoch_tracking:\n",
    "        metric_collection.reset()\n",
    "        val_metric_collection.reset()\n",
    "\n",
    "wandb.finish()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e7f769",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert 1 == 2"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-8.m73",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-8:m73"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
