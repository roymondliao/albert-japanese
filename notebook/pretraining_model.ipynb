{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7acaf109",
   "metadata": {},
   "outputs": [],
   "source": [
    "A#%pip install --user lightning-bolts -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64db446c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import time\n",
    "import os\n",
    "import gc\n",
    "import math\n",
    "import multiprocessing\n",
    "import io\n",
    "import logging \n",
    "import itertools\n",
    "import shutil\n",
    "import pysnooper\n",
    "import warnings\n",
    "import glob\n",
    "import pendulum\n",
    "import random\n",
    "import json\n",
    "import sys\n",
    "import wandb\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import scikitplot as skplt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from icecream import ic\n",
    "from collections import Counter\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from pathlib import Path\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import transformers\n",
    "import torchmetrics\n",
    "from torch import nn\n",
    "from torch import cuda\n",
    "\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "matplotlib.use('Agg')\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "seed = 9527\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.WARNING)\n",
    "\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "print('Torch version: ', torch.__version__)\n",
    "print('Device available:', torch.cuda.is_available())\n",
    "print('Device name:', torch.cuda.get_device_name(0))\n",
    "torch.set_printoptions(precision=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e54d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "torch.distributed.init_process_group(backend='nccl')\n",
    "\n",
    "local_rank = -1\n",
    "if local_rank not in [-1, 0]:\n",
    "    torch.distributed.barrier()  # Make sure only the first process in distributed training will download model & vocab\n",
    "    \n",
    "def setup(rank, world_size):\n",
    "    os.environ['MASTER_ADDR'] = 'localhost'\n",
    "    os.environ['MASTER_PORT'] = '12355'\n",
    "\n",
    "    # initialize the process group\n",
    "    dist.init_process_group(\"gloo\", rank=rank, world_size=world_size)\n",
    "'''    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f99f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_value = 9527\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "set_seed(seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4474757b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--input_file', default=None, help=\"Input raw text file (or comma-separated list of files).\")\n",
    "parser.add_argument('--output_file', default=None, help=\"Output TF example file (or comma-separated list of files).\")\n",
    "parser.add_argument('--vocab_file', default=None, help=\"The vocabulary file that the ALBERT model was trained on.\")\n",
    "parser.add_argument('--spm_model_file', default=None, help=\"The model file for sentence piece tokenization.\")\n",
    "parser.add_argument('--input_file_mode', default=\"r\",  help=\"The data format of the input file.\")\n",
    "parser.add_argument('--do_lower_case', default=True, help=\"Whether to lower case the input text. Should be True for uncased models and False for cased models.\")\n",
    "parser.add_argument('--do_whole_word_mask', default=True, help=\"Whether to use whole word masking rather than per-WordPiece masking.\")\n",
    "parser.add_argument('--do_permutation', default=False, help=\"Whether to do the permutation training.\")\n",
    "parser.add_argument('--favor_shorter_ngram', default=True, help=\"Whether to set higher probabilities for sampling shorter ngrams.\")\n",
    "parser.add_argument('--random_next_sentence', default=False, help=\"Whether to use the sentence that's right before the current sentence \"\n",
    "                    \"as the negative sample for next sentence prection, rather than using \"\n",
    "                    \"sentences from other random documents.\")\n",
    "parser.add_argument('--max_seq_length', default=512, help=\"Maximum sequence length.\")\n",
    "parser.add_argument('--ngram', default=3, help=\"Maximum number of ngrams to mask.\")\n",
    "parser.add_argument('--max_predictions_per_seq', default=20, help=\"Maximum number of masked LM predictions per sequence.\")\n",
    "parser.add_argument('--random_seed', default=12345, help=\"Random seed for data generation.\")\n",
    "parser.add_argument('--dupe_factor', default=5, help=\"Number of times to duplicate the input data (with different masks).\")\n",
    "parser.add_argument('--masked_lm_prob', default=0.15, help=\"Masked LM probability.\")\n",
    "parser.add_argument('--short_seq_prob', default=0.1, help=\"Probability of creating sequences which are shorter than the maximum length.\")\n",
    "\n",
    "\n",
    "opt = parser.parse_args(args=[\n",
    "    '--input_file', '1995_income',  \n",
    "    '--output_file', 'MLP',\n",
    "    '--spm_model_file', './wiki-ja_albert.model',\n",
    "    '--vocab_file', './wiki-ja_albert.vocab',\n",
    "    '--do_whole_word_mask', False,\n",
    "    '--do_permutation', False,\n",
    "    '--favor_shorter_ngram', False,\n",
    "    '--random_next_sentenc', False\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0418b1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 202105\n",
    "\n",
    "# main'\n",
    "main_path = Path('/home/jupyter/gogolook')\n",
    "main_cached_path = Path('/home/jupyter/gogolook/data')\n",
    "\n",
    "# general setting\n",
    "main_data_path = main_path / 'data' / 'jp_data' \n",
    "main_model_path = main_path / 'models'\n",
    "cache_data_path = main_cached_path / 'cache_data_dir'\n",
    "cache_models_path = main_cached_path / 'cache_models_fir'\n",
    "\n",
    "# models\n",
    "albert_zh_path = main_model_path / 'albert_zh'\n",
    "\n",
    "# data\n",
    "regex_file_format = '*.json'\n",
    "data_tag = 'pretraining_data'\n",
    "valid_data_tag = 'pretraining_data'\n",
    "test_data_tag = 'pretraining_data'\n",
    "\n",
    "experiment_train_data_path = main_data_path / f'train_{data_tag}'    \n",
    "experiment_valid_data_path = main_data_path / f'valid_{valid_data_tag}'\n",
    "experiment_test_data_path = main_data_path / f'test_{test_data_tag}'\n",
    "\n",
    "training_data_path = experiment_train_data_path\n",
    "validation_data_path = experiment_valid_data_path\n",
    "testing_data_path = experiment_test_data_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b283ad06",
   "metadata": {},
   "outputs": [],
   "source": [
    "project = 'jp-pretrain-model'\n",
    "project_shortname = 'jp-sms'\n",
    "group_tag = 'experiment' # 1. functional 2. experiment 3. staging 4. production\n",
    "job_type = 'baseline' # 1. baseline 2. optimize 3. hyper-tuning\n",
    "addition_tag = [data_tag, 'pytorch'] # exponential_decay\n",
    "method_tag = 'pretrain' # pretrain / finetune / pretrain_finetune\n",
    "time_tag = pendulum.now(tz='Asia/Taipei').strftime('%Y%m%d%H%M%S')\n",
    "run_id = '{}_{}_{}'.format(project, job_type, time_tag)\n",
    "print('Run id is {}'.format(run_id))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14da51c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "from datasets import load_dataset\n",
    "from datasets import total_allocated_bytes\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, Optional, Union, List\n",
    "from transformers import (\n",
    "    BertTokenizer, AlbertForPreTraining, AlbertModel, AlbertConfig, PreTrainedTokenizer, PreTrainedModel\n",
    ")\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer, BertJapaneseTokenizer\n",
    "from pl_bolts.optimizers.lr_scheduler import LinearWarmupCosineAnnealingLR\n",
    "\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch import optim\n",
    "from torch.optim.swa_utils import AveragedModel, SWALR\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, CyclicLR\n",
    "from transformers import get_polynomial_decay_schedule_with_warmup\n",
    "        \n",
    "#import datasets\n",
    "#datasets.logging.set_verbosity_info()\n",
    "#datasets.logging.get_verbosity()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a7b322",
   "metadata": {},
   "outputs": [],
   "source": [
    "mecab_tokenizer = BertJapaneseTokenizer.from_pretrained(\"cl-tohoku/bert-base-japanese\", word_tokenizer_type=\"mecab\", cache_dir=cache_models_path)\n",
    "# Input Japanese Text\n",
    "line = \"アンパサンド (&、英語名：) とは並立助詞「…と…」を意味する記号である。ラテン語の の合字で、Trebuchet MSフォントでは、と表示され \\\"et\\\" の合字であることが容易にわかる。\"\n",
    "mecab_inputs = mecab_tokenizer(line, return_tensors=\"pt\")\n",
    "print(mecab_tokenizer.decode(mecab_inputs['input_ids'][0]))\n",
    "corpus_size = len(mecab_tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a361654d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mecab_tokenizer.max_model_input_sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd607939",
   "metadata": {},
   "source": [
    "# Model config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c45811b",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_config = {\n",
    "    \"SGD\": {\n",
    "        \"init_learning_rate\": 1e-1, \n",
    "        \"pre_finetune_learning_rate\": 1e-3\n",
    "    },\n",
    "    \"Adam\": {\n",
    "        \"init_learning_rate\": 1e-3,\n",
    "        \"pre_finetune_learning_rate\": 1e-5\n",
    "    }, \n",
    "    \"RAdam\": {\n",
    "        \"init_learning_rate\": 1e-3,\n",
    "        \"pre_finetune_learning_rate\": 2e-5\n",
    "    }     \n",
    "}\n",
    "\n",
    "model_config = {\n",
    "    \"epochs\": 1,\n",
    "    \"initial_epochs\": 20,\n",
    "    \"batch_size\": 128,\n",
    "    \"max_tokens_length\": 512,\n",
    "    \"threshold\": 0.5,\n",
    "    \"optimizer_method\": \"Adam\",\n",
    "    \"init_learning_rate\": optimizer_config['Adam']['init_learning_rate'],\n",
    "    \"pre_finetune_learning_rate\": optimizer_config['Adam']['pre_finetune_learning_rate'],\n",
    "    \"end_learning_rate\": 1e-5,\n",
    "    \"lsm\": 0.0,\n",
    "    \"hidden_dropout_prob\": 0.1,\n",
    "    \"use_warmup\": True,\n",
    "    \"use_multi_gpus\": True\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9ccf5c",
   "metadata": {},
   "source": [
    "\n",
    "# Data loader pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e46049",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.multiprocessing.set_start_method('spawn')\n",
    "#datasets.config.IN_MEMORY_MAX_SIZE\n",
    "@dataclass(eq=False)\n",
    "class GenerateDatasets: \n",
    "    #files_list: str = field(\n",
    "    #    default=None, metadata={\"help\": \"The files list of data path\"}\n",
    "    #)\n",
    "    data_path: str = field(\n",
    "        default=None, metadata={\"help\": \"The prefix path of files location\"}\n",
    "    )\n",
    "    regex_file_format: str = field(\n",
    "        default='*.parquet', metadata={\"help\": \"The files format.\"}\n",
    "    )\n",
    "    batch_size: int = field(\n",
    "        default=128, metadata={\"help\": \"Batch size\"}\n",
    "    )\n",
    "    is_training: bool = field(\n",
    "        default=True, metadata={\"help\": \"Is use training mode to create data pipeline\"}\n",
    "    )\n",
    "    device: str = field(\n",
    "        default='cpu', metadata={\"help\": \"Which device to use [cpu, cuda]\"}\n",
    "    )\n",
    "    cache_data_path: str = field(\n",
    "        default=None, metadata={\"help\": \"The path to cache data.\"}\n",
    "    )\n",
    "        \n",
    "    def __post_init__(self):\n",
    "        self.get_files_list = glob.glob(os.path.join(str(self.data_path), self.regex_file_format))\n",
    "        #self.get_files_list = '/home/jupyter/gogolook/data/jp_data/valid_pretraining_data/valid_all-maxseq512_BG.parquet'\n",
    "        self.encoding_columns = ['input_ids', 'token_type_ids', 'attention_mask']\n",
    "        self.target_columns = ['masked_lm_labels', 'next_sentence_labels']\n",
    "        \n",
    "    def __call__(self, **kwargs):\n",
    "        # data 已經存在 device (cuda) 裡，所以再用 pin_memory 會出現 error\n",
    "        # RuntimeError: cannot pin 'torch.cuda.LongTensor' only dense CPU tensors can be pinned        \n",
    "        dataset = load_dataset('parquet', data_files=self.get_files_list, cache_dir=self.cache_data_path, split='train')\n",
    "        dataset.set_format(type='torch', columns=self.encoding_columns + self.target_columns) # , device=self.device\n",
    "        #dataset = dataset.rename_column(self.target_column, 'labels')\n",
    "        if self.is_training:\n",
    "            drop_last = True\n",
    "        else: \n",
    "            drop_last = False\n",
    "            \n",
    "        dataloader = torch.utils.data.DataLoader(\n",
    "            dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            pin_memory=True,\n",
    "            shuffle=True,\n",
    "            drop_last=drop_last,\n",
    "            num_workers=multiprocessing.cpu_count())\n",
    "        return dataloader\n",
    "        \n",
    "\n",
    "get_train_dataset = GenerateDatasets(\n",
    "    data_path=training_data_path,\n",
    "    batch_size=model_config['batch_size'],\n",
    "    is_training=True,\n",
    "    device=device,\n",
    "    cache_data_path=cache_data_path)\n",
    "\n",
    "get_valid_dataset = GenerateDatasets(\n",
    "    data_path=training_data_path,\n",
    "    batch_size=model_config['batch_size'],\n",
    "    is_training=False,\n",
    "    device=device,\n",
    "    cache_data_path=cache_data_path)\n",
    "\n",
    "train_dataloader = get_train_dataset()\n",
    "val_dataloader = get_valid_dataset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db052210",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_train_dataset.get_files_list\n",
    "#get_valid_dataset.get_files_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af57fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36ebd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_config['use_warmup']:\n",
    "    model_config['warmup_steps'] = int(len(train_dataloader) * model_config['epochs'] * 0.1)\n",
    "    model_config['decay_steps'] = len(train_dataloader) * model_config['epochs']\n",
    "else:\n",
    "    model_config['warmup_steps'] = None \n",
    "    model_config['decay_steps'] = None\n",
    "model_config['training_steps'] = len(train_dataloader)\n",
    "\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "albert_config = AlbertConfig.from_json_file(albert_zh_path / 'albert_config' / 'albert_config_tiny.json')\n",
    "pretrained_model_name_or_path = 'voidful/albert_chinese_tiny'\n",
    "albert_pretrain_model = AlbertForPreTraining.from_pretrained(\n",
    "    pretrained_model_name_or_path, \n",
    "    config=albert_config,             \n",
    "    cache_dir=cache_models_path)\n",
    "albert_pretrain_model.resize_token_embeddings(corpus_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2861575e",
   "metadata": {},
   "outputs": [],
   "source": [
    "albert_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ce571b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_config[\"use_multi_gpus\"]:\n",
    "    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    device_ids = [idx for idx in range(torch.cuda.device_count())]\n",
    "    albert_pretrain_model = nn.DataParallel(albert_pretrain_model, device_ids=device_ids)\n",
    "albert_pretrain_model.to(device)   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff75ced",
   "metadata": {},
   "source": [
    "# Define Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ecb5fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = list(albert_pretrain_model.named_parameters())\n",
    "optimizer_grounded_parameters_by_name = [\n",
    "    {'params': [n for n, p in model_params if not any(nd in n for nd in ['bias', 'gamma', 'beta'])], \n",
    "     'weight_decay_rate': 1e-2  },\n",
    "    {'params': [n for n, p in model_params if any(nd in n for nd in ['bias', 'gamma', 'beta'])], \n",
    "     'weight_decay_rate': 0.0 }\n",
    "]\n",
    "\n",
    "optimizer_grounded_parameters_by_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb35ef18",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = list(albert_pretrain_model.named_parameters())\n",
    "\n",
    "optimizer_grounded_parameters = [\n",
    "    {'params': [p for n, p in model_params if not any(nd in n for nd in ['bias', 'gamma', 'beta'])], \n",
    "     'weight_decay_rate': 1e-2  },\n",
    "    {'params': [p for n, p in model_params if any(nd in n for nd in ['bias', 'gamma', 'beta'])], \n",
    "     'weight_decay_rate': 0.0 }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b520464d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "\n",
    "class PolynomialDecay(_LRScheduler):\n",
    "    def __init__(self, optimizer, decay_steps, end_learning_rate=0.0001, power=0.5, cycle=False, last_epoch=-1, verbose=False):\n",
    "        if decay_steps <= 1.:\n",
    "            raise ValueError('max_decay_steps should be greater than 1.')            \n",
    "        self.decay_steps = decay_steps\n",
    "        self.end_learning_rate = end_learning_rate\n",
    "        self.power = power\n",
    "        self.cycle = cycle\n",
    "        super(PolynomialDecay, self).__init__(optimizer, last_epoch, verbose)\n",
    "    \n",
    "    def get_lr(self):\n",
    "        if not self._get_lr_called_within_step:\n",
    "            warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
    "                          \"please use `get_last_lr()`.\")\n",
    "        #dtype = initial_learning_rate.dtype\n",
    "        #end_learning_rate = math_ops.cast(self.end_learning_rate, dtype)\n",
    "        #power = math_ops.cast(self.power, dtype)\n",
    "        #global_step_recomp = math_ops.cast(step, dtype)\n",
    "        #decay_steps_recomp = math_ops.cast(self.decay_steps, dtype)\n",
    "        global_step_recomp = self.last_epoch\n",
    "        decay_steps_recomp = self.decay_steps\n",
    "        \n",
    "        if self.cycle:\n",
    "            if global_step_recomp == 0:\n",
    "                multiplier = 1.0 \n",
    "            else:\n",
    "                multiplier = math.ceil(global_step_recomp / self.decay_steps)\n",
    "            decay_steps_recomp = decay_steps_recomp * multiplier\n",
    "        else:\n",
    "            global_step_recomp = min(global_step_recomp, decay_steps_recomp)\n",
    "            \n",
    "        p = global_step_recomp / decay_steps_recomp\n",
    "        ic(self.last_epoch, optimizer.param_groups[0]['lr'], p)\n",
    "        return [((group['lr'] - self.end_learning_rate) * math.pow(1 - p, self.power) + self.end_learning_rate) for group in self.optimizer.param_groups]\n",
    "    \n",
    "    def _get_closed_form_lr(self):\n",
    "        return [(base_lr - self.end_learning_rate) * math.pow(1 - p, self.power) + self.end_learning_rate for base_lr in self.base_lrs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ef3f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(\n",
    "    params=optimizer_grounded_parameters,\n",
    "    lr=model_config[\"init_learning_rate\"],\n",
    "    betas=(0.9, 0.98),\n",
    "    weight_decay=0.0,\n",
    "    eps=1e-6)\n",
    "\n",
    "#optimizer = optim.SGD(sms_model.parameters(), lr=model_config['init_learning_rate'], weight_decay=1e-4)\n",
    "#optimizer = optim.SGD(filter(lambda p: p.requires_grad, sms_model.parameters()), lr=model_config['init_learning_rate'], weight_decay=1e-4)\n",
    "\n",
    "#scheduler = CyclicLR(\n",
    "#    optimizer, \n",
    "#    base_lr=1e-5,\n",
    "#    max_lr=model_config['init_learning_rate'],\n",
    "#    step_size_up=model_config['training_steps'] * 1,\n",
    "#    mode='triangular2',\n",
    "#    scale_mode='cycle',\n",
    "#    cycle_momentum=False\n",
    "#)\n",
    "\n",
    "\n",
    "#if model_config[\"use_multi_gpus\"]:\n",
    "#optimizer = nn.DataParallel(optimizer, device_ids=device_ids)\n",
    "\n",
    " \n",
    "scheduler = LinearWarmupCosineAnnealingLR(\n",
    "    optimizer, \n",
    "    warmup_epochs=model_config['warmup_steps'], \n",
    "    max_epochs=model_config['training_steps'] * model_config['epochs'], \n",
    "    eta_min=model_config[\"end_learning_rate\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac9d0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Mertice\n",
    "from torchmetrics import MetricCollection\n",
    "metric_collection = MetricCollection([\n",
    "    torchmetrics.Accuracy(num_classes=2, average='macro', multiclass=True, dist_sync_on_step=True, mdmc_average='global').to(device),\n",
    "    torchmetrics.Precision(num_classes=2, average='macro', multiclass=True, dist_sync_on_step=True, mdmc_average='global').to(device),\n",
    "    torchmetrics.Recall(num_classes=2, average='macro', multiclass=True, dist_sync_on_step=True, mdmc_average='global').to(device),\n",
    "    torchmetrics.F1(num_classes=2, average='macro', multiclass=True, dist_sync_on_step=True, mdmc_average='global').to(device)\n",
    "], prefix='Train_')\n",
    "\n",
    "val_metric_collection = MetricCollection([\n",
    "    torchmetrics.Accuracy(num_classes=2, average='macro', multiclass=True, dist_sync_on_step=True, mdmc_average='global').to(device),\n",
    "    torchmetrics.Precision(num_classes=2, average='macro', multiclass=True, dist_sync_on_step=True, mdmc_average='global').to(device),\n",
    "    torchmetrics.Recall(num_classes=2, average='macro', multiclass=True, dist_sync_on_step=True, mdmc_average='global').to(device),\n",
    "    torchmetrics.F1(num_classes=2, average='macro', multiclass=True, dist_sync_on_step=True, mdmc_average='global').to(device),\n",
    "], prefix='Val_')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d95526",
   "metadata": {},
   "source": [
    "# Training model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acda171b",
   "metadata": {},
   "source": [
    "### Init wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1634c8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_config['use_warmup']:\n",
    "    model_config['warmup_steps'] = int(len(train_dataloader) * model_config['epochs'] * 0.1)\n",
    "    model_config['decay_steps'] = len(train_dataloader) * model_config['epochs']\n",
    "else:\n",
    "    model_config['warmup_steps'] = None \n",
    "    model_config['decay_steps'] = None\n",
    "model_config['training_steps'] = len(train_dataloader)\n",
    "\n",
    "wandb.init(\n",
    "    project=project,\n",
    "    group=group_tag,\n",
    "    job_type=job_type,\n",
    "    name=run_id,\n",
    "    notes=method_tag,\n",
    "    tags=addition_tag,\n",
    "    sync_tensorboard=False,\n",
    "    config={**model_config},\n",
    "    reinit=True    \n",
    ")\n",
    "\n",
    "wandb_config = wandb.config\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3766bcd6",
   "metadata": {},
   "source": [
    "### Simple test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa7c0da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "torch.cuda.empty_cache()\n",
    "prefix = 'train'\n",
    "for epoch in tqdm(range(10)): # model_config['epochs']\n",
    "    start_time = time.time()    \n",
    "    train_batch_loss = 0\n",
    "    val_batch_loss = 0    \n",
    "    \n",
    "    # Training Step\n",
    "    albert_pretrain_model = albert_pretrain_model.train()\n",
    "    for step, train_batch in tqdm(enumerate(train_dataloader), \n",
    "                                  dynamic_ncols=False, \n",
    "                                  bar_format=\"{n_fmt}/{total_fmt}{bar} ETA: {remaining}s - {desc}\", \n",
    "                                  total=len(train_dataloader),\n",
    "                                  leave=True, \n",
    "                                  unit='steps'):        \n",
    "        input_ids = train_batch['input_ids'].to(device)\n",
    "        attention_mask = train_batch['attention_mask'].to(device)\n",
    "        token_type_ids = train_batch['token_type_ids'].to(device)\n",
    "\n",
    "        mlm_labels = train_batch['masked_lm_labels'].to(device)\n",
    "        sop_labels = train_batch['next_sentence_labels'].to(device)\n",
    "        \n",
    "        outputs = albert_pretrain_model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            labels=mlm_labels,\n",
    "            sentence_order_label=sop_labels\n",
    "        )\n",
    "                    \n",
    "        loss = outputs.loss.mean()\n",
    "        perplexity  = torch.exp(loss)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        if model_config[\"use_multi_gpus\"]:\n",
    "            optimizer.module.step()\n",
    "        else:\n",
    "            optimizer.step()\n",
    "            \n",
    "        wandb.log({\n",
    "            \"loss\": loss,\n",
    "            \"perplexity\": perplexity,            \n",
    "        }, step=step)\n",
    "        \n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022d8f60",
   "metadata": {},
   "source": [
    "### Define training stepsm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27eb5da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_step(model, input_ids, attention_mask, token_type_ids, mlm_labels, sop_labels, scaler, use_multi_gpus=False):\n",
    "    with autocast():\n",
    "        # Forward pass\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            labels=mlm_labels,\n",
    "            sentence_order_label=sop_labels\n",
    "        )\n",
    "        assert outputs.prediction_logits.dtype is torch.float16\n",
    "        \n",
    "        loss = outputs.loss.mean()\n",
    "        assert loss.dtype is torch.float32\n",
    "    # Backward pass\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    scaler.scale(loss).backward()\n",
    "    #if use_multi_gpus:\n",
    "    #    scaler.step(optimizer.module)\n",
    "    #else:\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "    #torch.nn.utils.clip_grad_norm_(optimizer_grounded_parameters, max_norm=0.5)\n",
    "    #if epoch > swa_start:\n",
    "    #    swa_model.update_parameters(model)\n",
    "    #    swa_scheduler.step()\n",
    "    #else:\n",
    "    #    scheduler.step()\n",
    "    return loss\n",
    "\n",
    "@torch.no_grad()\n",
    "def validataion_step(model, input_ids, attention_mask, token_type_ids, mlm_labels, sop_labels):    \n",
    "    with autocast():\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            labels=mlm_labels,\n",
    "            sentence_order_label=sop_labels\n",
    "        )\n",
    "        loss = outputs.loss.mean()\n",
    "    return loss\n",
    "\n",
    "@torch.no_grad()\n",
    "def testing_step(model, dataset_inputs):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef75eaf",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08dfa3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('[RUN ID]: {}'.format(run_id))\n",
    "torch.cuda.empty_cache()\n",
    "use_epoch_tracking = False\n",
    "use_step_tracking = True\n",
    "#wandb.watch(sms_model, log=\"all\", log_freq=1000)        \n",
    "def show_logs(loss, step, is_epoch=False, prefix='Train', **kwargs):\n",
    "    loss = float(loss)\n",
    "    if is_epoch:\n",
    "        wandb.log({\"epoch\": step, f\"{prefix}_loss\": loss}, step=step)\n",
    "    else:\n",
    "        wandb.log({f\"{prefix}_step_loss\": loss}, step=step)\n",
    "        #print(f\"{prefix} loss after \" + str(example_ct).zfill(5) + f\" examples: {loss:.3f}\")\n",
    "    if \"perplexity\" in kwargs.keys():\n",
    "        wandb.log({f\"{prefix}_perplexity\": kwargs[\"perplexity\"]}, step=step)\n",
    "\n",
    "# Creates a GradScaler once at the beginning of training.\n",
    "scaler = GradScaler()\n",
    "\n",
    "for epoch in tqdm(range(model_config['epochs'])): # model_config['epochs']\n",
    "    start_time = time.time()    \n",
    "    train_batch_loss = 0\n",
    "    valid_batch_loss = 0  \n",
    "    \n",
    "    train_perplexity = 0\n",
    "    valid_perplexity = 0\n",
    "    \n",
    "    # Training Step\n",
    "    albert_pretrain_model = albert_pretrain_model.train()\n",
    "    for step, train_batch in tqdm(enumerate(train_dataloader), \n",
    "                                  dynamic_ncols=False, \n",
    "                                  bar_format=\"{n_fmt}/{total_fmt}{bar} ETA: {remaining}s - {desc}\", \n",
    "                                  total=len(train_dataloader),\n",
    "                                  leave=True, \n",
    "                                  unit='steps'):        \n",
    "        input_ids = train_batch['input_ids'].to(device)\n",
    "        attention_mask = train_batch['attention_mask'].to(device)\n",
    "        token_type_ids = train_batch['token_type_ids'].to(device)        \n",
    "        mlm_labels = train_batch['masked_lm_labels'].to(device)\n",
    "        sop_labels = train_batch['next_sentence_labels'].to(device)\n",
    "        \n",
    "        train_loss = training_step(\n",
    "            model=albert_pretrain_model, \n",
    "            input_ids=input_ids, \n",
    "            attention_mask=attention_mask, \n",
    "            token_type_ids=token_type_ids, \n",
    "            mlm_labels=mlm_labels, \n",
    "            sop_labels=sop_labels,\n",
    "            scaler=scaler,\n",
    "            use_multi_gpus=model_config[\"use_multi_gpus\"]\n",
    "        )\n",
    "        scheduler.step()\n",
    "        train_batch_loss += train_loss.item()\n",
    "        train_perplexity += torch.exp(train_loss)\n",
    "        \n",
    "        #if model_config[\"use_multi_gpus\"]:\n",
    "        #    last_lr = optimizer.module.param_groups[0]['lr']\n",
    "        #else:\n",
    "        #last_lr = optimizer.param_groups[0]['lr']\n",
    "        last_lr = scheduler.optimizer.param_groups[0][\"lr\"]\n",
    "          \n",
    "        if use_step_tracking:\n",
    "            record_step = (step + 1) * (epoch + 1)\n",
    "            wandb.log({'learning_rate': last_lr}, step=record_step)\n",
    "            show_logs(\n",
    "                train_batch_loss / record_step, \n",
    "                record_step, \n",
    "                perplexity=train_perplexity.item() / record_step)\n",
    "    \n",
    "    if use_epoch_tracking:\n",
    "        train_epoch_loss = train_batch_loss / step\n",
    "        wandb.log({'learning_rate': last_lr}, step=epoch)\n",
    "        show_log(train_epoch_loss, epoch, is_epoch=True)  \n",
    "        #train_metric_records = metric_collection.compute()\n",
    "        #wandb.log(train_metric_records, step=epoch)\n",
    "    \n",
    "    # Validation Step\n",
    "    albert_pretrain_model = albert_pretrain_model.eval()\n",
    "    for step, valid_batch in tqdm(enumerate(val_dataloader), \n",
    "                                dynamic_ncols=False, \n",
    "                                bar_format=\"{n_fmt}/{total_fmt}{bar} ETA: {remaining}s - {desc}\", \n",
    "                                total=len(val_dataloader),\n",
    "                                leave=True, \n",
    "                                unit='steps'):            \n",
    "        input_ids = valid_batch['input_ids'].to(device)\n",
    "        attention_mask = valid_batch['attention_mask'].to(device)\n",
    "        token_type_ids = valid_batch['token_type_ids'].to(device)\n",
    "        mlm_labels = valid_batch['masked_lm_labels'].to(device)\n",
    "        sop_labels = valid_batch['next_sentence_labels'].to(device)\n",
    "            \n",
    "        valid_loss = validataion_step(\n",
    "            model=albert_pretrain_model, \n",
    "            input_ids=input_ids, \n",
    "            attention_mask=attention_mask, \n",
    "            token_type_ids=token_type_ids, \n",
    "            mlm_labels=mlm_labels,\n",
    "            sop_labels=sop_labels\n",
    "        )\n",
    "        valid_batch_loss += valid_loss.item()\n",
    "        valid_perplexity += torch.exp(valid_loss)\n",
    "        \n",
    "        if use_step_tracking:\n",
    "            record_step = (step + 1) * (epoch + 1)\n",
    "            show_logs(\n",
    "                valid_batch_loss / record_step, \n",
    "                record_step, \n",
    "                prefix='valid', \n",
    "                perplexity=valid_perplexity.item() / record_step)\n",
    "            \n",
    "        #sk_metrics = sklearn_metrics(val_outputs, labels, 'train')\n",
    "        #ic(sk_metrics)\n",
    "        #ic(val_metric_collection(outputs, labels).compute())\n",
    "        #ic(val_metric_collection(outputs, labels))\n",
    "        \n",
    "    if use_epoch_tracking:        \n",
    "        valid_epoch_loss = valid_batch_loss / step \n",
    "        show_logs(valid_epoch_loss, epoch, is_epoch=True, prefix='Val')\n",
    "        #val_metric_records = val_metric_collection.compute()\n",
    "        #wandb.log(val_metric_records, step=epoch)\n",
    "    \n",
    "    loss_template = (\"Epoch {}/{} - {:.0f}s {:.0f}ms/step - lr:{:} - loss: {:.6f} - val_loss: {:.6f}\")  \n",
    "    #metrics_template = (\n",
    "    #    \"\"\"\n",
    "    #    categorical_accuracy: {:.4f} - f1_score: {:.4f} - multi_precision: {:.4f} - multi_recall: {:.4f}\n",
    "    #    val_categorical_accuracy: {:.4f} -  val_f1_score: {:.4f} - val_multi_precision: {:.4f} - val_multi_recall: {:.4f}\n",
    "    #    \"\"\"\n",
    "    #)\n",
    "    end_time = time.time()\n",
    "    each_steps_compute_time = (end_time - start_time)\n",
    "    print(loss_template.format(\n",
    "        epoch,\n",
    "        model_config['epochs'], \n",
    "        each_steps_compute_time,\n",
    "        each_steps_compute_time * 1000 / model_config['training_steps'],\n",
    "        last_lr,\n",
    "        train_epoch_loss,\n",
    "        val_epoch_loss)\n",
    "    )\n",
    "\n",
    "    #print(metrics_template.format(\n",
    "    #    train_metric_records['Train_Accuracy'],\n",
    "    #    train_metric_records['Train_F1'],\n",
    "    #    train_metric_records['Train_Precision'],\n",
    "    #    train_metric_records['Train_Recall'],\n",
    "    #    val_metric_records['Val_Accuracy'],\n",
    "    #    val_metric_records['Val_F1'],\n",
    "    #    val_metric_records['Val_Precision'],\n",
    "    #    val_metric_records['Val_Recall']\n",
    "    #))\n",
    "    \n",
    "    if use_epoch_tracking:\n",
    "        metric_collection.reset()\n",
    "        val_metric_collection.reset()\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8594a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98323e27",
   "metadata": {},
   "source": [
    "# Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b115817",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f0f2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_models_path = main_model_path / wandb.run.name\n",
    "if not save_models_path.exists():\n",
    "    save_models_path.mkdir()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab5f36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': albert_pretrain_model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "}, str(save_models_path / 'jp_pretrain_model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7601427c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(albert_pretrain_model.state_dict(), str(save_models_path / 'jp_pretrain_model_weight.pt'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd18969e",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(str(save_models_path / 'jp_pretrain_model_weight.pt'))\n",
    "albert_pretrain_model.load_state_dict(checkpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a5dbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "albert_pretrain_model.module.state_dict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d62d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67850f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert 1 == 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b73aa9",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d915de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import math\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "from torch import nn\n",
    "from torch import cuda\n",
    "from torch import optim\n",
    "from torch.optim.swa_utils import AveragedModel, SWALR\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, CyclicLR\n",
    "from pl_bolts.optimizers.lr_scheduler import LinearWarmupCosineAnnealingLR\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "net = NeuralNetwork()\n",
    "\n",
    "optimizer = optim.SGD(net.parameters(), lr = 1e-2)\n",
    "lambda1 = lambda epoch: 0.2 if epoch % 5 == 0 else 1\n",
    "lambda2 = lambda epoch: 0.2\n",
    "\n",
    "#scheduler = optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda = lambda2)\n",
    "#scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[5,10,15], gamma=0.1)\n",
    "#scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma = 0.9)\n",
    "#scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda step: PolynomialDecay(step))\n",
    "\n",
    "class PolynomialDecay(_LRScheduler):\n",
    "    def __init__(self, optimizer, decay_steps, end_learning_rate=0.0001, power=0.5, cycle=False, last_epoch=-1, verbose=False):\n",
    "        if decay_steps <= 1.:\n",
    "            raise ValueError('max_decay_steps should be greater than 1.')            \n",
    "        self.decay_steps = decay_steps\n",
    "        self.end_learning_rate = end_learning_rate\n",
    "        self.power = power\n",
    "        self.cycle = cycle\n",
    "        super(PolynomialDecay, self).__init__(optimizer, last_epoch, verbose)\n",
    "    \n",
    "    def get_lr(self):\n",
    "        if not self._get_lr_called_within_step:\n",
    "            warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
    "                          \"please use `get_last_lr()`.\")\n",
    "        #dtype = initial_learning_rate.dtype\n",
    "        #end_learning_rate = math_ops.cast(self.end_learning_rate, dtype)\n",
    "        #power = math_ops.cast(self.power, dtype)\n",
    "        #global_step_recomp = math_ops.cast(step, dtype)\n",
    "        #decay_steps_recomp = math_ops.cast(self.decay_steps, dtype)\n",
    "        global_step_recomp = self.last_epoch\n",
    "        decay_steps_recomp = self.decay_steps\n",
    "        \n",
    "        if self.cycle:\n",
    "            if global_step_recomp == 0:\n",
    "                multiplier = 1.0 \n",
    "            else:\n",
    "                multiplier = math.ceil(global_step_recomp / self.decay_steps)\n",
    "            decay_steps_recomp = decay_steps_recomp * multiplier\n",
    "        else:\n",
    "            global_step_recomp = min(global_step_recomp, decay_steps_recomp)\n",
    "            \n",
    "        p = global_step_recomp / decay_steps_recomp\n",
    "        #c(self.last_epoch, optimizer.param_groups[0]['lr'], p)\n",
    "        return [((group['lr'] - self.end_learning_rate) * math.pow(1 - p, self.power) + self.end_learning_rate) for group in self.optimizer.param_groups]\n",
    "    \n",
    "    def _get_closed_form_lr(self):\n",
    "        return [(base_lr - self.end_learning_rate) * math.pow(1 - p, self.power) + self.end_learning_rate for base_lr in self.base_lrs]\n",
    "\n",
    "\n",
    "    \n",
    "def polynomial_decay_scale_fun(global_steps, initial_learning_rate=1e-2, decay_steps=100, power=0.5, end_learning_rate=1e-5, cycle=False):\n",
    "    if cycle:\n",
    "        if global_steps == 0:\n",
    "            multiplier = 1.0 \n",
    "        else:\n",
    "            multiplier = math.ceil(global_steps / decay_steps)\n",
    "            decay_steps = decay_steps * multiplier\n",
    "    else:\n",
    "        global_steps = min(global_steps, decay_steps)\n",
    "    p = global_steps / decay_steps\n",
    "    #ic(global_steps, p)\n",
    "    return (initial_learning_rate - end_learning_rate) * math.pow(1 - p, power) + end_learning_rate\n",
    "    \n",
    "    \n",
    "#optimizer = optim.SGD(net.parameters(), lr=1e-2)\n",
    "optimizer = optim.Adam(net.parameters(), lr=1e-3)\n",
    "#scheduler = PolynomialDecay(optimizer, decay_steps=1000, end_learning_rate=1e-5)\n",
    " \n",
    "scheduler = LinearWarmupCosineAnnealingLR(\n",
    "    optimizer, \n",
    "    warmup_epochs=model_config['warmup_steps'], \n",
    "    max_epochs=model_config['training_steps'] * model_config['epochs'], \n",
    "    eta_min=model_config[\"end_learning_rate\"])\n",
    "\n",
    "#scheduler = optim.lr_scheduler.CyclicLR(\n",
    "#    optimizer, \n",
    "#    base_lr=1e-5,\n",
    "#    max_lr=1e-2,\n",
    "#    step_size_up=20,\n",
    "#    scale_fn=polynomial_decay_scale_fun,\n",
    "#    mode='triangular2',\n",
    "#    scale_mode='cycle',\n",
    "#    cycle_momentum=False)\n",
    "\n",
    "iteration = model_config['epochs']\n",
    "scheduler_lr_list = []\n",
    "for epoch in range(1, iteration):\n",
    "    scheduler.step()\n",
    "    #print(epoch, scheduler.get_last_lr()[0])\n",
    "    scheduler_lr_list.append(scheduler.get_last_lr()[0])\n",
    "\n",
    "plt.xlabel('Training Iterations')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.title(\"CLR - 'triangular' Policy\")\n",
    "plt.plot(range(1, iteration), scheduler_lr_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272eb386",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-8.m73",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-8:m73"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
