{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445949ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import gc\n",
    "import math\n",
    "import multiprocessing\n",
    "import io\n",
    "import logging \n",
    "import itertools\n",
    "import shutil\n",
    "import pysnooper\n",
    "import warnings\n",
    "import glob\n",
    "import pendulum\n",
    "import json\n",
    "import sys\n",
    "import subprocess\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import scikitplot as skplt\n",
    "import seaborn as sns\n",
    "from icecream import ic\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from pathlib import Path\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "matplotlib.use('Agg')\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "seed = 9527\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.WARNING)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17fe549",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--input_file', default=None, help=\"Input raw text file (or comma-separated list of files).\")\n",
    "parser.add_argument('--output_file', default=None, help=\"Output TF example file (or comma-separated list of files).\")\n",
    "parser.add_argument('--vocab_file', default=None, help=\"The vocabulary file that the ALBERT model was trained on.\")\n",
    "parser.add_argument('--spm_model_file', default=None, help=\"The model file for sentence piece tokenization.\")\n",
    "parser.add_argument('--input_file_mode', default=\"r\",  help=\"The data format of the input file.\")\n",
    "parser.add_argument('--do_lower_case', default=True, help=\"Whether to lower case the input text. Should be True for uncased models and False for cased models.\")\n",
    "parser.add_argument('--do_whole_word_mask', default=True, help=\"Whether to use whole word masking rather than per-WordPiece masking.\")\n",
    "parser.add_argument('--do_permutation', default=False, help=\"Whether to do the permutation training.\")\n",
    "parser.add_argument('--favor_shorter_ngram', default=True, help=\"Whether to set higher probabilities for sampling shorter ngrams.\")\n",
    "parser.add_argument('--random_next_sentence', default=False, help=\"Whether to use the sentence that's right before the current sentence \"\n",
    "                    \"as the negative sample for next sentence prection, rather than using \"\n",
    "                    \"sentences from other random documents.\")\n",
    "parser.add_argument('--max_seq_length', default=512, help=\"Maximum sequence length.\")\n",
    "parser.add_argument('--ngram', default=3, help=\"Maximum number of ngrams to mask.\")\n",
    "parser.add_argument('--max_predictions_per_seq', default=20, help=\"Maximum number of masked LM predictions per sequence.\")\n",
    "parser.add_argument('--random_seed', default=12345, help=\"Random seed for data generation.\")\n",
    "parser.add_argument('--dupe_factor', default=5, help=\"Number of times to duplicate the input data (with different masks).\")\n",
    "parser.add_argument('--masked_lm_prob', default=0.15, help=\"Masked LM probability.\")\n",
    "parser.add_argument('--short_seq_prob', default=0.1, help=\"Probability of creating sequences which are shorter than the maximum length.\")\n",
    "\n",
    "\n",
    "opt = parser.parse_args(args=[\n",
    "    '--input_file', '1995_income',  \n",
    "    '--output_file', 'MLP',\n",
    "    '--spm_model_file', './wiki-ja_albert.model',\n",
    "    '--vocab_file', './wiki-ja_albert.vocab',\n",
    "    '--do_whole_word_mask', False,\n",
    "    '--do_permutation', False,\n",
    "    '--favor_shorter_ngram', False,\n",
    "    '--random_next_sentenc', False\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09102f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_model_folder = './model'\n",
    "cache_data_folder = './data'\n",
    "\n",
    "seed = 202105\n",
    "main_path = Path('/home/jupyter/gogolook')\n",
    "main_data_path = main_path / 'data' / 'sms' / 'experiment_data'\n",
    "main_model_path = main_path / 'models'\n",
    "main_record_folders_path =  main_path / 'tf_record_folders'\n",
    "\n",
    "# general\n",
    "saved_pytorch_model_dir = main_record_folders_path / 'saved_tf_model'\n",
    "cached_pretarin_model_folder = main_record_folders_path / 'cache_pretrain_model'\n",
    "\n",
    "\n",
    "# model\n",
    "albert_zh_path = main_model_path / 'albert_zh'\n",
    "\n",
    "# data\n",
    "regex_file_type = '*.csv'\n",
    "data_tag = 'extand_augment_and_trust_data' # basic_data, extand_augment_data\n",
    "extand_data_tag = 'bleu_score_9'\n",
    "valid_data_tag = 'extand_augment_and_trust_data'\n",
    "test_data_tag = 'extand_augment_and_trust_data'\n",
    "\n",
    "if data_tag == \"basic_data\":\n",
    "    experiment_train_data_path = main_data_path / f'train_{data_tag}'\n",
    "else:\n",
    "    experiment_train_data_path = main_data_path / f'train_{data_tag}'  #/ extand_data_tag\n",
    "    \n",
    "experiment_valid_data_path = main_data_path / f'valid_{valid_data_tag}'\n",
    "experiment_test_data_path = main_data_path / f'test_{test_data_tag}'\n",
    "\n",
    "training_data_path = experiment_train_data_path\n",
    "validation_data_path = experiment_valid_data_path\n",
    "testing_data_path = experiment_test_data_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275fdbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer, BertJapaneseTokenizer\n",
    "mecab_tokenizer = BertJapaneseTokenizer.from_pretrained(\"cl-tohoku/bert-base-japanese\", word_tokenizer_type=\"mecab\", cache_dir=cache_model_folder)\n",
    "## Input Japanese Text\n",
    "line = \"アンパサンド (&、英語名：) とは並立助詞「…と…」を意味する記号である。ラテン語の の合字で、Trebuchet MSフォントでは、と表示され \\\"et\\\" の合字であることが容易にわかる。\"\n",
    "mecab_inputs = mecab_tokenizer(line, return_tensors=\"pt\")\n",
    "print(mecab_tokenizer.decode(mecab_inputs['input_ids'][0]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907843fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-8.m73",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-8:m73"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
